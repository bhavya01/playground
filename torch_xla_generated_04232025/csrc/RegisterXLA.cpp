// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_XPU_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by lazy_tensor_generator.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <optional>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>

#include "bazel-out/k8-opt/bin/torch_xla/csrc/XLANativeFunctions.h"
#include <ATen/ops/as_strided_native.h>
#include <ATen/ops/empty.h>
#include <ATen/ops/empty_strided.h>
#include <ATen/ops/_copy_from_and_resize.h>
#include <ATen/ops/_copy_from.h>
#include <c10/macros/Macros.h>


namespace at {
namespace {
C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED("-Wunused-function")

void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}

void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
C10_DIAGNOSTIC_POP()
} // namespace
} // namespace at

// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
namespace {
at::Tensor wrapper_XLA_Scalar___lshift__(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::__lshift__(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out___lshift___out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out___lshift___out_tmp = wrapper_XLA_Scalar___lshift__(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out___lshift___out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA_Scalar___ilshift__(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::__ilshift__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA_Tensor___lshift__(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::__lshift__(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out___lshift___out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out___lshift___out_tmp = wrapper_XLA_Tensor___lshift__(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out___lshift___out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA_Tensor___ilshift__(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::__ilshift__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA_Scalar___rshift__(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::__rshift__(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out___rshift___out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out___rshift___out_tmp = wrapper_XLA_Scalar___rshift__(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out___rshift___out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA_Scalar___irshift__(at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::__irshift__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA_Tensor___rshift__(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::__rshift__(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out___rshift___out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out___rshift___out_tmp = wrapper_XLA_Tensor___rshift__(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out___rshift___out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA_Tensor___irshift__(at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::__irshift__(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA___adaptive_avg_pool2d(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_adaptive_avg_pool2d(self, C10_AS_INTARRAYREF_SLOW(output_size));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__adaptive_avg_pool2d_out(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
  auto wrapper_XLA_out__adaptive_avg_pool2d_out_tmp = wrapper_XLA___adaptive_avg_pool2d(self, output_size);
  at::_copy_from_and_resize(wrapper_XLA_out__adaptive_avg_pool2d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___adaptive_avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_adaptive_avg_pool2d_backward(grad_output, self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__adaptive_avg_pool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out__adaptive_avg_pool2d_backward_out_tmp = wrapper_XLA___adaptive_avg_pool2d_backward(grad_output, self);
  at::_copy_from_and_resize(wrapper_XLA_out__adaptive_avg_pool2d_backward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___adaptive_avg_pool3d(const at::Tensor & self, c10::SymIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_adaptive_avg_pool3d(self, C10_AS_INTARRAYREF_SLOW(output_size));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__adaptive_avg_pool3d_out(const at::Tensor & self, c10::SymIntArrayRef output_size, at::Tensor & out) {
  auto wrapper_XLA_out__adaptive_avg_pool3d_out_tmp = wrapper_XLA___adaptive_avg_pool3d(self, output_size);
  at::_copy_from_and_resize(wrapper_XLA_out__adaptive_avg_pool3d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___adaptive_avg_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_adaptive_avg_pool3d_backward(grad_output, self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__adaptive_avg_pool3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out__adaptive_avg_pool3d_backward_out_tmp = wrapper_XLA___adaptive_avg_pool3d_backward(grad_output, self);
  at::_copy_from_and_resize(wrapper_XLA_out__adaptive_avg_pool3d_backward_out_tmp, out);
  return out;
}
namespace {
void wrapper_XLA___amp_foreach_non_finite_check_and_unscale_(at::TensorList self, at::Tensor & found_inf, const at::Tensor & inv_scale) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_amp_foreach_non_finite_check_and_unscale_(self, found_inf, inv_scale);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA___amp_update_scale_(at::Tensor & self, at::Tensor & growth_tracker, const at::Tensor & found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_amp_update_scale_(self, growth_tracker, found_inf, scale_growth_factor, scale_backoff_factor, growth_interval);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA___cdist_forward(const at::Tensor & x1, const at::Tensor & x2, double p, ::std::optional<int64_t> compute_mode) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_cdist_forward(x1, x2, p, compute_mode);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__cdist_forward_out(const at::Tensor & x1, const at::Tensor & x2, double p, ::std::optional<int64_t> compute_mode, at::Tensor & out) {
  auto wrapper_XLA_out__cdist_forward_out_tmp = wrapper_XLA___cdist_forward(x1, x2, p, compute_mode);
  at::_copy_from_and_resize(wrapper_XLA_out__cdist_forward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___conj_copy(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_conj_copy(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__conj_copy_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out__conj_copy_out_tmp = wrapper_XLA___conj_copy(self);
  at::_copy_from_and_resize(wrapper_XLA_out__conj_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___convolution(const at::Tensor & input, const at::Tensor & weight, const ::std::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_convolution(input, weight, bias, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding), C10_AS_INTARRAYREF_SLOW(dilation), transposed, C10_AS_INTARRAYREF_SLOW(output_padding), groups.guard_int(__FILE__, __LINE__), benchmark, deterministic, cudnn_enabled, allow_tf32);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__convolution_out(const at::Tensor & input, const at::Tensor & weight, const ::std::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, at::Tensor & out) {
  auto wrapper_XLA_out__convolution_out_tmp = wrapper_XLA___convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
  at::_copy_from_and_resize(wrapper_XLA_out__convolution_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___copy_from(const at::Tensor & self, const at::Tensor & dst, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_copy_from(self, dst, non_blocking);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__copy_from_out(const at::Tensor & self, const at::Tensor & dst, bool non_blocking, at::Tensor & out) {
  auto wrapper_XLA_out__copy_from_out_tmp = wrapper_XLA___copy_from(self, dst, non_blocking);
  at::_copy_from_and_resize(wrapper_XLA_out__copy_from_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___copy_from_and_resize(const at::Tensor & self, const at::Tensor & dst) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_copy_from_and_resize(self, dst);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__copy_from_and_resize_out(const at::Tensor & self, const at::Tensor & dst, at::Tensor & out) {
  auto wrapper_XLA_out__copy_from_and_resize_out_tmp = wrapper_XLA___copy_from_and_resize(self, dst);
  at::_copy_from_and_resize(wrapper_XLA_out__copy_from_and_resize_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___embedding_bag_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, c10::SymInt num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const ::std::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_embedding_bag_backward(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights.guard_int(__FILE__, __LINE__), scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_XLA___embedding_bag_forward_only(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const ::std::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_embedding_bag_forward_only(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_out__embedding_bag_forward_only_out(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const ::std::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2, at::Tensor & out3) {
  auto wrapper_XLA_out__embedding_bag_forward_only_out_tmp = wrapper_XLA___embedding_bag_forward_only(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out__embedding_bag_forward_only_out_tmp), out0);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out__embedding_bag_forward_only_out_tmp), out1);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_out__embedding_bag_forward_only_out_tmp), out2);
  at::_copy_from_and_resize(std::get<3>(wrapper_XLA_out__embedding_bag_forward_only_out_tmp), out3);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2, out3);
}
namespace {
at::Tensor wrapper_XLA___euclidean_dist(const at::Tensor & x1, const at::Tensor & x2) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_euclidean_dist(x1, x2);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__euclidean_dist_out(const at::Tensor & x1, const at::Tensor & x2, at::Tensor & out) {
  auto wrapper_XLA_out__euclidean_dist_out_tmp = wrapper_XLA___euclidean_dist(x1, x2);
  at::_copy_from_and_resize(wrapper_XLA_out__euclidean_dist_out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA___index_put_impl_(at::Tensor & self, const c10::List<::std::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_index_put_impl_(self, indices, values, accumulate, unsafe);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA___linalg_eigh(const at::Tensor & A, c10::string_view UPLO, bool compute_v) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_linalg_eigh(A, UPLO, compute_v);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_eigenvalues__linalg_eigh_out(const at::Tensor & A, c10::string_view UPLO, bool compute_v, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
  auto wrapper_XLA_eigenvalues__linalg_eigh_out_tmp = wrapper_XLA___linalg_eigh(A, UPLO, compute_v);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_eigenvalues__linalg_eigh_out_tmp), eigenvalues);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_eigenvalues__linalg_eigh_out_tmp), eigenvectors);
  return ::std::tuple<at::Tensor &,at::Tensor &>(eigenvalues, eigenvectors);
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_XLA___linalg_slogdet(const at::Tensor & A) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_linalg_slogdet(A);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_sign__linalg_slogdet_out(const at::Tensor & A, at::Tensor & sign, at::Tensor & logabsdet, at::Tensor & LU, at::Tensor & pivots) {
  auto wrapper_XLA_sign__linalg_slogdet_out_tmp = wrapper_XLA___linalg_slogdet(A);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_sign__linalg_slogdet_out_tmp), sign);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_sign__linalg_slogdet_out_tmp), logabsdet);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_sign__linalg_slogdet_out_tmp), LU);
  at::_copy_from_and_resize(std::get<3>(wrapper_XLA_sign__linalg_slogdet_out_tmp), pivots);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(sign, logabsdet, LU, pivots);
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_XLA___linalg_svd(const at::Tensor & A, bool full_matrices, bool compute_uv, ::std::optional<c10::string_view> driver) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_linalg_svd(A, full_matrices, compute_uv, driver);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_U__linalg_svd_out(const at::Tensor & A, bool full_matrices, bool compute_uv, ::std::optional<c10::string_view> driver, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
  auto wrapper_XLA_U__linalg_svd_out_tmp = wrapper_XLA___linalg_svd(A, full_matrices, compute_uv, driver);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_U__linalg_svd_out_tmp), U);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_U__linalg_svd_out_tmp), S);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_U__linalg_svd_out_tmp), Vh);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(U, S, Vh);
}
namespace {
at::Scalar wrapper_XLA___local_scalar_dense(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_local_scalar_dense(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA___log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_log_softmax(self, dim, half_to_float);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__log_softmax_out(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
  auto wrapper_XLA_out__log_softmax_out_tmp = wrapper_XLA___log_softmax(self, dim, half_to_float);
  at::_copy_from_and_resize(wrapper_XLA_out__log_softmax_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_log_softmax_backward_data(grad_output, output, dim, input_dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__log_softmax_backward_data_out(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & out) {
  auto wrapper_XLA_out__log_softmax_backward_data_out_tmp = wrapper_XLA___log_softmax_backward_data(grad_output, output, dim, input_dtype);
  at::_copy_from_and_resize(wrapper_XLA_out__log_softmax_backward_data_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_XLA_no_stats__native_batch_norm_legit(const at::Tensor & input, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & bias, bool training, double momentum, double eps) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_native_batch_norm_legit(input, weight, bias, training, momentum, eps);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_no_stats_out__native_batch_norm_legit_out(const at::Tensor & input, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & bias, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
  auto wrapper_XLA_no_stats_out__native_batch_norm_legit_out_tmp = wrapper_XLA_no_stats__native_batch_norm_legit(input, weight, bias, training, momentum, eps);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_no_stats_out__native_batch_norm_legit_out_tmp), out);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_no_stats_out__native_batch_norm_legit_out_tmp), save_mean);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_no_stats_out__native_batch_norm_legit_out_tmp), save_invstd);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_XLA___native_batch_norm_legit(const at::Tensor & input, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & bias, at::Tensor & running_mean, at::Tensor & running_var, bool training, double momentum, double eps) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_native_batch_norm_legit(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA___pack_padded_sequence(const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_pack_padded_sequence(input, lengths, batch_first);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_out__pack_padded_sequence_out(const at::Tensor & input, const at::Tensor & lengths, bool batch_first, at::Tensor & out0, at::Tensor & out1) {
  auto wrapper_XLA_out__pack_padded_sequence_out_tmp = wrapper_XLA___pack_padded_sequence(input, lengths, batch_first);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out__pack_padded_sequence_out_tmp), out0);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out__pack_padded_sequence_out_tmp), out1);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
}
namespace {
at::Tensor wrapper_XLA___pdist_forward(const at::Tensor & self, double p) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_pdist_forward(self, p);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__pdist_forward_out(const at::Tensor & self, double p, at::Tensor & out) {
  auto wrapper_XLA_out__pdist_forward_out_tmp = wrapper_XLA___pdist_forward(self, p);
  at::_copy_from_and_resize(wrapper_XLA_out__pdist_forward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___prelu_kernel(const at::Tensor & self, const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_prelu_kernel(self, weight);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA___prelu_kernel_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_prelu_kernel_backward(grad_output, self, weight);
}
} // anonymous namespace
namespace {
void wrapper_XLA___propagate_xla_data(const at::Tensor & input, const at::Tensor & output) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_propagate_xla_data(input, output);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA___softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_softmax(self, dim, half_to_float);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__softmax_out(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
  auto wrapper_XLA_out__softmax_out_tmp = wrapper_XLA___softmax(self, dim, half_to_float);
  at::_copy_from_and_resize(wrapper_XLA_out__softmax_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_softmax_backward_data(grad_output, output, dim, input_dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__softmax_backward_data_out(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & grad_input) {
  auto wrapper_XLA_out__softmax_backward_data_out_tmp = wrapper_XLA___softmax_backward_data(grad_output, output, dim, input_dtype);
  at::_copy_from_and_resize(wrapper_XLA_out__softmax_backward_data_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA___to_copy(const at::Tensor & self, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory, bool non_blocking, ::std::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_to_copy(self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__to_copy_out(const at::Tensor & self, bool non_blocking, ::std::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  auto wrapper_XLA_out__to_copy_out_tmp = wrapper_XLA___to_copy(self, out.scalar_type(), out.layout(), out.device(), ::std::nullopt, non_blocking, memory_format);
  at::_copy_from_and_resize(wrapper_XLA_out__to_copy_out_tmp, out);
  return out;
}
namespace {
::std::vector<at::Tensor> wrapper_XLA___to_cpu(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_to_cpu(tensors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA___trilinear(const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_trilinear(i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__trilinear_out(const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim, at::Tensor & out) {
  auto wrapper_XLA_out__trilinear_out_tmp = wrapper_XLA___trilinear(i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
  at::_copy_from_and_resize(wrapper_XLA_out__trilinear_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA___unsafe_view(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::_unsafe_view(self, C10_AS_INTARRAYREF_SLOW(size));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out__unsafe_view_out(const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
  auto wrapper_XLA_out__unsafe_view_out_tmp = wrapper_XLA___unsafe_view(self, size);
  at::_copy_from_and_resize(wrapper_XLA_out__unsafe_view_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__abs(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::abs(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_abs_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_abs_out_tmp = wrapper_XLA__abs(self);
  at::_copy_from_and_resize(wrapper_XLA_out_abs_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__abs_(at::Tensor & self) {
  auto wrapper_XLA__abs__tmp = wrapper_XLA__abs(self);
  at::_copy_from(wrapper_XLA__abs__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__acos(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::acos(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_acos_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_acos_out_tmp = wrapper_XLA__acos(self);
  at::_copy_from_and_resize(wrapper_XLA_out_acos_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__acos_(at::Tensor & self) {
  auto wrapper_XLA__acos__tmp = wrapper_XLA__acos(self);
  at::_copy_from(wrapper_XLA__acos__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__acosh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::acosh(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_acosh_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_acosh_out_tmp = wrapper_XLA__acosh(self);
  at::_copy_from_and_resize(wrapper_XLA_out_acosh_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__acosh_(at::Tensor & self) {
  auto wrapper_XLA__acosh__tmp = wrapper_XLA__acosh(self);
  at::_copy_from(wrapper_XLA__acosh__tmp, self);
  return self;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__adaptive_max_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::adaptive_max_pool2d(self, output_size);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_out_adaptive_max_pool2d_out(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
  auto wrapper_XLA_out_adaptive_max_pool2d_out_tmp = wrapper_XLA__adaptive_max_pool2d(self, output_size);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_adaptive_max_pool2d_out_tmp), out);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_adaptive_max_pool2d_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}
namespace {
at::Tensor wrapper_XLA__adaptive_max_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::adaptive_max_pool2d_backward(grad_output, self, indices);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_adaptive_max_pool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_adaptive_max_pool2d_backward_out_tmp = wrapper_XLA__adaptive_max_pool2d_backward(grad_output, self, indices);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_adaptive_max_pool2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA_Tensor_add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::add(self, other, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_add_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_out_add_out_tmp = wrapper_XLA_Tensor_add(self, other, alpha);
  at::_copy_from_and_resize(wrapper_XLA_out_add_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  auto wrapper_XLA_Tensor_add__tmp = wrapper_XLA_Tensor_add(self, other, alpha);
  at::_copy_from(wrapper_XLA_Tensor_add__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Scalar_add(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::add(self, other, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_add_out(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_add_out_tmp = wrapper_XLA_Scalar_add(self, other, alpha);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_add_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_add_(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  auto wrapper_XLA_Scalar_add__tmp = wrapper_XLA_Scalar_add(self, other, alpha);
  at::_copy_from(wrapper_XLA_Scalar_add__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__addcdiv(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::addcdiv(self, tensor1, tensor2, value);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_addcdiv_out(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_XLA_out_addcdiv_out_tmp = wrapper_XLA__addcdiv(self, tensor1, tensor2, value);
  at::_copy_from_and_resize(wrapper_XLA_out_addcdiv_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__addcdiv_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  auto wrapper_XLA__addcdiv__tmp = wrapper_XLA__addcdiv(self, tensor1, tensor2, value);
  at::_copy_from(wrapper_XLA__addcdiv__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__addcmul(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::addcmul(self, tensor1, tensor2, value);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_addcmul_out(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_XLA_out_addcmul_out_tmp = wrapper_XLA__addcmul(self, tensor1, tensor2, value);
  at::_copy_from_and_resize(wrapper_XLA_out_addcmul_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__addcmul_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  auto wrapper_XLA__addcmul__tmp = wrapper_XLA__addcmul(self, tensor1, tensor2, value);
  at::_copy_from(wrapper_XLA__addcmul__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::addmm(self, mat1, mat2, beta, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_addmm_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_out_addmm_out_tmp = wrapper_XLA__addmm(self, mat1, mat2, beta, alpha);
  at::_copy_from_and_resize(wrapper_XLA_out_addmm_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  auto wrapper_XLA__addmm__tmp = wrapper_XLA__addmm(self, mat1, mat2, beta, alpha);
  at::_copy_from(wrapper_XLA__addmm__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__affine_grid_generator(const at::Tensor & theta, c10::SymIntArrayRef size, bool align_corners) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::affine_grid_generator(theta, C10_AS_INTARRAYREF_SLOW(size), align_corners);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_affine_grid_generator_out(const at::Tensor & theta, c10::SymIntArrayRef size, bool align_corners, at::Tensor & out) {
  auto wrapper_XLA_out_affine_grid_generator_out_tmp = wrapper_XLA__affine_grid_generator(theta, size, align_corners);
  at::_copy_from_and_resize(wrapper_XLA_out_affine_grid_generator_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__alias(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::alias(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__alias_copy(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::alias_copy(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_alias_copy_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_alias_copy_out_tmp = wrapper_XLA__alias_copy(self);
  at::_copy_from_and_resize(wrapper_XLA_out_alias_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_dim_all(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::all(self, dim, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_all_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_all_out_tmp = wrapper_XLA_dim_all(self, dim, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_all_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__all(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::all(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_all_out_all_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_all_out_all_out_tmp = wrapper_XLA__all(self);
  at::_copy_from_and_resize(wrapper_XLA_all_out_all_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__amax(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::amax(self, dim, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_amax_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_amax_out_tmp = wrapper_XLA__amax(self, dim, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_amax_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__amin(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::amin(self, dim, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_amin_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_amin_out_tmp = wrapper_XLA__amin(self, dim, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_amin_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_dim_any(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::any(self, dim, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_any_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_any_out_tmp = wrapper_XLA_dim_any(self, dim, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_any_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__any(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::any(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_all_out_any_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_all_out_any_out_tmp = wrapper_XLA__any(self);
  at::_copy_from_and_resize(wrapper_XLA_all_out_any_out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA_start_out_arange_out(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::arange_out(start, end, step, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__argmax(const at::Tensor & self, ::std::optional<int64_t> dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::argmax(self, dim, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_argmax_out(const at::Tensor & self, ::std::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_argmax_out_tmp = wrapper_XLA__argmax(self, dim, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_argmax_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__argmin(const at::Tensor & self, ::std::optional<int64_t> dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::argmin(self, dim, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_argmin_out(const at::Tensor & self, ::std::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_argmin_out_tmp = wrapper_XLA__argmin(self, dim, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_argmin_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__as_strided(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<c10::SymInt> storage_offset) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::as_strided(self, C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride), storage_offset.has_value() ? ::std::make_optional(storage_offset->guard_int(__FILE__, __LINE__)) : ::std::nullopt);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_XLA__as_strided_(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<c10::SymInt> storage_offset) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::as_strided_(self, C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride), storage_offset.has_value() ? ::std::make_optional(storage_offset->guard_int(__FILE__, __LINE__)) : ::std::nullopt);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__as_strided_copy(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<c10::SymInt> storage_offset) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::as_strided_copy(self, C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride), storage_offset.has_value() ? ::std::make_optional(storage_offset->guard_int(__FILE__, __LINE__)) : ::std::nullopt);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_as_strided_copy_out(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<c10::SymInt> storage_offset, at::Tensor & out) {
  auto wrapper_XLA_out_as_strided_copy_out_tmp = wrapper_XLA__as_strided_copy(self, size, stride, storage_offset);
  at::_copy_from_and_resize(wrapper_XLA_out_as_strided_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__as_strided_scatter(const at::Tensor & self, const at::Tensor & src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<c10::SymInt> storage_offset) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::as_strided_scatter(self, src, C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride), storage_offset.has_value() ? ::std::make_optional(storage_offset->guard_int(__FILE__, __LINE__)) : ::std::nullopt);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_as_strided_scatter_out(const at::Tensor & self, const at::Tensor & src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<c10::SymInt> storage_offset, at::Tensor & out) {
  auto wrapper_XLA_out_as_strided_scatter_out_tmp = wrapper_XLA__as_strided_scatter(self, src, size, stride, storage_offset);
  at::_copy_from_and_resize(wrapper_XLA_out_as_strided_scatter_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__asin(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::asin(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_asin_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_asin_out_tmp = wrapper_XLA__asin(self);
  at::_copy_from_and_resize(wrapper_XLA_out_asin_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__asin_(at::Tensor & self) {
  auto wrapper_XLA__asin__tmp = wrapper_XLA__asin(self);
  at::_copy_from(wrapper_XLA__asin__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__asinh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::asinh(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_asinh_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_asinh_out_tmp = wrapper_XLA__asinh(self);
  at::_copy_from_and_resize(wrapper_XLA_out_asinh_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__asinh_(at::Tensor & self) {
  auto wrapper_XLA__asinh__tmp = wrapper_XLA__asinh(self);
  at::_copy_from(wrapper_XLA__asinh__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__atan(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::atan(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_atan_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_atan_out_tmp = wrapper_XLA__atan(self);
  at::_copy_from_and_resize(wrapper_XLA_out_atan_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__atan_(at::Tensor & self) {
  auto wrapper_XLA__atan__tmp = wrapper_XLA__atan(self);
  at::_copy_from(wrapper_XLA__atan__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__atan2(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::atan2(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_atan2_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_atan2_out_tmp = wrapper_XLA__atan2(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_atan2_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__atan2_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA__atan2__tmp = wrapper_XLA__atan2(self, other);
  at::_copy_from(wrapper_XLA__atan2__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__atanh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::atanh(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_atanh_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_atanh_out_tmp = wrapper_XLA__atanh(self);
  at::_copy_from_and_resize(wrapper_XLA_out_atanh_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__atanh_(at::Tensor & self) {
  auto wrapper_XLA__atanh__tmp = wrapper_XLA__atanh(self);
  at::_copy_from(wrapper_XLA__atanh__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_avg_pool2d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override, at::Tensor & out) {
  auto wrapper_XLA_out_avg_pool2d_out_tmp = wrapper_XLA__avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  at::_copy_from_and_resize(wrapper_XLA_out_avg_pool2d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_avg_pool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_avg_pool2d_backward_out_tmp = wrapper_XLA__avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_avg_pool2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__avg_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::avg_pool3d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_avg_pool3d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override, at::Tensor & out) {
  auto wrapper_XLA_out_avg_pool3d_out_tmp = wrapper_XLA__avg_pool3d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  at::_copy_from_and_resize(wrapper_XLA_out_avg_pool3d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__avg_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::avg_pool3d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_avg_pool3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_avg_pool3d_backward_out_tmp = wrapper_XLA__avg_pool3d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_avg_pool3d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__baddbmm(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::baddbmm(self, batch1, batch2, beta, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_baddbmm_out(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_out_baddbmm_out_tmp = wrapper_XLA__baddbmm(self, batch1, batch2, beta, alpha);
  at::_copy_from_and_resize(wrapper_XLA_out_baddbmm_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__baddbmm_(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  auto wrapper_XLA__baddbmm__tmp = wrapper_XLA__baddbmm(self, batch1, batch2, beta, alpha);
  at::_copy_from(wrapper_XLA__baddbmm__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__bernoulli(const at::Tensor & self, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bernoulli(self, generator);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_bernoulli_out(const at::Tensor & self, ::std::optional<at::Generator> generator, at::Tensor & out) {
  auto wrapper_XLA_out_bernoulli_out_tmp = wrapper_XLA__bernoulli(self, generator);
  at::_copy_from_and_resize(wrapper_XLA_out_bernoulli_out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA_Tensor_bernoulli_(at::Tensor & self, const at::Tensor & p, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bernoulli_(self, p, generator);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA_p_bernoulli(const at::Tensor & self, double p, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bernoulli(self, p, generator);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_float_out_bernoulli_out(const at::Tensor & self, double p, ::std::optional<at::Generator> generator, at::Tensor & out) {
  auto wrapper_XLA_float_out_bernoulli_out_tmp = wrapper_XLA_p_bernoulli(self, p, generator);
  at::_copy_from_and_resize(wrapper_XLA_float_out_bernoulli_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_float_bernoulli_(at::Tensor & self, double p, ::std::optional<at::Generator> generator) {
  auto wrapper_XLA_float_bernoulli__tmp = wrapper_XLA_p_bernoulli(self, p, generator);
  at::_copy_from(wrapper_XLA_float_bernoulli__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__binary_cross_entropy(const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::binary_cross_entropy(self, target, weight, reduction);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_binary_cross_entropy_out(const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
  auto wrapper_XLA_out_binary_cross_entropy_out_tmp = wrapper_XLA__binary_cross_entropy(self, target, weight, reduction);
  at::_copy_from_and_resize(wrapper_XLA_out_binary_cross_entropy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__binary_cross_entropy_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::binary_cross_entropy_backward(grad_output, self, target, weight, reduction);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_binary_cross_entropy_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_binary_cross_entropy_backward_out_tmp = wrapper_XLA__binary_cross_entropy_backward(grad_output, self, target, weight, reduction);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_binary_cross_entropy_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__binary_cross_entropy_with_logits(const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & pos_weight, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::binary_cross_entropy_with_logits(self, target, weight, pos_weight, reduction);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_binary_cross_entropy_with_logits_out(const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & pos_weight, int64_t reduction, at::Tensor & out) {
  auto wrapper_XLA_out_binary_cross_entropy_with_logits_out_tmp = wrapper_XLA__binary_cross_entropy_with_logits(self, target, weight, pos_weight, reduction);
  at::_copy_from_and_resize(wrapper_XLA_out_binary_cross_entropy_with_logits_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Tensor_bitwise_and(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bitwise_and(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_bitwise_and_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_bitwise_and_out_tmp = wrapper_XLA_Tensor_bitwise_and(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_bitwise_and_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_bitwise_and_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_bitwise_and__tmp = wrapper_XLA_Tensor_bitwise_and(self, other);
  at::_copy_from(wrapper_XLA_Tensor_bitwise_and__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_bitwise_left_shift(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bitwise_left_shift(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_bitwise_left_shift_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_bitwise_left_shift_out_tmp = wrapper_XLA_Tensor_bitwise_left_shift(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_bitwise_left_shift_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_bitwise_left_shift_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_bitwise_left_shift__tmp = wrapper_XLA_Tensor_bitwise_left_shift(self, other);
  at::_copy_from(wrapper_XLA_Tensor_bitwise_left_shift__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__bitwise_not(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bitwise_not(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_bitwise_not_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_bitwise_not_out_tmp = wrapper_XLA__bitwise_not(self);
  at::_copy_from_and_resize(wrapper_XLA_out_bitwise_not_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__bitwise_not_(at::Tensor & self) {
  auto wrapper_XLA__bitwise_not__tmp = wrapper_XLA__bitwise_not(self);
  at::_copy_from(wrapper_XLA__bitwise_not__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_bitwise_or(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bitwise_or(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_bitwise_or_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_bitwise_or_out_tmp = wrapper_XLA_Tensor_bitwise_or(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_bitwise_or_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_bitwise_or_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_bitwise_or__tmp = wrapper_XLA_Tensor_bitwise_or(self, other);
  at::_copy_from(wrapper_XLA_Tensor_bitwise_or__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_bitwise_right_shift(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bitwise_right_shift(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_bitwise_right_shift_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_bitwise_right_shift_out_tmp = wrapper_XLA_Tensor_bitwise_right_shift(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_bitwise_right_shift_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_bitwise_right_shift_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_bitwise_right_shift__tmp = wrapper_XLA_Tensor_bitwise_right_shift(self, other);
  at::_copy_from(wrapper_XLA_Tensor_bitwise_right_shift__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_bitwise_xor(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bitwise_xor(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_bitwise_xor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_bitwise_xor_out_tmp = wrapper_XLA_Tensor_bitwise_xor(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_bitwise_xor_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_bitwise_xor_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_bitwise_xor__tmp = wrapper_XLA_Tensor_bitwise_xor(self, other);
  at::_copy_from(wrapper_XLA_Tensor_bitwise_xor__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__block_diag(at::TensorList tensors) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::block_diag(tensors);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_block_diag_out(at::TensorList tensors, at::Tensor & out) {
  auto wrapper_XLA_out_block_diag_out_tmp = wrapper_XLA__block_diag(tensors);
  at::_copy_from_and_resize(wrapper_XLA_out_block_diag_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__bmm(const at::Tensor & self, const at::Tensor & mat2) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::bmm(self, mat2);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_bmm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  auto wrapper_XLA_out_bmm_out_tmp = wrapper_XLA__bmm(self, mat2);
  at::_copy_from_and_resize(wrapper_XLA_out_bmm_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__cat(const at::ITensorListRef & tensors, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::cat(tensors, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_cat_out(const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
  auto wrapper_XLA_out_cat_out_tmp = wrapper_XLA__cat(tensors, dim);
  at::_copy_from_and_resize(wrapper_XLA_out_cat_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__ceil(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::ceil(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_ceil_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_ceil_out_tmp = wrapper_XLA__ceil(self);
  at::_copy_from_and_resize(wrapper_XLA_out_ceil_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__ceil_(at::Tensor & self) {
  auto wrapper_XLA__ceil__tmp = wrapper_XLA__ceil(self);
  at::_copy_from(wrapper_XLA__ceil__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__celu(const at::Tensor & self, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::celu(self, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_celu_out(const at::Tensor & self, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_out_celu_out_tmp = wrapper_XLA__celu(self, alpha);
  at::_copy_from_and_resize(wrapper_XLA_out_celu_out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA__celu_(at::Tensor & self, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::celu_(self, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__cholesky(const at::Tensor & self, bool upper) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::cholesky(self, upper);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_cholesky_out(const at::Tensor & self, bool upper, at::Tensor & out) {
  auto wrapper_XLA_out_cholesky_out_tmp = wrapper_XLA__cholesky(self, upper);
  at::_copy_from_and_resize(wrapper_XLA_out_cholesky_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__clamp(const at::Tensor & self, const ::std::optional<at::Scalar> & min, const ::std::optional<at::Scalar> & max) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::clamp(self, min, max);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_clamp_out(const at::Tensor & self, const ::std::optional<at::Scalar> & min, const ::std::optional<at::Scalar> & max, at::Tensor & out) {
  auto wrapper_XLA_out_clamp_out_tmp = wrapper_XLA__clamp(self, min, max);
  at::_copy_from_and_resize(wrapper_XLA_out_clamp_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__clamp_(at::Tensor & self, const ::std::optional<at::Scalar> & min, const ::std::optional<at::Scalar> & max) {
  auto wrapper_XLA__clamp__tmp = wrapper_XLA__clamp(self, min, max);
  at::_copy_from(wrapper_XLA__clamp__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_clamp(const at::Tensor & self, const ::std::optional<at::Tensor> & min, const ::std::optional<at::Tensor> & max) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::clamp(self, min, max);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_clamp_out(const at::Tensor & self, const ::std::optional<at::Tensor> & min, const ::std::optional<at::Tensor> & max, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_clamp_out_tmp = wrapper_XLA_Tensor_clamp(self, min, max);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_clamp_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_clamp_(at::Tensor & self, const ::std::optional<at::Tensor> & min, const ::std::optional<at::Tensor> & max) {
  auto wrapper_XLA_Tensor_clamp__tmp = wrapper_XLA_Tensor_clamp(self, min, max);
  at::_copy_from(wrapper_XLA_Tensor_clamp__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__clamp_max(const at::Tensor & self, const at::Scalar & max) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::clamp_max(self, max);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_clamp_max_out(const at::Tensor & self, const at::Scalar & max, at::Tensor & out) {
  auto wrapper_XLA_out_clamp_max_out_tmp = wrapper_XLA__clamp_max(self, max);
  at::_copy_from_and_resize(wrapper_XLA_out_clamp_max_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__clamp_max_(at::Tensor & self, const at::Scalar & max) {
  auto wrapper_XLA__clamp_max__tmp = wrapper_XLA__clamp_max(self, max);
  at::_copy_from(wrapper_XLA__clamp_max__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_clamp_max(const at::Tensor & self, const at::Tensor & max) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::clamp_max(self, max);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_clamp_max_out(const at::Tensor & self, const at::Tensor & max, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_clamp_max_out_tmp = wrapper_XLA_Tensor_clamp_max(self, max);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_clamp_max_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_clamp_max_(at::Tensor & self, const at::Tensor & max) {
  auto wrapper_XLA_Tensor_clamp_max__tmp = wrapper_XLA_Tensor_clamp_max(self, max);
  at::_copy_from(wrapper_XLA_Tensor_clamp_max__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__clamp_min(const at::Tensor & self, const at::Scalar & min) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::clamp_min(self, min);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_clamp_min_out(const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
  auto wrapper_XLA_out_clamp_min_out_tmp = wrapper_XLA__clamp_min(self, min);
  at::_copy_from_and_resize(wrapper_XLA_out_clamp_min_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__clamp_min_(at::Tensor & self, const at::Scalar & min) {
  auto wrapper_XLA__clamp_min__tmp = wrapper_XLA__clamp_min(self, min);
  at::_copy_from(wrapper_XLA__clamp_min__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_clamp_min(const at::Tensor & self, const at::Tensor & min) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::clamp_min(self, min);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_clamp_min_out(const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_clamp_min_out_tmp = wrapper_XLA_Tensor_clamp_min(self, min);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_clamp_min_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_clamp_min_(at::Tensor & self, const at::Tensor & min) {
  auto wrapper_XLA_Tensor_clamp_min__tmp = wrapper_XLA_Tensor_clamp_min(self, min);
  at::_copy_from(wrapper_XLA_Tensor_clamp_min__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__clone(const at::Tensor & self, ::std::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::clone(self, memory_format);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_clone_out(const at::Tensor & self, ::std::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  auto wrapper_XLA_out_clone_out_tmp = wrapper_XLA__clone(self, memory_format);
  at::_copy_from_and_resize(wrapper_XLA_out_clone_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__constant_pad_nd(const at::Tensor & self, c10::SymIntArrayRef pad, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::constant_pad_nd(self, C10_AS_INTARRAYREF_SLOW(pad), value);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_constant_pad_nd_out(const at::Tensor & self, c10::SymIntArrayRef pad, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_XLA_out_constant_pad_nd_out_tmp = wrapper_XLA__constant_pad_nd(self, pad, value);
  at::_copy_from_and_resize(wrapper_XLA_out_constant_pad_nd_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_XLA__convolution_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::OptionalSymIntArrayRef bias_sizes, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::convolution_backward(grad_output, input, weight, bias_sizes.has_value() ? ::std::make_optional(C10_AS_INTARRAYREF_SLOW(*bias_sizes)) : ::std::nullopt, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding), C10_AS_INTARRAYREF_SLOW(dilation), transposed, C10_AS_INTARRAYREF_SLOW(output_padding), groups.guard_int(__FILE__, __LINE__), output_mask);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_out_convolution_backward_out(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::OptionalSymIntArrayRef bias_sizes, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
  auto wrapper_XLA_out_convolution_backward_out_tmp = wrapper_XLA__convolution_backward(grad_output, input, weight, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, output_mask);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_convolution_backward_out_tmp), out0);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_convolution_backward_out_tmp), out1);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_out_convolution_backward_out_tmp), out2);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_XLA__convolution_backward_overrideable(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::convolution_backward_overrideable(grad_output, input, weight, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding), C10_AS_INTARRAYREF_SLOW(dilation), transposed, C10_AS_INTARRAYREF_SLOW(output_padding), groups.guard_int(__FILE__, __LINE__), output_mask);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_out_convolution_backward_overrideable_out(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
  auto wrapper_XLA_out_convolution_backward_overrideable_out_tmp = wrapper_XLA__convolution_backward_overrideable(grad_output, input, weight, stride, padding, dilation, transposed, output_padding, groups, output_mask);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_convolution_backward_overrideable_out_tmp), out0);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_convolution_backward_overrideable_out_tmp), out1);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_out_convolution_backward_overrideable_out_tmp), out2);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
}
namespace {
at::Tensor wrapper_XLA__convolution_overrideable(const at::Tensor & input, const at::Tensor & weight, const ::std::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::convolution_overrideable(input, weight, bias, C10_AS_INTARRAYREF_SLOW(stride), C10_AS_INTARRAYREF_SLOW(padding), C10_AS_INTARRAYREF_SLOW(dilation), transposed, C10_AS_INTARRAYREF_SLOW(output_padding), groups.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_convolution_overrideable_out(const at::Tensor & input, const at::Tensor & weight, const ::std::optional<at::Tensor> & bias, c10::SymIntArrayRef stride, c10::SymIntArrayRef padding, c10::SymIntArrayRef dilation, bool transposed, c10::SymIntArrayRef output_padding, c10::SymInt groups, at::Tensor & out) {
  auto wrapper_XLA_out_convolution_overrideable_out_tmp = wrapper_XLA__convolution_overrideable(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
  at::_copy_from_and_resize(wrapper_XLA_out_convolution_overrideable_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__copy(const at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::copy(self, src, non_blocking);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_copy_out(const at::Tensor & self, const at::Tensor & src, bool non_blocking, at::Tensor & out) {
  auto wrapper_XLA_out_copy_out_tmp = wrapper_XLA__copy(self, src, non_blocking);
  at::_copy_from_and_resize(wrapper_XLA_out_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA__copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::copy_(self, src, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__cos(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::cos(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_cos_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_cos_out_tmp = wrapper_XLA__cos(self);
  at::_copy_from_and_resize(wrapper_XLA_out_cos_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__cos_(at::Tensor & self) {
  auto wrapper_XLA__cos__tmp = wrapper_XLA__cos(self);
  at::_copy_from(wrapper_XLA__cos__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__cosh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::cosh(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_cosh_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_cosh_out_tmp = wrapper_XLA__cosh(self);
  at::_copy_from_and_resize(wrapper_XLA_out_cosh_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__cosh_(at::Tensor & self) {
  auto wrapper_XLA__cosh__tmp = wrapper_XLA__cosh(self);
  at::_copy_from(wrapper_XLA__cosh__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_dim_IntList_count_nonzero(const at::Tensor & self, at::IntArrayRef dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::count_nonzero(self, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_dim_IntList_out_count_nonzero_out(const at::Tensor & self, at::IntArrayRef dim, at::Tensor & out) {
  auto wrapper_XLA_dim_IntList_out_count_nonzero_out_tmp = wrapper_XLA_dim_IntList_count_nonzero(self, dim);
  at::_copy_from_and_resize(wrapper_XLA_dim_IntList_out_count_nonzero_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__count_nonzero(const at::Tensor & self, ::std::optional<int64_t> dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::count_nonzero(self, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_count_nonzero_out(const at::Tensor & self, ::std::optional<int64_t> dim, at::Tensor & out) {
  auto wrapper_XLA_out_count_nonzero_out_tmp = wrapper_XLA__count_nonzero(self, dim);
  at::_copy_from_and_resize(wrapper_XLA_out_count_nonzero_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__cross(const at::Tensor & self, const at::Tensor & other, ::std::optional<int64_t> dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::cross(self, other, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_cross_out(const at::Tensor & self, const at::Tensor & other, ::std::optional<int64_t> dim, at::Tensor & out) {
  auto wrapper_XLA_out_cross_out_tmp = wrapper_XLA__cross(self, other, dim);
  at::_copy_from_and_resize(wrapper_XLA_out_cross_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__cummax(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::cummax(self, dim);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_out_cummax_out(const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
  auto wrapper_XLA_out_cummax_out_tmp = wrapper_XLA__cummax(self, dim);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_cummax_out_tmp), values);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_cummax_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}
namespace {
at::Tensor wrapper_XLA__cumprod(const at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::cumprod(self, dim, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_cumprod_out(const at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_out_cumprod_out_tmp = wrapper_XLA__cumprod(self, dim, dtype);
  at::_copy_from_and_resize(wrapper_XLA_out_cumprod_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__cumprod_(at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype) {
  auto wrapper_XLA__cumprod__tmp = wrapper_XLA__cumprod(self, dim, dtype);
  at::_copy_from(wrapper_XLA__cumprod__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__cumsum(const at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::cumsum(self, dim, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_cumsum_out(const at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_out_cumsum_out_tmp = wrapper_XLA__cumsum(self, dim, dtype);
  at::_copy_from_and_resize(wrapper_XLA_out_cumsum_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__cumsum_(at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype) {
  auto wrapper_XLA__cumsum__tmp = wrapper_XLA__cumsum(self, dim, dtype);
  at::_copy_from(wrapper_XLA__cumsum__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__detach_copy(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::detach_copy(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_detach_copy_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_detach_copy_out_tmp = wrapper_XLA__detach_copy(self);
  at::_copy_from_and_resize(wrapper_XLA_out_detach_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__diag(const at::Tensor & self, int64_t diagonal) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::diag(self, diagonal);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_diag_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  auto wrapper_XLA_out_diag_out_tmp = wrapper_XLA__diag(self, diagonal);
  at::_copy_from_and_resize(wrapper_XLA_out_diag_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__diag_embed(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::diag_embed(self, offset, dim1, dim2);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_diag_embed_out(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
  auto wrapper_XLA_out_diag_embed_out_tmp = wrapper_XLA__diag_embed(self, offset, dim1, dim2);
  at::_copy_from_and_resize(wrapper_XLA_out_diag_embed_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__diagonal(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::diagonal(self, offset, dim1, dim2);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__diagonal_backward(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::diagonal_backward_symint(grad_output, input_sizes, offset, dim1, dim2);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_diagonal_backward_out(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
  auto wrapper_XLA_out_diagonal_backward_out_tmp = wrapper_XLA__diagonal_backward(grad_output, input_sizes, offset, dim1, dim2);
  at::_copy_from_and_resize(wrapper_XLA_out_diagonal_backward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__diagonal_copy(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::diagonal_copy(self, offset, dim1, dim2);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_diagonal_copy_out(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
  auto wrapper_XLA_out_diagonal_copy_out_tmp = wrapper_XLA__diagonal_copy(self, offset, dim1, dim2);
  at::_copy_from_and_resize(wrapper_XLA_out_diagonal_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__diagonal_scatter(const at::Tensor & self, const at::Tensor & src, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::diagonal_scatter(self, src, offset, dim1, dim2);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_diagonal_scatter_out(const at::Tensor & self, const at::Tensor & src, int64_t offset, int64_t dim1, int64_t dim2, at::Tensor & out) {
  auto wrapper_XLA_out_diagonal_scatter_out_tmp = wrapper_XLA__diagonal_scatter(self, src, offset, dim1, dim2);
  at::_copy_from_and_resize(wrapper_XLA_out_diagonal_scatter_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Tensor_div(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::div(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_div_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_div_out_tmp = wrapper_XLA_Tensor_div(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_div_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_div_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_div__tmp = wrapper_XLA_Tensor_div(self, other);
  at::_copy_from(wrapper_XLA_Tensor_div__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_mode_div(const at::Tensor & self, const at::Tensor & other, ::std::optional<c10::string_view> rounding_mode) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::div(self, other, rounding_mode);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_mode_div_out(const at::Tensor & self, const at::Tensor & other, ::std::optional<c10::string_view> rounding_mode, at::Tensor & out) {
  auto wrapper_XLA_out_mode_div_out_tmp = wrapper_XLA_Tensor_mode_div(self, other, rounding_mode);
  at::_copy_from_and_resize(wrapper_XLA_out_mode_div_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_mode_div_(at::Tensor & self, const at::Tensor & other, ::std::optional<c10::string_view> rounding_mode) {
  auto wrapper_XLA_Tensor_mode_div__tmp = wrapper_XLA_Tensor_mode_div(self, other, rounding_mode);
  at::_copy_from(wrapper_XLA_Tensor_mode_div__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Scalar_div(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::div(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_div_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_div_out_tmp = wrapper_XLA_Scalar_div(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_div_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_div_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_div__tmp = wrapper_XLA_Scalar_div(self, other);
  at::_copy_from(wrapper_XLA_Scalar_div__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__dot(const at::Tensor & self, const at::Tensor & tensor) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::dot(self, tensor);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_dot_out(const at::Tensor & self, const at::Tensor & tensor, at::Tensor & out) {
  auto wrapper_XLA_out_dot_out_tmp = wrapper_XLA__dot(self, tensor);
  at::_copy_from_and_resize(wrapper_XLA_out_dot_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__elu(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::elu(self, alpha, scale, input_scale);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_elu_out(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, at::Tensor & out) {
  auto wrapper_XLA_out_elu_out_tmp = wrapper_XLA__elu(self, alpha, scale, input_scale);
  at::_copy_from_and_resize(wrapper_XLA_out_elu_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__elu_(at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
  auto wrapper_XLA__elu__tmp = wrapper_XLA__elu(self, alpha, scale, input_scale);
  at::_copy_from(wrapper_XLA__elu__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__elu_backward(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::elu_backward(grad_output, alpha, scale, input_scale, is_result, self_or_result);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_elu_backward_out(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_elu_backward_out_tmp = wrapper_XLA__elu_backward(grad_output, alpha, scale, input_scale, is_result, self_or_result);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_elu_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__embedding(const at::Tensor & weight, const at::Tensor & indices, c10::SymInt padding_idx, bool scale_grad_by_freq, bool sparse) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::embedding_symint(weight, indices, padding_idx, scale_grad_by_freq, sparse);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_embedding_out(const at::Tensor & weight, const at::Tensor & indices, c10::SymInt padding_idx, bool scale_grad_by_freq, bool sparse, at::Tensor & out) {
  auto wrapper_XLA_out_embedding_out_tmp = wrapper_XLA__embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse);
  at::_copy_from_and_resize(wrapper_XLA_out_embedding_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__embedding_dense_backward(const at::Tensor & grad_output, const at::Tensor & indices, c10::SymInt num_weights, c10::SymInt padding_idx, bool scale_grad_by_freq) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::embedding_dense_backward(grad_output, indices, num_weights.guard_int(__FILE__, __LINE__), padding_idx.guard_int(__FILE__, __LINE__), scale_grad_by_freq);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_embedding_dense_backward_out(const at::Tensor & grad_output, const at::Tensor & indices, c10::SymInt num_weights, c10::SymInt padding_idx, bool scale_grad_by_freq, at::Tensor & out) {
  auto wrapper_XLA_out_embedding_dense_backward_out_tmp = wrapper_XLA__embedding_dense_backward(grad_output, indices, num_weights, padding_idx, scale_grad_by_freq);
  at::_copy_from_and_resize(wrapper_XLA_out_embedding_dense_backward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_memory_format_empty(c10::SymIntArrayRef size, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory, ::std::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::empty_symint(size, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_empty_out(c10::SymIntArrayRef size, ::std::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
  auto wrapper_XLA_out_empty_out_tmp = wrapper_XLA_memory_format_empty(size, out.scalar_type(), out.layout(), out.device(), ::std::nullopt, memory_format);
  at::_copy_from_and_resize(wrapper_XLA_out_empty_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__empty_strided(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::empty_strided_symint(size, stride, dtype, layout, device, pin_memory);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_empty_strided_out(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor & out) {
  auto wrapper_XLA_out_empty_strided_out_tmp = wrapper_XLA__empty_strided(size, stride, out.scalar_type(), out.layout(), out.device(), ::std::nullopt);
  at::_copy_from_and_resize(wrapper_XLA_out_empty_strided_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Scalar_eq(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::eq(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_eq_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_eq_out_tmp = wrapper_XLA_Scalar_eq(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_eq_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_eq_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_eq__tmp = wrapper_XLA_Scalar_eq(self, other);
  at::_copy_from(wrapper_XLA_Scalar_eq__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_eq(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::eq(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_eq_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_eq_out_tmp = wrapper_XLA_Tensor_eq(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_eq_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_eq_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_eq__tmp = wrapper_XLA_Tensor_eq(self, other);
  at::_copy_from(wrapper_XLA_Tensor_eq__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__erf(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::erf(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_erf_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_erf_out_tmp = wrapper_XLA__erf(self);
  at::_copy_from_and_resize(wrapper_XLA_out_erf_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__erf_(at::Tensor & self) {
  auto wrapper_XLA__erf__tmp = wrapper_XLA__erf(self);
  at::_copy_from(wrapper_XLA__erf__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__erfc(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::erfc(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_erfc_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_erfc_out_tmp = wrapper_XLA__erfc(self);
  at::_copy_from_and_resize(wrapper_XLA_out_erfc_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__erfc_(at::Tensor & self) {
  auto wrapper_XLA__erfc__tmp = wrapper_XLA__erfc(self);
  at::_copy_from(wrapper_XLA__erfc__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__erfinv(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::erfinv(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_erfinv_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_erfinv_out_tmp = wrapper_XLA__erfinv(self);
  at::_copy_from_and_resize(wrapper_XLA_out_erfinv_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__erfinv_(at::Tensor & self) {
  auto wrapper_XLA__erfinv__tmp = wrapper_XLA__erfinv(self);
  at::_copy_from(wrapper_XLA__erfinv__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__exp(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::exp(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_exp_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_exp_out_tmp = wrapper_XLA__exp(self);
  at::_copy_from_and_resize(wrapper_XLA_out_exp_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__exp_(at::Tensor & self) {
  auto wrapper_XLA__exp__tmp = wrapper_XLA__exp(self);
  at::_copy_from(wrapper_XLA__exp__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__expand(const at::Tensor & self, c10::SymIntArrayRef size, bool implicit) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::expand_symint(self, size, implicit);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__expand_copy(const at::Tensor & self, c10::SymIntArrayRef size, bool implicit) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::expand_copy_symint(self, size, implicit);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_expand_copy_out(const at::Tensor & self, c10::SymIntArrayRef size, bool implicit, at::Tensor & out) {
  auto wrapper_XLA_out_expand_copy_out_tmp = wrapper_XLA__expand_copy(self, size, implicit);
  at::_copy_from_and_resize(wrapper_XLA_out_expand_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__expm1(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::expm1(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_expm1_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_expm1_out_tmp = wrapper_XLA__expm1(self);
  at::_copy_from_and_resize(wrapper_XLA_out_expm1_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__expm1_(at::Tensor & self) {
  auto wrapper_XLA__expm1__tmp = wrapper_XLA__expm1(self);
  at::_copy_from(wrapper_XLA__expm1__tmp, self);
  return self;
}
namespace {
at::Tensor & wrapper_XLA__exponential_(at::Tensor & self, double lambd, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::exponential_(self, lambd, generator);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA_out_eye_out(c10::SymInt n, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::eye_out(n.guard_int(__FILE__, __LINE__), out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA_m_out_eye_out(c10::SymInt n, c10::SymInt m, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::eye_out(n.guard_int(__FILE__, __LINE__), m.guard_int(__FILE__, __LINE__), out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA_Scalar_fill_(at::Tensor & self, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::fill_(self, value);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA_Tensor_fill_(at::Tensor & self, const at::Tensor & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::fill_(self, value);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__flip(const at::Tensor & self, at::IntArrayRef dims) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::flip(self, dims);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_flip_out(const at::Tensor & self, at::IntArrayRef dims, at::Tensor & out) {
  auto wrapper_XLA_out_flip_out_tmp = wrapper_XLA__flip(self, dims);
  at::_copy_from_and_resize(wrapper_XLA_out_flip_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__floor(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::floor(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_floor_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_floor_out_tmp = wrapper_XLA__floor(self);
  at::_copy_from_and_resize(wrapper_XLA_out_floor_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__floor_(at::Tensor & self) {
  auto wrapper_XLA__floor__tmp = wrapper_XLA__floor(self);
  at::_copy_from(wrapper_XLA__floor__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__floor_divide(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::floor_divide(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_floor_divide_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_floor_divide_out_tmp = wrapper_XLA__floor_divide(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_floor_divide_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_floor_divide_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_floor_divide__tmp = wrapper_XLA__floor_divide(self, other);
  at::_copy_from(wrapper_XLA_Tensor_floor_divide__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Scalar_fmod(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::fmod(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_fmod_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_fmod_out_tmp = wrapper_XLA_Scalar_fmod(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_fmod_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_fmod_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_fmod__tmp = wrapper_XLA_Scalar_fmod(self, other);
  at::_copy_from(wrapper_XLA_Scalar_fmod__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_fmod(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::fmod(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_fmod_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_fmod_out_tmp = wrapper_XLA_Tensor_fmod(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_fmod_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_fmod_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_fmod__tmp = wrapper_XLA_Tensor_fmod(self, other);
  at::_copy_from(wrapper_XLA_Tensor_fmod__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__frac(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::frac(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_frac_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_frac_out_tmp = wrapper_XLA__frac(self);
  at::_copy_from_and_resize(wrapper_XLA_out_frac_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__frac_(at::Tensor & self) {
  auto wrapper_XLA__frac__tmp = wrapper_XLA__frac(self);
  at::_copy_from(wrapper_XLA__frac__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__full(c10::SymIntArrayRef size, const at::Scalar & fill_value, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::full(C10_AS_INTARRAYREF_SLOW(size), fill_value, dtype, layout, device, pin_memory);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_full_out(c10::SymIntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
  auto wrapper_XLA_out_full_out_tmp = wrapper_XLA__full(size, fill_value, out.scalar_type(), out.layout(), out.device(), ::std::nullopt);
  at::_copy_from_and_resize(wrapper_XLA_out_full_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__gather(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::gather(self, dim, index, sparse_grad);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_gather_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
  auto wrapper_XLA_out_gather_out_tmp = wrapper_XLA__gather(self, dim, index, sparse_grad);
  at::_copy_from_and_resize(wrapper_XLA_out_gather_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Scalar_ge(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::ge(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_ge_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_ge_out_tmp = wrapper_XLA_Scalar_ge(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_ge_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_ge_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_ge__tmp = wrapper_XLA_Scalar_ge(self, other);
  at::_copy_from(wrapper_XLA_Scalar_ge__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_ge(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::ge(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_ge_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_ge_out_tmp = wrapper_XLA_Tensor_ge(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_ge_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_ge_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_ge__tmp = wrapper_XLA_Tensor_ge(self, other);
  at::_copy_from(wrapper_XLA_Tensor_ge__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__gelu(const at::Tensor & self, c10::string_view approximate) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::gelu(self, approximate);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_gelu_out(const at::Tensor & self, c10::string_view approximate, at::Tensor & out) {
  auto wrapper_XLA_out_gelu_out_tmp = wrapper_XLA__gelu(self, approximate);
  at::_copy_from_and_resize(wrapper_XLA_out_gelu_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__gelu_(at::Tensor & self, c10::string_view approximate) {
  auto wrapper_XLA__gelu__tmp = wrapper_XLA__gelu(self, approximate);
  at::_copy_from(wrapper_XLA__gelu__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__gelu_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::gelu_backward(grad_output, self, approximate);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_gelu_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_gelu_backward_out_tmp = wrapper_XLA__gelu_backward(grad_output, self, approximate);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_gelu_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__glu(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::glu(self, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_glu_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
  auto wrapper_XLA_out_glu_out_tmp = wrapper_XLA__glu(self, dim);
  at::_copy_from_and_resize(wrapper_XLA_out_glu_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Scalar_gt(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::gt(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_gt_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_gt_out_tmp = wrapper_XLA_Scalar_gt(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_gt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_gt_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_gt__tmp = wrapper_XLA_Scalar_gt(self, other);
  at::_copy_from(wrapper_XLA_Scalar_gt__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_gt(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::gt(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_gt_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_gt_out_tmp = wrapper_XLA_Tensor_gt(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_gt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_gt_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_gt__tmp = wrapper_XLA_Tensor_gt(self, other);
  at::_copy_from(wrapper_XLA_Tensor_gt__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__hardshrink(const at::Tensor & self, const at::Scalar & lambd) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::hardshrink(self, lambd);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_hardshrink_out(const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
  auto wrapper_XLA_out_hardshrink_out_tmp = wrapper_XLA__hardshrink(self, lambd);
  at::_copy_from_and_resize(wrapper_XLA_out_hardshrink_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__hardshrink_backward(const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::hardshrink_backward(grad_out, self, lambd);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_hardshrink_backward_out(const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_hardshrink_backward_out_tmp = wrapper_XLA__hardshrink_backward(grad_out, self, lambd);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_hardshrink_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__hardsigmoid(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::hardsigmoid(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_hardsigmoid_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_hardsigmoid_out_tmp = wrapper_XLA__hardsigmoid(self);
  at::_copy_from_and_resize(wrapper_XLA_out_hardsigmoid_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__hardsigmoid_(at::Tensor & self) {
  auto wrapper_XLA__hardsigmoid__tmp = wrapper_XLA__hardsigmoid(self);
  at::_copy_from(wrapper_XLA__hardsigmoid__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__hardsigmoid_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::hardsigmoid_backward(grad_output, self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_hardsigmoid_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_hardsigmoid_backward_out_tmp = wrapper_XLA__hardsigmoid_backward(grad_output, self);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_hardsigmoid_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__hardswish(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::hardswish(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_hardswish_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_hardswish_out_tmp = wrapper_XLA__hardswish(self);
  at::_copy_from_and_resize(wrapper_XLA_out_hardswish_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__hardswish_(at::Tensor & self) {
  auto wrapper_XLA__hardswish__tmp = wrapper_XLA__hardswish(self);
  at::_copy_from(wrapper_XLA__hardswish__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__hardswish_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::hardswish_backward(grad_output, self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_hardswish_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_hardswish_backward_out_tmp = wrapper_XLA__hardswish_backward(grad_output, self);
  at::_copy_from_and_resize(wrapper_XLA_out_hardswish_backward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__hardtanh(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::hardtanh(self, min_val, max_val);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_hardtanh_out(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
  auto wrapper_XLA_out_hardtanh_out_tmp = wrapper_XLA__hardtanh(self, min_val, max_val);
  at::_copy_from_and_resize(wrapper_XLA_out_hardtanh_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__hardtanh_(at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  auto wrapper_XLA__hardtanh__tmp = wrapper_XLA__hardtanh(self, min_val, max_val);
  at::_copy_from(wrapper_XLA__hardtanh__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__hardtanh_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::hardtanh_backward(grad_output, self, min_val, max_val);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_hardtanh_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_hardtanh_backward_out_tmp = wrapper_XLA__hardtanh_backward(grad_output, self, min_val, max_val);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_hardtanh_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA_Tensor_index(const at::Tensor & self, const c10::List<::std::optional<at::Tensor>> & indices) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::index(self, indices);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_index_out(const at::Tensor & self, const c10::List<::std::optional<at::Tensor>> & indices, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_index_out_tmp = wrapper_XLA_Tensor_index(self, indices);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_index_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__index_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::index_add(self, dim, index, source, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_index_add_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_out_index_add_out_tmp = wrapper_XLA__index_add(self, dim, index, source, alpha);
  at::_copy_from_and_resize(wrapper_XLA_out_index_add_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__index_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
  auto wrapper_XLA__index_add__tmp = wrapper_XLA__index_add(self, dim, index, source, alpha);
  at::_copy_from(wrapper_XLA__index_add__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__index_copy(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::index_copy(self, dim, index, source);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_index_copy_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, at::Tensor & out) {
  auto wrapper_XLA_out_index_copy_out_tmp = wrapper_XLA__index_copy(self, dim, index, source);
  at::_copy_from_and_resize(wrapper_XLA_out_index_copy_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__index_copy_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  auto wrapper_XLA__index_copy__tmp = wrapper_XLA__index_copy(self, dim, index, source);
  at::_copy_from(wrapper_XLA__index_copy__tmp, self);
  return self;
}
namespace {
at::Tensor & wrapper_XLA_int_Scalar_index_fill_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::index_fill_(self, dim, index, value);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA_int_Tensor_index_fill_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::index_fill_(self, dim, index, value);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA__index_put_(at::Tensor & self, const c10::List<::std::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::index_put_(self, indices, values, accumulate);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::index_select(self, dim, index);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_index_select_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
  auto wrapper_XLA_out_index_select_out_tmp = wrapper_XLA__index_select(self, dim, index);
  at::_copy_from_and_resize(wrapper_XLA_out_index_select_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__inverse(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::inverse(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_inverse_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_inverse_out_tmp = wrapper_XLA__inverse(self);
  at::_copy_from_and_resize(wrapper_XLA_out_inverse_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__isnan(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::isnan(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_isnan_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_isnan_out_tmp = wrapper_XLA__isnan(self);
  at::_copy_from_and_resize(wrapper_XLA_out_isnan_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__isneginf(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::isneginf(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_isneginf_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_isneginf_out_tmp = wrapper_XLA__isneginf(self);
  at::_copy_from_and_resize(wrapper_XLA_out_isneginf_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__kl_div(const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::kl_div(self, target, reduction, log_target);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__kthvalue(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::kthvalue(self, k, dim, keepdim);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_values_kthvalue_out(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  auto wrapper_XLA_values_kthvalue_out_tmp = wrapper_XLA__kthvalue(self, k, dim, keepdim);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_values_kthvalue_out_tmp), values);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_values_kthvalue_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}
namespace {
at::Tensor wrapper_XLA_Scalar_le(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::le(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_le_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_le_out_tmp = wrapper_XLA_Scalar_le(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_le_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_le_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_le__tmp = wrapper_XLA_Scalar_le(self, other);
  at::_copy_from(wrapper_XLA_Scalar_le__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_le(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::le(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_le_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_le_out_tmp = wrapper_XLA_Tensor_le(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_le_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_le_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_le__tmp = wrapper_XLA_Tensor_le(self, other);
  at::_copy_from(wrapper_XLA_Tensor_le__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::leaky_relu(self, negative_slope);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_leaky_relu_out(const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
  auto wrapper_XLA_out_leaky_relu_out_tmp = wrapper_XLA__leaky_relu(self, negative_slope);
  at::_copy_from_and_resize(wrapper_XLA_out_leaky_relu_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__leaky_relu_(at::Tensor & self, const at::Scalar & negative_slope) {
  auto wrapper_XLA__leaky_relu__tmp = wrapper_XLA__leaky_relu(self, negative_slope);
  at::_copy_from(wrapper_XLA__leaky_relu__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__leaky_relu_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::leaky_relu_backward(grad_output, self, negative_slope, self_is_result);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_leaky_relu_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_leaky_relu_backward_out_tmp = wrapper_XLA__leaky_relu_backward(grad_output, self, negative_slope, self_is_result);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_leaky_relu_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA_Scalar_lerp(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::lerp(self, end, weight);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_lerp_out(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_lerp_out_tmp = wrapper_XLA_Scalar_lerp(self, end, weight);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_lerp_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_lerp_(at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
  auto wrapper_XLA_Scalar_lerp__tmp = wrapper_XLA_Scalar_lerp(self, end, weight);
  at::_copy_from(wrapper_XLA_Scalar_lerp__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_lerp(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::lerp(self, end, weight);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_lerp_out(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_lerp_out_tmp = wrapper_XLA_Tensor_lerp(self, end, weight);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_lerp_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_lerp_(at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
  auto wrapper_XLA_Tensor_lerp__tmp = wrapper_XLA_Tensor_lerp(self, end, weight);
  at::_copy_from(wrapper_XLA_Tensor_lerp__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__lift(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::lift(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_lift_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_lift_out_tmp = wrapper_XLA__lift(self);
  at::_copy_from_and_resize(wrapper_XLA_out_lift_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__lift_fresh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::lift_fresh(self);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__linalg_inv_ex(const at::Tensor & A, bool check_errors) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::linalg_inv_ex(A, check_errors);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_inverse_linalg_inv_ex_out(const at::Tensor & A, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
  auto wrapper_XLA_inverse_linalg_inv_ex_out_tmp = wrapper_XLA__linalg_inv_ex(A, check_errors);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_inverse_linalg_inv_ex_out_tmp), inverse);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_inverse_linalg_inv_ex_out_tmp), info);
  return ::std::tuple<at::Tensor &,at::Tensor &>(inverse, info);
}
namespace {
at::Tensor wrapper_XLA_atol_rtol_tensor_linalg_pinv(const at::Tensor & self, const ::std::optional<at::Tensor> & atol, const ::std::optional<at::Tensor> & rtol, bool hermitian) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::linalg_pinv(self, atol, rtol, hermitian);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_atol_rtol_tensor_out_linalg_pinv_out(const at::Tensor & self, const ::std::optional<at::Tensor> & atol, const ::std::optional<at::Tensor> & rtol, bool hermitian, at::Tensor & out) {
  auto wrapper_XLA_atol_rtol_tensor_out_linalg_pinv_out_tmp = wrapper_XLA_atol_rtol_tensor_linalg_pinv(self, atol, rtol, hermitian);
  at::_copy_from_and_resize(wrapper_XLA_atol_rtol_tensor_out_linalg_pinv_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__linalg_vector_norm(const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::linalg_vector_norm(self, ord, dim, keepdim, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_linalg_vector_norm_out(const at::Tensor & self, const at::Scalar & ord, at::OptionalIntArrayRef dim, bool keepdim, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_out_linalg_vector_norm_out_tmp = wrapper_XLA__linalg_vector_norm(self, ord, dim, keepdim, dtype);
  at::_copy_from_and_resize(wrapper_XLA_out_linalg_vector_norm_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__linspace(const at::Scalar & start, const at::Scalar & end, int64_t steps, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::linspace(start, end, steps, dtype, layout, device, pin_memory);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_linspace_out(const at::Scalar & start, const at::Scalar & end, int64_t steps, at::Tensor & out) {
  auto wrapper_XLA_out_linspace_out_tmp = wrapper_XLA__linspace(start, end, steps, out.scalar_type(), out.layout(), out.device(), ::std::nullopt);
  at::_copy_from_and_resize(wrapper_XLA_out_linspace_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__log(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::log(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_log_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_log_out_tmp = wrapper_XLA__log(self);
  at::_copy_from_and_resize(wrapper_XLA_out_log_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__log_(at::Tensor & self) {
  auto wrapper_XLA__log__tmp = wrapper_XLA__log(self);
  at::_copy_from(wrapper_XLA__log__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__log10(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::log10(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_log10_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_log10_out_tmp = wrapper_XLA__log10(self);
  at::_copy_from_and_resize(wrapper_XLA_out_log10_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__log10_(at::Tensor & self) {
  auto wrapper_XLA__log10__tmp = wrapper_XLA__log10(self);
  at::_copy_from(wrapper_XLA__log10__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__log1p(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::log1p(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_log1p_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_log1p_out_tmp = wrapper_XLA__log1p(self);
  at::_copy_from_and_resize(wrapper_XLA_out_log1p_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__log1p_(at::Tensor & self) {
  auto wrapper_XLA__log1p__tmp = wrapper_XLA__log1p(self);
  at::_copy_from(wrapper_XLA__log1p__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__log2(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::log2(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_log2_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_log2_out_tmp = wrapper_XLA__log2(self);
  at::_copy_from_and_resize(wrapper_XLA_out_log2_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__log2_(at::Tensor & self) {
  auto wrapper_XLA__log2__tmp = wrapper_XLA__log2(self);
  at::_copy_from(wrapper_XLA__log2__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__log_sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::log_sigmoid_backward(grad_output, self, buffer);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_log_sigmoid_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_log_sigmoid_backward_out_tmp = wrapper_XLA__log_sigmoid_backward(grad_output, self, buffer);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_log_sigmoid_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__log_sigmoid_forward(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::log_sigmoid_forward(self);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_output_log_sigmoid_forward_out(const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
  auto wrapper_XLA_output_log_sigmoid_forward_out_tmp = wrapper_XLA__log_sigmoid_forward(self);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_output_log_sigmoid_forward_out_tmp), output);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_output_log_sigmoid_forward_out_tmp), buffer);
  return ::std::tuple<at::Tensor &,at::Tensor &>(output, buffer);
}
namespace {
at::Tensor wrapper_XLA__logdet(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::logdet(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__logical_and(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::logical_and(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_logical_and_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_logical_and_out_tmp = wrapper_XLA__logical_and(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_logical_and_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__logical_and_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA__logical_and__tmp = wrapper_XLA__logical_and(self, other);
  at::_copy_from(wrapper_XLA__logical_and__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__logical_not(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::logical_not(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_logical_not_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_logical_not_out_tmp = wrapper_XLA__logical_not(self);
  at::_copy_from_and_resize(wrapper_XLA_out_logical_not_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__logical_not_(at::Tensor & self) {
  auto wrapper_XLA__logical_not__tmp = wrapper_XLA__logical_not(self);
  at::_copy_from(wrapper_XLA__logical_not__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__logical_or(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::logical_or(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_logical_or_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_logical_or_out_tmp = wrapper_XLA__logical_or(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_logical_or_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__logical_or_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA__logical_or__tmp = wrapper_XLA__logical_or(self, other);
  at::_copy_from(wrapper_XLA__logical_or__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__logical_xor(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::logical_xor(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_logical_xor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_logical_xor_out_tmp = wrapper_XLA__logical_xor(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_logical_xor_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__logical_xor_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA__logical_xor__tmp = wrapper_XLA__logical_xor(self, other);
  at::_copy_from(wrapper_XLA__logical_xor__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__logit(const at::Tensor & self, ::std::optional<double> eps) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::logit(self, eps);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_logit_out(const at::Tensor & self, ::std::optional<double> eps, at::Tensor & out) {
  auto wrapper_XLA_out_logit_out_tmp = wrapper_XLA__logit(self, eps);
  at::_copy_from_and_resize(wrapper_XLA_out_logit_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__logit_(at::Tensor & self, ::std::optional<double> eps) {
  auto wrapper_XLA__logit__tmp = wrapper_XLA__logit(self, eps);
  at::_copy_from(wrapper_XLA__logit__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__logsumexp(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::logsumexp(self, dim, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_logsumexp_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_logsumexp_out_tmp = wrapper_XLA__logsumexp(self, dim, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_logsumexp_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Scalar_lt(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::lt(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_lt_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_lt_out_tmp = wrapper_XLA_Scalar_lt(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_lt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_lt_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_lt__tmp = wrapper_XLA_Scalar_lt(self, other);
  at::_copy_from(wrapper_XLA_Scalar_lt__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_lt(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::lt(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_lt_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_lt_out_tmp = wrapper_XLA_Tensor_lt(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_lt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_lt_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_lt__tmp = wrapper_XLA_Tensor_lt(self, other);
  at::_copy_from(wrapper_XLA_Tensor_lt__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Scalar_masked_fill(const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::masked_fill(self, mask, value);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_masked_fill_out(const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_masked_fill_out_tmp = wrapper_XLA_Scalar_masked_fill(self, mask, value);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_masked_fill_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
  auto wrapper_XLA_Scalar_masked_fill__tmp = wrapper_XLA_Scalar_masked_fill(self, mask, value);
  at::_copy_from(wrapper_XLA_Scalar_masked_fill__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_masked_fill(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::masked_fill(self, mask, value);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_masked_fill_out(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_masked_fill_out_tmp = wrapper_XLA_Tensor_masked_fill(self, mask, value);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_masked_fill_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
  auto wrapper_XLA_Tensor_masked_fill__tmp = wrapper_XLA_Tensor_masked_fill(self, mask, value);
  at::_copy_from(wrapper_XLA_Tensor_masked_fill__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__masked_scatter(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::masked_scatter(self, mask, source);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_masked_scatter_out(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source, at::Tensor & out) {
  auto wrapper_XLA_out_masked_scatter_out_tmp = wrapper_XLA__masked_scatter(self, mask, source);
  at::_copy_from_and_resize(wrapper_XLA_out_masked_scatter_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__masked_scatter_(at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
  auto wrapper_XLA__masked_scatter__tmp = wrapper_XLA__masked_scatter(self, mask, source);
  at::_copy_from(wrapper_XLA__masked_scatter__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__masked_select(const at::Tensor & self, const at::Tensor & mask) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::masked_select(self, mask);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_masked_select_out(const at::Tensor & self, const at::Tensor & mask, at::Tensor & out) {
  auto wrapper_XLA_out_masked_select_out_tmp = wrapper_XLA__masked_select(self, mask);
  at::_copy_from_and_resize(wrapper_XLA_out_masked_select_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA_dim_max(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max(self, dim, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_dim_max_max_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max_out(self, dim, keepdim, max, max_values);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__max(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_unary_out_max_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_unary_out_max_out_tmp = wrapper_XLA__max(self);
  at::_copy_from_and_resize(wrapper_XLA_unary_out_max_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__max_pool2d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_out_max_pool2d_with_indices_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
  auto wrapper_XLA_out_max_pool2d_with_indices_out_tmp = wrapper_XLA__max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_max_pool2d_with_indices_out_tmp), out);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_max_pool2d_with_indices_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}
namespace {
at::Tensor wrapper_XLA__max_pool2d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_max_pool2d_with_indices_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_max_pool2d_with_indices_backward_out_tmp = wrapper_XLA__max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_max_pool2d_with_indices_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__max_pool3d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max_pool3d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_out_max_pool3d_with_indices_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
  auto wrapper_XLA_out_max_pool3d_with_indices_out_tmp = wrapper_XLA__max_pool3d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_max_pool3d_with_indices_out_tmp), out);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_max_pool3d_with_indices_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}
namespace {
at::Tensor wrapper_XLA__max_pool3d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max_pool3d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_max_pool3d_with_indices_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_max_pool3d_with_indices_backward_out_tmp = wrapper_XLA__max_pool3d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_max_pool3d_with_indices_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__max_unpool2d(const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max_unpool2d(self, indices, C10_AS_INTARRAYREF_SLOW(output_size));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_max_unpool2d_out(const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::Tensor & out) {
  auto wrapper_XLA_out_max_unpool2d_out_tmp = wrapper_XLA__max_unpool2d(self, indices, output_size);
  at::_copy_from_and_resize(wrapper_XLA_out_max_unpool2d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__max_unpool3d(const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::max_unpool3d(self, indices, C10_AS_INTARRAYREF_SLOW(output_size), stride, padding);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_max_unpool3d_out(const at::Tensor & self, const at::Tensor & indices, c10::SymIntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
  auto wrapper_XLA_out_max_unpool3d_out_tmp = wrapper_XLA__max_unpool3d(self, indices, output_size, stride, padding);
  at::_copy_from_and_resize(wrapper_XLA_out_max_unpool3d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__maximum(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::maximum(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_maximum_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_maximum_out_tmp = wrapper_XLA__maximum(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_maximum_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__mean(const at::Tensor & self, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mean(self, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_dtype_out_mean_out(const at::Tensor & self, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_dtype_out_mean_out_tmp = wrapper_XLA__mean(self, dtype);
  at::_copy_from_and_resize(wrapper_XLA_dtype_out_mean_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_dim_mean(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mean(self, dim, keepdim, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_mean_out(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_out_mean_out_tmp = wrapper_XLA_dim_mean(self, dim, keepdim, dtype);
  at::_copy_from_and_resize(wrapper_XLA_out_mean_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA_dim_min(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::min(self, dim, keepdim);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_dim_min_min_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::min_out(self, dim, keepdim, min, min_indices);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__min(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::min(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_unary_out_min_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_unary_out_min_out_tmp = wrapper_XLA__min(self);
  at::_copy_from_and_resize(wrapper_XLA_unary_out_min_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__minimum(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::minimum(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_minimum_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_minimum_out_tmp = wrapper_XLA__minimum(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_minimum_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__mish(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mish(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_mish_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_mish_out_tmp = wrapper_XLA__mish(self);
  at::_copy_from_and_resize(wrapper_XLA_out_mish_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__mish_(at::Tensor & self) {
  auto wrapper_XLA__mish__tmp = wrapper_XLA__mish(self);
  at::_copy_from(wrapper_XLA__mish__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__mm(const at::Tensor & self, const at::Tensor & mat2) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mm(self, mat2);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_mm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  auto wrapper_XLA_out_mm_out_tmp = wrapper_XLA__mm(self, mat2);
  at::_copy_from_and_resize(wrapper_XLA_out_mm_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__mse_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mse_loss(self, target, reduction);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_mse_loss_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
  auto wrapper_XLA_out_mse_loss_out_tmp = wrapper_XLA__mse_loss(self, target, reduction);
  at::_copy_from_and_resize(wrapper_XLA_out_mse_loss_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__mse_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mse_loss_backward(grad_output, self, target, reduction);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_mse_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_mse_loss_backward_out_tmp = wrapper_XLA__mse_loss_backward(grad_output, self, target, reduction);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_mse_loss_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA_Tensor_mul(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mul(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_mul_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_out_mul_out_tmp = wrapper_XLA_Tensor_mul(self, other);
  at::_copy_from_and_resize(wrapper_XLA_out_mul_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_mul_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_mul__tmp = wrapper_XLA_Tensor_mul(self, other);
  at::_copy_from(wrapper_XLA_Tensor_mul__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Scalar_mul(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mul(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_mul_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_mul_out_tmp = wrapper_XLA_Scalar_mul(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_mul_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_mul_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_mul__tmp = wrapper_XLA_Scalar_mul(self, other);
  at::_copy_from(wrapper_XLA_Scalar_mul__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__multinomial(const at::Tensor & self, c10::SymInt num_samples, bool replacement, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::multinomial(self, num_samples.guard_int(__FILE__, __LINE__), replacement, generator);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_multinomial_out(const at::Tensor & self, c10::SymInt num_samples, bool replacement, ::std::optional<at::Generator> generator, at::Tensor & out) {
  auto wrapper_XLA_out_multinomial_out_tmp = wrapper_XLA__multinomial(self, num_samples, replacement, generator);
  at::_copy_from_and_resize(wrapper_XLA_out_multinomial_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__mv(const at::Tensor & self, const at::Tensor & vec) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mv(self, vec);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA_out_mv_out(const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mv_out(self, vec, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__mvlgamma(const at::Tensor & self, int64_t p) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::mvlgamma(self, p);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_mvlgamma_out(const at::Tensor & self, int64_t p, at::Tensor & out) {
  auto wrapper_XLA_out_mvlgamma_out_tmp = wrapper_XLA__mvlgamma(self, p);
  at::_copy_from_and_resize(wrapper_XLA_out_mvlgamma_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__mvlgamma_(at::Tensor & self, int64_t p) {
  auto wrapper_XLA__mvlgamma__tmp = wrapper_XLA__mvlgamma(self, p);
  at::_copy_from(wrapper_XLA__mvlgamma__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__nan_to_num(const at::Tensor & self, ::std::optional<double> nan, ::std::optional<double> posinf, ::std::optional<double> neginf) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::nan_to_num(self, nan, posinf, neginf);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_nan_to_num_out(const at::Tensor & self, ::std::optional<double> nan, ::std::optional<double> posinf, ::std::optional<double> neginf, at::Tensor & out) {
  auto wrapper_XLA_out_nan_to_num_out_tmp = wrapper_XLA__nan_to_num(self, nan, posinf, neginf);
  at::_copy_from_and_resize(wrapper_XLA_out_nan_to_num_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__nan_to_num_(at::Tensor & self, ::std::optional<double> nan, ::std::optional<double> posinf, ::std::optional<double> neginf) {
  auto wrapper_XLA__nan_to_num__tmp = wrapper_XLA__nan_to_num(self, nan, posinf, neginf);
  at::_copy_from(wrapper_XLA__nan_to_num__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__narrow_copy(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::narrow_copy_symint(self, dim, start, length);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_narrow_copy_out(const at::Tensor & self, int64_t dim, c10::SymInt start, c10::SymInt length, at::Tensor & out) {
  auto wrapper_XLA_out_narrow_copy_out_tmp = wrapper_XLA__narrow_copy(self, dim, start, length);
  at::_copy_from_and_resize(wrapper_XLA_out_narrow_copy_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_XLA__native_batch_norm(const at::Tensor & input, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & bias, const ::std::optional<at::Tensor> & running_mean, const ::std::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::native_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_out_native_batch_norm_out(const at::Tensor & input, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & bias, const ::std::optional<at::Tensor> & running_mean, const ::std::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
  auto wrapper_XLA_out_native_batch_norm_out_tmp = wrapper_XLA__native_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_native_batch_norm_out_tmp), out);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_native_batch_norm_out_tmp), save_mean);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_out_native_batch_norm_out_tmp), save_invstd);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_XLA__native_batch_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & running_mean, const ::std::optional<at::Tensor> & running_var, const ::std::optional<at::Tensor> & save_mean, const ::std::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::native_batch_norm_backward(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_out_native_batch_norm_backward_out(const at::Tensor & grad_out, const at::Tensor & input, const ::std::optional<at::Tensor> & weight, const ::std::optional<at::Tensor> & running_mean, const ::std::optional<at::Tensor> & running_var, const ::std::optional<at::Tensor> & save_mean, const ::std::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask, at::Tensor & out0, at::Tensor & out1, at::Tensor & out2) {
  auto wrapper_XLA_out_native_batch_norm_backward_out_tmp = wrapper_XLA__native_batch_norm_backward(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_native_batch_norm_backward_out_tmp), out0);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_native_batch_norm_backward_out_tmp), out1);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_out_native_batch_norm_backward_out_tmp), out2);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out0, out1, out2);
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__native_dropout(const at::Tensor & input, double p, ::std::optional<bool> train) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::native_dropout(input, p, train);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_out_native_dropout_out(const at::Tensor & input, double p, ::std::optional<bool> train, at::Tensor & out0, at::Tensor & out1) {
  auto wrapper_XLA_out_native_dropout_out_tmp = wrapper_XLA__native_dropout(input, p, train);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_out_native_dropout_out_tmp), out0);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_out_native_dropout_out_tmp), out1);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
}
namespace {
at::Tensor wrapper_XLA__native_dropout_backward(const at::Tensor & grad_output, const at::Tensor & mask, double scale) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::native_dropout_backward(grad_output, mask, scale);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_native_dropout_backward_out(const at::Tensor & grad_output, const at::Tensor & mask, double scale, at::Tensor & out) {
  auto wrapper_XLA_out_native_dropout_backward_out_tmp = wrapper_XLA__native_dropout_backward(grad_output, mask, scale);
  at::_copy_from_and_resize(wrapper_XLA_out_native_dropout_backward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Scalar_ne(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::ne(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_ne_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_ne_out_tmp = wrapper_XLA_Scalar_ne(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_ne_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_ne_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_ne__tmp = wrapper_XLA_Scalar_ne(self, other);
  at::_copy_from(wrapper_XLA_Scalar_ne__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_ne(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::ne(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_ne_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_ne_out_tmp = wrapper_XLA_Tensor_ne(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_ne_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_ne_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_ne__tmp = wrapper_XLA_Tensor_ne(self, other);
  at::_copy_from(wrapper_XLA_Tensor_ne__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__neg(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::neg(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_neg_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_neg_out_tmp = wrapper_XLA__neg(self);
  at::_copy_from_and_resize(wrapper_XLA_out_neg_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__neg_(at::Tensor & self) {
  auto wrapper_XLA__neg__tmp = wrapper_XLA__neg(self);
  at::_copy_from(wrapper_XLA__neg__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__new_empty_strided(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::new_empty_strided_symint(self, size, stride, dtype, layout, device, pin_memory);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_new_empty_strided_out(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::Tensor & out) {
  auto wrapper_XLA_out_new_empty_strided_out_tmp = wrapper_XLA__new_empty_strided(self, size, stride, out.scalar_type(), out.layout(), out.device(), ::std::nullopt);
  at::_copy_from_and_resize(wrapper_XLA_out_new_empty_strided_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__nll_loss2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::nll_loss2d_backward(grad_output, self, target, weight, reduction, ignore_index.guard_int(__FILE__, __LINE__), total_weight);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_nll_loss2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_nll_loss2d_backward_out_tmp = wrapper_XLA__nll_loss2d_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_nll_loss2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__nll_loss2d_forward(const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::nll_loss2d_forward(self, target, weight, reduction, ignore_index.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_output_nll_loss2d_forward_out(const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  auto wrapper_XLA_output_nll_loss2d_forward_out_tmp = wrapper_XLA__nll_loss2d_forward(self, target, weight, reduction, ignore_index);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_output_nll_loss2d_forward_out_tmp), output);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_output_nll_loss2d_forward_out_tmp), total_weight);
  return ::std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);
}
namespace {
at::Tensor wrapper_XLA__nll_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::nll_loss_backward(grad_output, self, target, weight, reduction, ignore_index.guard_int(__FILE__, __LINE__), total_weight);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_nll_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_nll_loss_backward_out_tmp = wrapper_XLA__nll_loss_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_nll_loss_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__nll_loss_forward(const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::nll_loss_forward(self, target, weight, reduction, ignore_index.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_output_nll_loss_forward_out(const at::Tensor & self, const at::Tensor & target, const ::std::optional<at::Tensor> & weight, int64_t reduction, c10::SymInt ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  auto wrapper_XLA_output_nll_loss_forward_out_tmp = wrapper_XLA__nll_loss_forward(self, target, weight, reduction, ignore_index);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_output_nll_loss_forward_out_tmp), output);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_output_nll_loss_forward_out_tmp), total_weight);
  return ::std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);
}
namespace {
at::Tensor wrapper_XLA__nonzero(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::nonzero(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_nonzero_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_nonzero_out_tmp = wrapper_XLA__nonzero(self);
  at::_copy_from_and_resize(wrapper_XLA_out_nonzero_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_ScalarOpt_dtype_norm(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::ScalarType dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::norm(self, p, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_ScalarOpt_dtype_out_norm_out(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::ScalarType dtype, at::Tensor & out) {
  auto wrapper_XLA_ScalarOpt_dtype_out_norm_out_tmp = wrapper_XLA_ScalarOpt_dtype_norm(self, p, dtype);
  at::_copy_from_and_resize(wrapper_XLA_ScalarOpt_dtype_out_norm_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Scalar_norm(const at::Tensor & self, const at::Scalar & p) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::norm(self, p);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_norm_out(const at::Tensor & self, const at::Scalar & p, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_norm_out_tmp = wrapper_XLA_Scalar_norm(self, p);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_norm_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_ScalarOpt_dim_dtype_norm(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::norm(self, p, dim, keepdim, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_dtype_out_norm_out(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
  auto wrapper_XLA_dtype_out_norm_out_tmp = wrapper_XLA_ScalarOpt_dim_dtype_norm(self, p, dim, keepdim, dtype);
  at::_copy_from_and_resize(wrapper_XLA_dtype_out_norm_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_ScalarOpt_dim_norm(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::norm(self, p, dim, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_norm_out(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_norm_out_tmp = wrapper_XLA_ScalarOpt_dim_norm(self, p, dim, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_norm_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Tensor_float_normal(const at::Tensor & mean, double std, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::normal(mean, std, generator);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_float_out_normal_out(const at::Tensor & mean, double std, ::std::optional<at::Generator> generator, at::Tensor & out) {
  auto wrapper_XLA_Tensor_float_out_normal_out_tmp = wrapper_XLA_Tensor_float_normal(mean, std, generator);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_float_out_normal_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_float_Tensor_normal(double mean, const at::Tensor & std, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::normal(mean, std, generator);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_float_Tensor_out_normal_out(double mean, const at::Tensor & std, ::std::optional<at::Generator> generator, at::Tensor & out) {
  auto wrapper_XLA_float_Tensor_out_normal_out_tmp = wrapper_XLA_float_Tensor_normal(mean, std, generator);
  at::_copy_from_and_resize(wrapper_XLA_float_Tensor_out_normal_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Tensor_Tensor_normal(const at::Tensor & mean, const at::Tensor & std, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::normal(mean, std, generator);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_Tensor_out_normal_out(const at::Tensor & mean, const at::Tensor & std, ::std::optional<at::Generator> generator, at::Tensor & out) {
  auto wrapper_XLA_Tensor_Tensor_out_normal_out_tmp = wrapper_XLA_Tensor_Tensor_normal(mean, std, generator);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_Tensor_out_normal_out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA__normal_(at::Tensor & self, double mean, double std, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::normal_(self, mean, std, generator);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__permute(const at::Tensor & self, at::IntArrayRef dims) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::permute(self, dims);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__permute_copy(const at::Tensor & self, at::IntArrayRef dims) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::permute_copy(self, dims);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_permute_copy_out(const at::Tensor & self, at::IntArrayRef dims, at::Tensor & out) {
  auto wrapper_XLA_out_permute_copy_out_tmp = wrapper_XLA__permute_copy(self, dims);
  at::_copy_from_and_resize(wrapper_XLA_out_permute_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__pixel_shuffle(const at::Tensor & self, int64_t upscale_factor) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::pixel_shuffle(self, upscale_factor);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_pixel_shuffle_out(const at::Tensor & self, int64_t upscale_factor, at::Tensor & out) {
  auto wrapper_XLA_out_pixel_shuffle_out_tmp = wrapper_XLA__pixel_shuffle(self, upscale_factor);
  at::_copy_from_and_resize(wrapper_XLA_out_pixel_shuffle_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__pixel_unshuffle(const at::Tensor & self, int64_t downscale_factor) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::pixel_unshuffle(self, downscale_factor);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_pixel_unshuffle_out(const at::Tensor & self, int64_t downscale_factor, at::Tensor & out) {
  auto wrapper_XLA_out_pixel_unshuffle_out_tmp = wrapper_XLA__pixel_unshuffle(self, downscale_factor);
  at::_copy_from_and_resize(wrapper_XLA_out_pixel_unshuffle_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Tensor_Tensor_pow(const at::Tensor & self, const at::Tensor & exponent) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::pow(self, exponent);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_Tensor_out_pow_out(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
  auto wrapper_XLA_Tensor_Tensor_out_pow_out_tmp = wrapper_XLA_Tensor_Tensor_pow(self, exponent);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_Tensor_out_pow_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_pow_(at::Tensor & self, const at::Tensor & exponent) {
  auto wrapper_XLA_Tensor_pow__tmp = wrapper_XLA_Tensor_Tensor_pow(self, exponent);
  at::_copy_from(wrapper_XLA_Tensor_pow__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Scalar_pow(const at::Scalar & self, const at::Tensor & exponent) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::pow(self, exponent);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_pow_out(const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_pow_out_tmp = wrapper_XLA_Scalar_pow(self, exponent);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_pow_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Tensor_Scalar_pow(const at::Tensor & self, const at::Scalar & exponent) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::pow(self, exponent);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_Scalar_out_pow_out(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
  auto wrapper_XLA_Tensor_Scalar_out_pow_out_tmp = wrapper_XLA_Tensor_Scalar_pow(self, exponent);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_Scalar_out_pow_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_pow_(at::Tensor & self, const at::Scalar & exponent) {
  auto wrapper_XLA_Scalar_pow__tmp = wrapper_XLA_Tensor_Scalar_pow(self, exponent);
  at::_copy_from(wrapper_XLA_Scalar_pow__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__prod(const at::Tensor & self, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::prod(self, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_prod_out(const at::Tensor & self, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_out_prod_out_tmp = wrapper_XLA__prod(self, dtype);
  at::_copy_from_and_resize(wrapper_XLA_out_prod_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_dim_int_prod(const at::Tensor & self, int64_t dim, bool keepdim, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::prod(self, dim, keepdim, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_int_out_prod_out(const at::Tensor & self, int64_t dim, bool keepdim, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_int_out_prod_out_tmp = wrapper_XLA_dim_int_prod(self, dim, keepdim, dtype);
  at::_copy_from_and_resize(wrapper_XLA_int_out_prod_out_tmp, out);
  return out;
}
namespace {
at::Tensor & wrapper_XLA__put_(at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::put_(self, index, source, accumulate);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__qr(const at::Tensor & self, bool some) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::qr(self, some);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_Q_qr_out(const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
  auto wrapper_XLA_Q_qr_out_tmp = wrapper_XLA__qr(self, some);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_Q_qr_out_tmp), Q);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_Q_qr_out_tmp), R);
  return ::std::tuple<at::Tensor &,at::Tensor &>(Q, R);
}
namespace {
at::Tensor & wrapper_XLA_from_random_(at::Tensor & self, int64_t from, ::std::optional<int64_t> to, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::random_(self, from, to, generator);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA_to_random_(at::Tensor & self, int64_t to, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::random_(self, to, generator);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA__random_(at::Tensor & self, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::random_(self, generator);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__randperm(c10::SymInt n, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::randperm(n.guard_int(__FILE__, __LINE__), dtype, layout, device, pin_memory);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_randperm_out(c10::SymInt n, at::Tensor & out) {
  auto wrapper_XLA_out_randperm_out_tmp = wrapper_XLA__randperm(n, out.scalar_type(), out.layout(), out.device(), ::std::nullopt);
  at::_copy_from_and_resize(wrapper_XLA_out_randperm_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__reciprocal(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::reciprocal(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_reciprocal_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_reciprocal_out_tmp = wrapper_XLA__reciprocal(self);
  at::_copy_from_and_resize(wrapper_XLA_out_reciprocal_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__reciprocal_(at::Tensor & self) {
  auto wrapper_XLA__reciprocal__tmp = wrapper_XLA__reciprocal(self);
  at::_copy_from(wrapper_XLA__reciprocal__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__reflection_pad1d(const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::reflection_pad1d(self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_reflection_pad1d_out(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto wrapper_XLA_out_reflection_pad1d_out_tmp = wrapper_XLA__reflection_pad1d(self, padding);
  at::_copy_from_and_resize(wrapper_XLA_out_reflection_pad1d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__reflection_pad1d_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::reflection_pad1d_backward(grad_output, self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_reflection_pad1d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_reflection_pad1d_backward_out_tmp = wrapper_XLA__reflection_pad1d_backward(grad_output, self, padding);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_reflection_pad1d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__reflection_pad2d(const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::reflection_pad2d(self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_reflection_pad2d_out(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto wrapper_XLA_out_reflection_pad2d_out_tmp = wrapper_XLA__reflection_pad2d(self, padding);
  at::_copy_from_and_resize(wrapper_XLA_out_reflection_pad2d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__reflection_pad2d_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::reflection_pad2d_backward(grad_output, self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_reflection_pad2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_reflection_pad2d_backward_out_tmp = wrapper_XLA__reflection_pad2d_backward(grad_output, self, padding);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_reflection_pad2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__reflection_pad3d(const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::reflection_pad3d(self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_reflection_pad3d_out(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto wrapper_XLA_out_reflection_pad3d_out_tmp = wrapper_XLA__reflection_pad3d(self, padding);
  at::_copy_from_and_resize(wrapper_XLA_out_reflection_pad3d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__reflection_pad3d_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::reflection_pad3d_backward(grad_output, self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_reflection_pad3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_reflection_pad3d_backward_out_tmp = wrapper_XLA__reflection_pad3d_backward(grad_output, self, padding);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_reflection_pad3d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__relu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::relu(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_relu_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_relu_out_tmp = wrapper_XLA__relu(self);
  at::_copy_from_and_resize(wrapper_XLA_out_relu_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__relu_(at::Tensor & self) {
  auto wrapper_XLA__relu__tmp = wrapper_XLA__relu(self);
  at::_copy_from(wrapper_XLA__relu__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Scalar_remainder(const at::Tensor & self, const at::Scalar & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::remainder(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_remainder_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_remainder_out_tmp = wrapper_XLA_Scalar_remainder(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_remainder_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_remainder_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_XLA_Scalar_remainder__tmp = wrapper_XLA_Scalar_remainder(self, other);
  at::_copy_from(wrapper_XLA_Scalar_remainder__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_remainder(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::remainder(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_remainder_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_remainder_out_tmp = wrapper_XLA_Tensor_remainder(self, other);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_remainder_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_remainder_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_remainder__tmp = wrapper_XLA_Tensor_remainder(self, other);
  at::_copy_from(wrapper_XLA_Tensor_remainder__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__repeat(const at::Tensor & self, c10::SymIntArrayRef repeats) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::repeat(self, C10_AS_INTARRAYREF_SLOW(repeats));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_repeat_out(const at::Tensor & self, c10::SymIntArrayRef repeats, at::Tensor & out) {
  auto wrapper_XLA_out_repeat_out_tmp = wrapper_XLA__repeat(self, repeats);
  at::_copy_from_and_resize(wrapper_XLA_out_repeat_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__replication_pad1d(const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::replication_pad1d(self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_replication_pad1d_out(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto wrapper_XLA_out_replication_pad1d_out_tmp = wrapper_XLA__replication_pad1d(self, padding);
  at::_copy_from_and_resize(wrapper_XLA_out_replication_pad1d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__replication_pad1d_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::replication_pad1d_backward(grad_output, self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_replication_pad1d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_replication_pad1d_backward_out_tmp = wrapper_XLA__replication_pad1d_backward(grad_output, self, padding);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_replication_pad1d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__replication_pad2d(const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::replication_pad2d(self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_replication_pad2d_out(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto wrapper_XLA_out_replication_pad2d_out_tmp = wrapper_XLA__replication_pad2d(self, padding);
  at::_copy_from_and_resize(wrapper_XLA_out_replication_pad2d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__replication_pad2d_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::replication_pad2d_backward(grad_output, self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_replication_pad2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_replication_pad2d_backward_out_tmp = wrapper_XLA__replication_pad2d_backward(grad_output, self, padding);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_replication_pad2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__replication_pad3d(const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::replication_pad3d(self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_replication_pad3d_out(const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & out) {
  auto wrapper_XLA_out_replication_pad3d_out_tmp = wrapper_XLA__replication_pad3d(self, padding);
  at::_copy_from_and_resize(wrapper_XLA_out_replication_pad3d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__replication_pad3d_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::replication_pad3d_backward(grad_output, self, C10_AS_INTARRAYREF_SLOW(padding));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_replication_pad3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::SymIntArrayRef padding, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_replication_pad3d_backward_out_tmp = wrapper_XLA__replication_pad3d_backward(grad_output, self, padding);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_replication_pad3d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
const at::Tensor & wrapper_XLA__resize_(const at::Tensor & self, c10::SymIntArrayRef size, ::std::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::resize_(self, C10_AS_INTARRAYREF_SLOW(size), memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__roll(const at::Tensor & self, c10::SymIntArrayRef shifts, at::IntArrayRef dims) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::roll(self, C10_AS_INTARRAYREF_SLOW(shifts), dims);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_roll_out(const at::Tensor & self, c10::SymIntArrayRef shifts, at::IntArrayRef dims, at::Tensor & out) {
  auto wrapper_XLA_out_roll_out_tmp = wrapper_XLA__roll(self, shifts, dims);
  at::_copy_from_and_resize(wrapper_XLA_out_roll_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__round(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::round(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_round_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_round_out_tmp = wrapper_XLA__round(self);
  at::_copy_from_and_resize(wrapper_XLA_out_round_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__round_(at::Tensor & self) {
  auto wrapper_XLA__round__tmp = wrapper_XLA__round(self);
  at::_copy_from(wrapper_XLA__round__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__rrelu_with_noise_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::rrelu_with_noise_backward(grad_output, self, noise, lower, upper, training, self_is_result);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_rrelu_with_noise_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result, at::Tensor & out) {
  auto wrapper_XLA_out_rrelu_with_noise_backward_out_tmp = wrapper_XLA__rrelu_with_noise_backward(grad_output, self, noise, lower, upper, training, self_is_result);
  at::_copy_from_and_resize(wrapper_XLA_out_rrelu_with_noise_backward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__rrelu_with_noise(const at::Tensor & self, at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::rrelu_with_noise(self, noise, lower, upper, training, generator);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__rsqrt(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::rsqrt(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_rsqrt_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_rsqrt_out_tmp = wrapper_XLA__rsqrt(self);
  at::_copy_from_and_resize(wrapper_XLA_out_rsqrt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__rsqrt_(at::Tensor & self) {
  auto wrapper_XLA__rsqrt__tmp = wrapper_XLA__rsqrt(self);
  at::_copy_from(wrapper_XLA__rsqrt__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_rsub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::rsub(self, other, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_rsub_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_rsub_out_tmp = wrapper_XLA_Tensor_rsub(self, other, alpha);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_rsub_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Scalar_rsub(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::rsub(self, other, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_rsub_out(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_rsub_out_tmp = wrapper_XLA_Scalar_rsub(self, other, alpha);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_rsub_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_src_scatter(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::scatter(self, dim, index, src);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_src_out_scatter_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
  auto wrapper_XLA_src_out_scatter_out_tmp = wrapper_XLA_src_scatter(self, dim, index, src);
  at::_copy_from_and_resize(wrapper_XLA_src_out_scatter_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_src_scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  auto wrapper_XLA_src_scatter__tmp = wrapper_XLA_src_scatter(self, dim, index, src);
  at::_copy_from(wrapper_XLA_src_scatter__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_value_scatter(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::scatter(self, dim, index, value);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_value_out_scatter_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_XLA_value_out_scatter_out_tmp = wrapper_XLA_value_scatter(self, dim, index, value);
  at::_copy_from_and_resize(wrapper_XLA_value_out_scatter_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_value_scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
  auto wrapper_XLA_value_scatter__tmp = wrapper_XLA_value_scatter(self, dim, index, value);
  at::_copy_from(wrapper_XLA_value_scatter__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_reduce_scatter(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::scatter(self, dim, index, src, reduce);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_reduce_out_scatter_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, at::Tensor & out) {
  auto wrapper_XLA_reduce_out_scatter_out_tmp = wrapper_XLA_reduce_scatter(self, dim, index, src, reduce);
  at::_copy_from_and_resize(wrapper_XLA_reduce_out_scatter_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_reduce_scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
  auto wrapper_XLA_reduce_scatter__tmp = wrapper_XLA_reduce_scatter(self, dim, index, src, reduce);
  at::_copy_from(wrapper_XLA_reduce_scatter__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_value_reduce_scatter(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::scatter(self, dim, index, value, reduce);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_value_reduce_out_scatter_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce, at::Tensor & out) {
  auto wrapper_XLA_value_reduce_out_scatter_out_tmp = wrapper_XLA_value_reduce_scatter(self, dim, index, value, reduce);
  at::_copy_from_and_resize(wrapper_XLA_value_reduce_out_scatter_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_value_reduce_scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
  auto wrapper_XLA_value_reduce_scatter__tmp = wrapper_XLA_value_reduce_scatter(self, dim, index, value, reduce);
  at::_copy_from(wrapper_XLA_value_reduce_scatter__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__scatter_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::scatter_add(self, dim, index, src);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_scatter_add_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
  auto wrapper_XLA_out_scatter_add_out_tmp = wrapper_XLA__scatter_add(self, dim, index, src);
  at::_copy_from_and_resize(wrapper_XLA_out_scatter_add_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__scatter_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  auto wrapper_XLA__scatter_add__tmp = wrapper_XLA__scatter_add(self, dim, index, src);
  at::_copy_from(wrapper_XLA__scatter_add__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_two_scatter_reduce(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, bool include_self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::scatter_reduce(self, dim, index, src, reduce, include_self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_two_out_scatter_reduce_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, bool include_self, at::Tensor & out) {
  auto wrapper_XLA_two_out_scatter_reduce_out_tmp = wrapper_XLA_two_scatter_reduce(self, dim, index, src, reduce, include_self);
  at::_copy_from_and_resize(wrapper_XLA_two_out_scatter_reduce_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_two_scatter_reduce_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, bool include_self) {
  auto wrapper_XLA_two_scatter_reduce__tmp = wrapper_XLA_two_scatter_reduce(self, dim, index, src, reduce, include_self);
  at::_copy_from(wrapper_XLA_two_scatter_reduce__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_int_select(const at::Tensor & self, int64_t dim, c10::SymInt index) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::select_symint(self, dim, index);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__select_backward(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::select_backward_symint(grad_output, input_sizes, dim, index);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_select_backward_out(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt index, at::Tensor & out) {
  auto wrapper_XLA_out_select_backward_out_tmp = wrapper_XLA__select_backward(grad_output, input_sizes, dim, index);
  at::_copy_from_and_resize(wrapper_XLA_out_select_backward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_int_select_copy(const at::Tensor & self, int64_t dim, c10::SymInt index) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::select_copy(self, dim, index.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_int_out_select_copy_out(const at::Tensor & self, int64_t dim, c10::SymInt index, at::Tensor & out) {
  auto wrapper_XLA_int_out_select_copy_out_tmp = wrapper_XLA_int_select_copy(self, dim, index);
  at::_copy_from_and_resize(wrapper_XLA_int_out_select_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__select_scatter(const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::SymInt index) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::select_scatter(self, src, dim, index.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_select_scatter_out(const at::Tensor & self, const at::Tensor & src, int64_t dim, c10::SymInt index, at::Tensor & out) {
  auto wrapper_XLA_out_select_scatter_out_tmp = wrapper_XLA__select_scatter(self, src, dim, index);
  at::_copy_from_and_resize(wrapper_XLA_out_select_scatter_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__selu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::selu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA__selu_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::selu_(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_XLA_source_Tensor_set_(at::Tensor & self, const at::Tensor & source) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::set_(self, source);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__sgn(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sgn(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_sgn_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_sgn_out_tmp = wrapper_XLA__sgn(self);
  at::_copy_from_and_resize(wrapper_XLA_out_sgn_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__sgn_(at::Tensor & self) {
  auto wrapper_XLA__sgn__tmp = wrapper_XLA__sgn(self);
  at::_copy_from(wrapper_XLA__sgn__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__sigmoid(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sigmoid(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_sigmoid_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_sigmoid_out_tmp = wrapper_XLA__sigmoid(self);
  at::_copy_from_and_resize(wrapper_XLA_out_sigmoid_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__sigmoid_(at::Tensor & self) {
  auto wrapper_XLA__sigmoid__tmp = wrapper_XLA__sigmoid(self);
  at::_copy_from(wrapper_XLA__sigmoid__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & output) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sigmoid_backward(grad_output, output);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_sigmoid_backward_out(const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_sigmoid_backward_out_tmp = wrapper_XLA__sigmoid_backward(grad_output, output);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_sigmoid_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__sign(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sign(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_sign_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_sign_out_tmp = wrapper_XLA__sign(self);
  at::_copy_from_and_resize(wrapper_XLA_out_sign_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__sign_(at::Tensor & self) {
  auto wrapper_XLA__sign__tmp = wrapper_XLA__sign(self);
  at::_copy_from(wrapper_XLA__sign__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__silu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::silu(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_silu_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_silu_out_tmp = wrapper_XLA__silu(self);
  at::_copy_from_and_resize(wrapper_XLA_out_silu_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__silu_(at::Tensor & self) {
  auto wrapper_XLA__silu__tmp = wrapper_XLA__silu(self);
  at::_copy_from(wrapper_XLA__silu__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::silu_backward(grad_output, self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_silu_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_silu_backward_out_tmp = wrapper_XLA__silu_backward(grad_output, self);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_silu_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__sin(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sin(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_sin_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_sin_out_tmp = wrapper_XLA__sin(self);
  at::_copy_from_and_resize(wrapper_XLA_out_sin_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__sin_(at::Tensor & self) {
  auto wrapper_XLA__sin__tmp = wrapper_XLA__sin(self);
  at::_copy_from(wrapper_XLA__sin__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__sinh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sinh(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_sinh_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_sinh_out_tmp = wrapper_XLA__sinh(self);
  at::_copy_from_and_resize(wrapper_XLA_out_sinh_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__sinh_(at::Tensor & self) {
  auto wrapper_XLA__sinh__tmp = wrapper_XLA__sinh(self);
  at::_copy_from(wrapper_XLA__sinh__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Tensor_slice(const at::Tensor & self, int64_t dim, ::std::optional<c10::SymInt> start, ::std::optional<c10::SymInt> end, c10::SymInt step) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::slice(self, dim, start.has_value() ? ::std::make_optional(start->guard_int(__FILE__, __LINE__)) : ::std::nullopt, end.has_value() ? ::std::make_optional(end->guard_int(__FILE__, __LINE__)) : ::std::nullopt, step.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__slice_backward(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::slice_backward(grad_output, C10_AS_INTARRAYREF_SLOW(input_sizes), dim, start.guard_int(__FILE__, __LINE__), end.guard_int(__FILE__, __LINE__), step.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_slice_backward_out(const at::Tensor & grad_output, c10::SymIntArrayRef input_sizes, int64_t dim, c10::SymInt start, c10::SymInt end, c10::SymInt step, at::Tensor & out) {
  auto wrapper_XLA_out_slice_backward_out_tmp = wrapper_XLA__slice_backward(grad_output, input_sizes, dim, start, end, step);
  at::_copy_from_and_resize(wrapper_XLA_out_slice_backward_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Tensor_slice_copy(const at::Tensor & self, int64_t dim, ::std::optional<c10::SymInt> start, ::std::optional<c10::SymInt> end, c10::SymInt step) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::slice_copy(self, dim, start.has_value() ? ::std::make_optional(start->guard_int(__FILE__, __LINE__)) : ::std::nullopt, end.has_value() ? ::std::make_optional(end->guard_int(__FILE__, __LINE__)) : ::std::nullopt, step.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Tensor_out_slice_copy_out(const at::Tensor & self, int64_t dim, ::std::optional<c10::SymInt> start, ::std::optional<c10::SymInt> end, c10::SymInt step, at::Tensor & out) {
  auto wrapper_XLA_Tensor_out_slice_copy_out_tmp = wrapper_XLA_Tensor_slice_copy(self, dim, start, end, step);
  at::_copy_from_and_resize(wrapper_XLA_Tensor_out_slice_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__slice_scatter(const at::Tensor & self, const at::Tensor & src, int64_t dim, ::std::optional<c10::SymInt> start, ::std::optional<c10::SymInt> end, c10::SymInt step) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::slice_scatter(self, src, dim, start.has_value() ? ::std::make_optional(start->guard_int(__FILE__, __LINE__)) : ::std::nullopt, end.has_value() ? ::std::make_optional(end->guard_int(__FILE__, __LINE__)) : ::std::nullopt, step.guard_int(__FILE__, __LINE__));
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_slice_scatter_out(const at::Tensor & self, const at::Tensor & src, int64_t dim, ::std::optional<c10::SymInt> start, ::std::optional<c10::SymInt> end, c10::SymInt step, at::Tensor & out) {
  auto wrapper_XLA_out_slice_scatter_out_tmp = wrapper_XLA__slice_scatter(self, src, dim, start, end, step);
  at::_copy_from_and_resize(wrapper_XLA_out_slice_scatter_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__smooth_l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::smooth_l1_loss(self, target, reduction, beta);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_smooth_l1_loss_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & out) {
  auto wrapper_XLA_out_smooth_l1_loss_out_tmp = wrapper_XLA__smooth_l1_loss(self, target, reduction, beta);
  at::_copy_from_and_resize(wrapper_XLA_out_smooth_l1_loss_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__smooth_l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::smooth_l1_loss_backward(grad_output, self, target, reduction, beta);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_smooth_l1_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_smooth_l1_loss_backward_out_tmp = wrapper_XLA__smooth_l1_loss_backward(grad_output, self, target, reduction, beta);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_smooth_l1_loss_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__softplus(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::softplus(self, beta, threshold);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_softplus_out(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & out) {
  auto wrapper_XLA_out_softplus_out_tmp = wrapper_XLA__softplus(self, beta, threshold);
  at::_copy_from_and_resize(wrapper_XLA_out_softplus_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__softplus_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::softplus_backward(grad_output, self, beta, threshold);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_softplus_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_softplus_backward_out_tmp = wrapper_XLA__softplus_backward(grad_output, self, beta, threshold);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_softplus_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__softshrink(const at::Tensor & self, const at::Scalar & lambd) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::softshrink(self, lambd);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_softshrink_out(const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
  auto wrapper_XLA_out_softshrink_out_tmp = wrapper_XLA__softshrink(self, lambd);
  at::_copy_from_and_resize(wrapper_XLA_out_softshrink_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__softshrink_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::softshrink_backward(grad_output, self, lambd);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_softshrink_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_softshrink_backward_out_tmp = wrapper_XLA__softshrink_backward(grad_output, self, lambd);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_softshrink_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__sort(const at::Tensor & self, int64_t dim, bool descending) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sort(self, dim, descending);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_values_sort_out(const at::Tensor & self, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  auto wrapper_XLA_values_sort_out_tmp = wrapper_XLA__sort(self, dim, descending);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_values_sort_out_tmp), values);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_values_sort_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA_stable_sort(const at::Tensor & self, ::std::optional<bool> stable, int64_t dim, bool descending) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sort(self, stable, dim, descending);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_values_stable_sort_out(const at::Tensor & self, ::std::optional<bool> stable, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  auto wrapper_XLA_values_stable_sort_out_tmp = wrapper_XLA_stable_sort(self, stable, dim, descending);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_values_stable_sort_out_tmp), values);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_values_stable_sort_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}
namespace {
::std::vector<at::Tensor> wrapper_XLA_Tensor_split_copy(const at::Tensor & self, c10::SymInt split_size, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::split_copy(self, split_size.guard_int(__FILE__, __LINE__), dim);
}
} // anonymous namespace
void wrapper_XLA_Tensor_out_split_copy_out(const at::Tensor & self, c10::SymInt split_size, int64_t dim, at::TensorList out) {
  auto wrapper_XLA_Tensor_out_split_copy_out_tmp = wrapper_XLA_Tensor_split_copy(self, split_size, dim);
      for (int64_t i = 0; i < wrapper_XLA_Tensor_out_split_copy_out_tmp.size(); ++i) {
        at::_copy_from_and_resize(wrapper_XLA_Tensor_out_split_copy_out_tmp[i], out[i]);
    }
  return ;
}
namespace {
::std::vector<at::Tensor> wrapper_XLA__split_with_sizes_copy(const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::split_with_sizes_copy(self, C10_AS_INTARRAYREF_SLOW(split_sizes), dim);
}
} // anonymous namespace
void wrapper_XLA_out_split_with_sizes_copy_out(const at::Tensor & self, c10::SymIntArrayRef split_sizes, int64_t dim, at::TensorList out) {
  auto wrapper_XLA_out_split_with_sizes_copy_out_tmp = wrapper_XLA__split_with_sizes_copy(self, split_sizes, dim);
      for (int64_t i = 0; i < wrapper_XLA_out_split_with_sizes_copy_out_tmp.size(); ++i) {
        at::_copy_from_and_resize(wrapper_XLA_out_split_with_sizes_copy_out_tmp[i], out[i]);
    }
  return ;
}
namespace {
at::Tensor wrapper_XLA__sqrt(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sqrt(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_sqrt_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_sqrt_out_tmp = wrapper_XLA__sqrt(self);
  at::_copy_from_and_resize(wrapper_XLA_out_sqrt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__sqrt_(at::Tensor & self) {
  auto wrapper_XLA__sqrt__tmp = wrapper_XLA__sqrt(self);
  at::_copy_from(wrapper_XLA__sqrt__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__squeeze_copy(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::squeeze_copy(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_squeeze_copy_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_squeeze_copy_out_tmp = wrapper_XLA__squeeze_copy(self);
  at::_copy_from_and_resize(wrapper_XLA_out_squeeze_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_dim_squeeze_copy(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::squeeze_copy(self, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_dim_out_squeeze_copy_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
  auto wrapper_XLA_dim_out_squeeze_copy_out_tmp = wrapper_XLA_dim_squeeze_copy(self, dim);
  at::_copy_from_and_resize(wrapper_XLA_dim_out_squeeze_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_dims_squeeze_copy(const at::Tensor & self, at::IntArrayRef dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::squeeze_copy(self, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_dims_out_squeeze_copy_out(const at::Tensor & self, at::IntArrayRef dim, at::Tensor & out) {
  auto wrapper_XLA_dims_out_squeeze_copy_out_tmp = wrapper_XLA_dims_squeeze_copy(self, dim);
  at::_copy_from_and_resize(wrapper_XLA_dims_out_squeeze_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__stack(at::TensorList tensors, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::stack(tensors, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_stack_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
  auto wrapper_XLA_out_stack_out_tmp = wrapper_XLA__stack(tensors, dim);
  at::_copy_from_and_resize(wrapper_XLA_out_stack_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__std(const at::Tensor & self, bool unbiased) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::std(self, unbiased);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA_dim_std(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::std(self, dim, unbiased, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_std_out(const at::Tensor & self, at::OptionalIntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_out_std_out_tmp = wrapper_XLA_dim_std(self, dim, unbiased, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_out_std_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_correction_std(const at::Tensor & self, at::OptionalIntArrayRef dim, const ::std::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::std(self, dim, correction, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_correction_out_std_out(const at::Tensor & self, at::OptionalIntArrayRef dim, const ::std::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_correction_out_std_out_tmp = wrapper_XLA_correction_std(self, dim, correction, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_correction_out_std_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA_correction_std_mean(const at::Tensor & self, at::OptionalIntArrayRef dim, const ::std::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::std_mean(self, dim, correction, keepdim);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_correction_out_std_mean_out(const at::Tensor & self, at::OptionalIntArrayRef dim, const ::std::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out0, at::Tensor & out1) {
  auto wrapper_XLA_correction_out_std_mean_out_tmp = wrapper_XLA_correction_std_mean(self, dim, correction, keepdim);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_correction_out_std_mean_out_tmp), out0);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_correction_out_std_mean_out_tmp), out1);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
}
namespace {
at::Tensor wrapper_XLA_Tensor_sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sub(self, other, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_sub_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_out_sub_out_tmp = wrapper_XLA_Tensor_sub(self, other, alpha);
  at::_copy_from_and_resize(wrapper_XLA_out_sub_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_sub_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  auto wrapper_XLA_Tensor_sub__tmp = wrapper_XLA_Tensor_sub(self, other, alpha);
  at::_copy_from(wrapper_XLA_Tensor_sub__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA_Scalar_sub(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sub(self, other, alpha);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_Scalar_out_sub_out(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_XLA_Scalar_out_sub_out_tmp = wrapper_XLA_Scalar_sub(self, other, alpha);
  at::_copy_from_and_resize(wrapper_XLA_Scalar_out_sub_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Scalar_sub_(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  auto wrapper_XLA_Scalar_sub__tmp = wrapper_XLA_Scalar_sub(self, other, alpha);
  at::_copy_from(wrapper_XLA_Scalar_sub__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__sum(const at::Tensor & self, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sum(self, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_sum_out(const at::Tensor & self, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_out_sum_out_tmp = wrapper_XLA__sum(self, dtype);
  at::_copy_from_and_resize(wrapper_XLA_out_sum_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_dim_IntList_sum(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, ::std::optional<at::ScalarType> dtype) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::sum(self, dim, keepdim, dtype);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_IntList_out_sum_out(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim, ::std::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_XLA_IntList_out_sum_out_tmp = wrapper_XLA_dim_IntList_sum(self, dim, keepdim, dtype);
  at::_copy_from_and_resize(wrapper_XLA_IntList_out_sum_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_XLA__svd(const at::Tensor & self, bool some, bool compute_uv) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::svd(self, some, compute_uv);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_XLA_U_svd_out(const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
  auto wrapper_XLA_U_svd_out_tmp = wrapper_XLA__svd(self, some, compute_uv);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_U_svd_out_tmp), U);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_U_svd_out_tmp), S);
  at::_copy_from_and_resize(std::get<2>(wrapper_XLA_U_svd_out_tmp), V);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(U, S, V);
}
namespace {
at::Tensor wrapper_XLA__t(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::t(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__t_copy(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::t_copy(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_t_copy_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_t_copy_out_tmp = wrapper_XLA__t_copy(self);
  at::_copy_from_and_resize(wrapper_XLA_out_t_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__take(const at::Tensor & self, const at::Tensor & index) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::take(self, index);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_take_out(const at::Tensor & self, const at::Tensor & index, at::Tensor & out) {
  auto wrapper_XLA_out_take_out_tmp = wrapper_XLA__take(self, index);
  at::_copy_from_and_resize(wrapper_XLA_out_take_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__tan(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::tan(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_tan_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_tan_out_tmp = wrapper_XLA__tan(self);
  at::_copy_from_and_resize(wrapper_XLA_out_tan_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__tan_(at::Tensor & self) {
  auto wrapper_XLA__tan__tmp = wrapper_XLA__tan(self);
  at::_copy_from(wrapper_XLA__tan__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__tanh(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::tanh(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_tanh_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_tanh_out_tmp = wrapper_XLA__tanh(self);
  at::_copy_from_and_resize(wrapper_XLA_out_tanh_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__tanh_(at::Tensor & self) {
  auto wrapper_XLA__tanh__tmp = wrapper_XLA__tanh(self);
  at::_copy_from(wrapper_XLA__tanh__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__tanh_backward(const at::Tensor & grad_output, const at::Tensor & output) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::tanh_backward(grad_output, output);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_tanh_backward_out(const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_tanh_backward_out_tmp = wrapper_XLA__tanh_backward(grad_output, output);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_tanh_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__threshold(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::threshold(self, threshold, value);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_threshold_out(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_XLA_out_threshold_out_tmp = wrapper_XLA__threshold(self, threshold, value);
  at::_copy_from_and_resize(wrapper_XLA_out_threshold_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__threshold_(at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
  auto wrapper_XLA__threshold__tmp = wrapper_XLA__threshold(self, threshold, value);
  at::_copy_from(wrapper_XLA__threshold__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::threshold_backward(grad_output, self, threshold);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_threshold_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_threshold_backward_out_tmp = wrapper_XLA__threshold_backward(grad_output, self, threshold);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_threshold_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__topk(const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::topk(self, k.guard_int(__FILE__, __LINE__), dim, largest, sorted);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_values_topk_out(const at::Tensor & self, c10::SymInt k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
  auto wrapper_XLA_values_topk_out_tmp = wrapper_XLA__topk(self, k, dim, largest, sorted);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_values_topk_out_tmp), values);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_values_topk_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}
namespace {
at::Tensor wrapper_XLA__trace(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::trace(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_trace_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_trace_out_tmp = wrapper_XLA__trace(self);
  at::_copy_from_and_resize(wrapper_XLA_out_trace_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_int_transpose_copy(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::transpose_copy(self, dim0, dim1);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_int_out_transpose_copy_out(const at::Tensor & self, int64_t dim0, int64_t dim1, at::Tensor & out) {
  auto wrapper_XLA_int_out_transpose_copy_out_tmp = wrapper_XLA_int_transpose_copy(self, dim0, dim1);
  at::_copy_from_and_resize(wrapper_XLA_int_out_transpose_copy_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA__triangular_solve(const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::triangular_solve(self, A, upper, transpose, unitriangular);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_X_triangular_solve_out(const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular, at::Tensor & X, at::Tensor & M) {
  auto wrapper_XLA_X_triangular_solve_out_tmp = wrapper_XLA__triangular_solve(self, A, upper, transpose, unitriangular);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_X_triangular_solve_out_tmp), X);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_X_triangular_solve_out_tmp), M);
  return ::std::tuple<at::Tensor &,at::Tensor &>(X, M);
}
namespace {
at::Tensor wrapper_XLA__tril(const at::Tensor & self, int64_t diagonal) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::tril(self, diagonal);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_tril_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  auto wrapper_XLA_out_tril_out_tmp = wrapper_XLA__tril(self, diagonal);
  at::_copy_from_and_resize(wrapper_XLA_out_tril_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__tril_(at::Tensor & self, int64_t diagonal) {
  auto wrapper_XLA__tril__tmp = wrapper_XLA__tril(self, diagonal);
  at::_copy_from(wrapper_XLA__tril__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__triu(const at::Tensor & self, int64_t diagonal) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::triu(self, diagonal);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_triu_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  auto wrapper_XLA_out_triu_out_tmp = wrapper_XLA__triu(self, diagonal);
  at::_copy_from_and_resize(wrapper_XLA_out_triu_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__triu_(at::Tensor & self, int64_t diagonal) {
  auto wrapper_XLA__triu__tmp = wrapper_XLA__triu(self, diagonal);
  at::_copy_from(wrapper_XLA__triu__tmp, self);
  return self;
}
namespace {
at::Tensor wrapper_XLA__trunc(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::trunc(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_trunc_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_trunc_out_tmp = wrapper_XLA__trunc(self);
  at::_copy_from_and_resize(wrapper_XLA_out_trunc_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA__trunc_(at::Tensor & self) {
  auto wrapper_XLA__trunc__tmp = wrapper_XLA__trunc(self);
  at::_copy_from(wrapper_XLA__trunc__tmp, self);
  return self;
}
namespace {
::std::vector<at::Tensor> wrapper_XLA_int_unbind_copy(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::unbind_copy(self, dim);
}
} // anonymous namespace
void wrapper_XLA_int_out_unbind_copy_out(const at::Tensor & self, int64_t dim, at::TensorList out) {
  auto wrapper_XLA_int_out_unbind_copy_out_tmp = wrapper_XLA_int_unbind_copy(self, dim);
      for (int64_t i = 0; i < wrapper_XLA_int_out_unbind_copy_out_tmp.size(); ++i) {
        at::_copy_from_and_resize(wrapper_XLA_int_out_unbind_copy_out_tmp[i], out[i]);
    }
  return ;
}
namespace {
at::Tensor & wrapper_XLA__uniform_(at::Tensor & self, double from, double to, ::std::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::uniform_(self, from, to, generator);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__unsqueeze_copy(const at::Tensor & self, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::unsqueeze_copy(self, dim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_unsqueeze_copy_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
  auto wrapper_XLA_out_unsqueeze_copy_out_tmp = wrapper_XLA__unsqueeze_copy(self, dim);
  at::_copy_from_and_resize(wrapper_XLA_out_unsqueeze_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__upsample_bilinear2d(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, ::std::optional<double> scales_h, ::std::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::upsample_bilinear2d(self, C10_AS_INTARRAYREF_SLOW(output_size), align_corners, scales_h, scales_w);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_upsample_bilinear2d_out(const at::Tensor & self, c10::SymIntArrayRef output_size, bool align_corners, ::std::optional<double> scales_h, ::std::optional<double> scales_w, at::Tensor & out) {
  auto wrapper_XLA_out_upsample_bilinear2d_out_tmp = wrapper_XLA__upsample_bilinear2d(self, output_size, align_corners, scales_h, scales_w);
  at::_copy_from_and_resize(wrapper_XLA_out_upsample_bilinear2d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__upsample_bilinear2d_backward(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, ::std::optional<double> scales_h, ::std::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::upsample_bilinear2d_backward(grad_output, C10_AS_INTARRAYREF_SLOW(output_size), C10_AS_INTARRAYREF_SLOW(input_size), align_corners, scales_h, scales_w);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_upsample_bilinear2d_backward_out(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, bool align_corners, ::std::optional<double> scales_h, ::std::optional<double> scales_w, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_upsample_bilinear2d_backward_out_tmp = wrapper_XLA__upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_upsample_bilinear2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA__upsample_nearest2d(const at::Tensor & self, c10::SymIntArrayRef output_size, ::std::optional<double> scales_h, ::std::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::upsample_nearest2d(self, C10_AS_INTARRAYREF_SLOW(output_size), scales_h, scales_w);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_upsample_nearest2d_out(const at::Tensor & self, c10::SymIntArrayRef output_size, ::std::optional<double> scales_h, ::std::optional<double> scales_w, at::Tensor & out) {
  auto wrapper_XLA_out_upsample_nearest2d_out_tmp = wrapper_XLA__upsample_nearest2d(self, output_size, scales_h, scales_w);
  at::_copy_from_and_resize(wrapper_XLA_out_upsample_nearest2d_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__upsample_nearest2d_backward(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional<double> scales_h, ::std::optional<double> scales_w) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::upsample_nearest2d_backward(grad_output, C10_AS_INTARRAYREF_SLOW(output_size), C10_AS_INTARRAYREF_SLOW(input_size), scales_h, scales_w);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_grad_input_upsample_nearest2d_backward_out(const at::Tensor & grad_output, c10::SymIntArrayRef output_size, c10::SymIntArrayRef input_size, ::std::optional<double> scales_h, ::std::optional<double> scales_w, at::Tensor & grad_input) {
  auto wrapper_XLA_grad_input_upsample_nearest2d_backward_out_tmp = wrapper_XLA__upsample_nearest2d_backward(grad_output, output_size, input_size, scales_h, scales_w);
  at::_copy_from_and_resize(wrapper_XLA_grad_input_upsample_nearest2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {
at::Tensor wrapper_XLA_correction_var(const at::Tensor & self, at::OptionalIntArrayRef dim, const ::std::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::var(self, dim, correction, keepdim);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_correction_out_var_out(const at::Tensor & self, at::OptionalIntArrayRef dim, const ::std::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out) {
  auto wrapper_XLA_correction_out_var_out_tmp = wrapper_XLA_correction_var(self, dim, correction, keepdim);
  at::_copy_from_and_resize(wrapper_XLA_correction_out_var_out_tmp, out);
  return out;
}
namespace {
::std::tuple<at::Tensor,at::Tensor> wrapper_XLA_correction_var_mean(const at::Tensor & self, at::OptionalIntArrayRef dim, const ::std::optional<at::Scalar> & correction, bool keepdim) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::var_mean(self, dim, correction, keepdim);
}
} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_XLA_correction_out_var_mean_out(const at::Tensor & self, at::OptionalIntArrayRef dim, const ::std::optional<at::Scalar> & correction, bool keepdim, at::Tensor & out0, at::Tensor & out1) {
  auto wrapper_XLA_correction_out_var_mean_out_tmp = wrapper_XLA_correction_var_mean(self, dim, correction, keepdim);
  at::_copy_from_and_resize(std::get<0>(wrapper_XLA_correction_out_var_mean_out_tmp), out0);
  at::_copy_from_and_resize(std::get<1>(wrapper_XLA_correction_out_var_mean_out_tmp), out1);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out0, out1);
}
namespace {
at::Tensor wrapper_XLA__view(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::view_symint(self, size);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_XLA__view_as_complex_copy(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::view_as_complex_copy(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_view_as_complex_copy_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_view_as_complex_copy_out_tmp = wrapper_XLA__view_as_complex_copy(self);
  at::_copy_from_and_resize(wrapper_XLA_out_view_as_complex_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__view_as_real_copy(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::view_as_real_copy(self);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_view_as_real_copy_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_XLA_out_view_as_real_copy_out_tmp = wrapper_XLA__view_as_real_copy(self);
  at::_copy_from_and_resize(wrapper_XLA_out_view_as_real_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA__view_copy(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::view_copy_symint(self, size);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_out_view_copy_out(const at::Tensor & self, c10::SymIntArrayRef size, at::Tensor & out) {
  auto wrapper_XLA_out_view_copy_out_tmp = wrapper_XLA__view_copy(self, size);
  at::_copy_from_and_resize(wrapper_XLA_out_view_copy_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_self_where(const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::where(condition, self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_self_out_where_out(const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_self_out_where_out_tmp = wrapper_XLA_self_where(condition, self, other);
  at::_copy_from_and_resize(wrapper_XLA_self_out_where_out_tmp, out);
  return out;
}
namespace {
at::Tensor wrapper_XLA_Tensor_xlogy(const at::Tensor & self, const at::Tensor & other) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::xlogy(self, other);
}
} // anonymous namespace
at::Tensor & wrapper_XLA_OutTensor_xlogy_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_XLA_OutTensor_xlogy_out_tmp = wrapper_XLA_Tensor_xlogy(self, other);
  at::_copy_from_and_resize(wrapper_XLA_OutTensor_xlogy_out_tmp, out);
  return out;
}
at::Tensor & wrapper_XLA_Tensor_xlogy_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_XLA_Tensor_xlogy__tmp = wrapper_XLA_Tensor_xlogy(self, other);
  at::_copy_from(wrapper_XLA_Tensor_xlogy__tmp, self);
  return self;
}
namespace {
at::Tensor & wrapper_XLA__zero_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return torch_xla::XLANativeFunctions::zero_(self);
}
} // anonymous namespace
} // anonymous namespace
TORCH_API void RegisterXLAXLANativeFunctions();
TORCH_API void RegisterXLAXLANativeFunctions() {
    static auto m = MAKE_TORCH_LIBRARY_IMPL(aten, XLA);
    m.impl("__lshift__.Scalar",
    TORCH_FN(wrapper_XLA_Scalar___lshift__));
    m.impl("__lshift__.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out___lshift___out));
    m.impl("__ilshift__.Scalar",
    TORCH_FN(wrapper_XLA_Scalar___ilshift__));
    m.impl("__lshift__.Tensor",
    TORCH_FN(wrapper_XLA_Tensor___lshift__));
    m.impl("__lshift__.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out___lshift___out));
    m.impl("__ilshift__.Tensor",
    TORCH_FN(wrapper_XLA_Tensor___ilshift__));
    m.impl("__rshift__.Scalar",
    TORCH_FN(wrapper_XLA_Scalar___rshift__));
    m.impl("__rshift__.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out___rshift___out));
    m.impl("__irshift__.Scalar",
    TORCH_FN(wrapper_XLA_Scalar___irshift__));
    m.impl("__rshift__.Tensor",
    TORCH_FN(wrapper_XLA_Tensor___rshift__));
    m.impl("__rshift__.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out___rshift___out));
    m.impl("__irshift__.Tensor",
    TORCH_FN(wrapper_XLA_Tensor___irshift__));
    m.impl("_adaptive_avg_pool2d",
    TORCH_FN(wrapper_XLA___adaptive_avg_pool2d));
    m.impl("_adaptive_avg_pool2d.out",
    TORCH_FN(wrapper_XLA_out__adaptive_avg_pool2d_out));
    m.impl("_adaptive_avg_pool2d_backward",
    TORCH_FN(wrapper_XLA___adaptive_avg_pool2d_backward));
    m.impl("_adaptive_avg_pool2d_backward.out",
    TORCH_FN(wrapper_XLA_out__adaptive_avg_pool2d_backward_out));
    m.impl("_adaptive_avg_pool3d",
    TORCH_FN(wrapper_XLA___adaptive_avg_pool3d));
    m.impl("_adaptive_avg_pool3d.out",
    TORCH_FN(wrapper_XLA_out__adaptive_avg_pool3d_out));
    m.impl("_adaptive_avg_pool3d_backward",
    TORCH_FN(wrapper_XLA___adaptive_avg_pool3d_backward));
    m.impl("_adaptive_avg_pool3d_backward.out",
    TORCH_FN(wrapper_XLA_out__adaptive_avg_pool3d_backward_out));
    m.impl("_amp_foreach_non_finite_check_and_unscale_",
    TORCH_FN(wrapper_XLA___amp_foreach_non_finite_check_and_unscale_));
    m.impl("_amp_update_scale_",
    TORCH_FN(wrapper_XLA___amp_update_scale_));
    m.impl("_cdist_forward",
    TORCH_FN(wrapper_XLA___cdist_forward));
    m.impl("_cdist_forward.out",
    TORCH_FN(wrapper_XLA_out__cdist_forward_out));
    m.impl("_conj_copy",
    TORCH_FN(wrapper_XLA___conj_copy));
    m.impl("_conj_copy.out",
    TORCH_FN(wrapper_XLA_out__conj_copy_out));
    m.impl("_convolution",
    TORCH_FN(wrapper_XLA___convolution));
    m.impl("_convolution.out",
    TORCH_FN(wrapper_XLA_out__convolution_out));
    m.impl("_copy_from",
    TORCH_FN(wrapper_XLA___copy_from));
    m.impl("_copy_from.out",
    TORCH_FN(wrapper_XLA_out__copy_from_out));
    m.impl("_copy_from_and_resize",
    TORCH_FN(wrapper_XLA___copy_from_and_resize));
    m.impl("_copy_from_and_resize.out",
    TORCH_FN(wrapper_XLA_out__copy_from_and_resize_out));
    m.impl("_embedding_bag_backward",
    TORCH_FN(wrapper_XLA___embedding_bag_backward));
    m.impl("_embedding_bag_forward_only",
    TORCH_FN(wrapper_XLA___embedding_bag_forward_only));
    m.impl("_embedding_bag_forward_only.out",
    TORCH_FN(wrapper_XLA_out__embedding_bag_forward_only_out));
    m.impl("_euclidean_dist",
    TORCH_FN(wrapper_XLA___euclidean_dist));
    m.impl("_euclidean_dist.out",
    TORCH_FN(wrapper_XLA_out__euclidean_dist_out));
    m.impl("_index_put_impl_",
    TORCH_FN(wrapper_XLA___index_put_impl_));
    m.impl("_linalg_eigh",
    TORCH_FN(wrapper_XLA___linalg_eigh));
    m.impl("_linalg_eigh.eigenvalues",
    TORCH_FN(wrapper_XLA_eigenvalues__linalg_eigh_out));
    m.impl("_linalg_slogdet",
    TORCH_FN(wrapper_XLA___linalg_slogdet));
    m.impl("_linalg_slogdet.sign",
    TORCH_FN(wrapper_XLA_sign__linalg_slogdet_out));
    m.impl("_linalg_svd",
    TORCH_FN(wrapper_XLA___linalg_svd));
    m.impl("_linalg_svd.U",
    TORCH_FN(wrapper_XLA_U__linalg_svd_out));
    m.impl("_local_scalar_dense",
    TORCH_FN(wrapper_XLA___local_scalar_dense));
    m.impl("_log_softmax",
    TORCH_FN(wrapper_XLA___log_softmax));
    m.impl("_log_softmax.out",
    TORCH_FN(wrapper_XLA_out__log_softmax_out));
    m.impl("_log_softmax_backward_data",
    TORCH_FN(wrapper_XLA___log_softmax_backward_data));
    m.impl("_log_softmax_backward_data.out",
    TORCH_FN(wrapper_XLA_out__log_softmax_backward_data_out));
    m.impl("_native_batch_norm_legit.no_stats",
    TORCH_FN(wrapper_XLA_no_stats__native_batch_norm_legit));
    m.impl("_native_batch_norm_legit.no_stats_out",
    TORCH_FN(wrapper_XLA_no_stats_out__native_batch_norm_legit_out));
    m.impl("_native_batch_norm_legit",
    TORCH_FN(wrapper_XLA___native_batch_norm_legit));
    m.impl("_pack_padded_sequence",
    TORCH_FN(wrapper_XLA___pack_padded_sequence));
    m.impl("_pack_padded_sequence.out",
    TORCH_FN(wrapper_XLA_out__pack_padded_sequence_out));
    m.impl("_pdist_forward",
    TORCH_FN(wrapper_XLA___pdist_forward));
    m.impl("_pdist_forward.out",
    TORCH_FN(wrapper_XLA_out__pdist_forward_out));
    m.impl("_prelu_kernel",
    TORCH_FN(wrapper_XLA___prelu_kernel));
    m.impl("_prelu_kernel_backward",
    TORCH_FN(wrapper_XLA___prelu_kernel_backward));
    m.impl("_propagate_xla_data",
    TORCH_FN(wrapper_XLA___propagate_xla_data));
    m.impl("_softmax",
    TORCH_FN(wrapper_XLA___softmax));
    m.impl("_softmax.out",
    TORCH_FN(wrapper_XLA_out__softmax_out));
    m.impl("_softmax_backward_data",
    TORCH_FN(wrapper_XLA___softmax_backward_data));
    m.impl("_softmax_backward_data.out",
    TORCH_FN(wrapper_XLA_out__softmax_backward_data_out));
    m.impl("_to_copy",
    TORCH_FN(wrapper_XLA___to_copy));
    m.impl("_to_copy.out",
    TORCH_FN(wrapper_XLA_out__to_copy_out));
    m.impl("_to_cpu",
    TORCH_FN(wrapper_XLA___to_cpu));
    m.impl("_trilinear",
    TORCH_FN(wrapper_XLA___trilinear));
    m.impl("_trilinear.out",
    TORCH_FN(wrapper_XLA_out__trilinear_out));
    m.impl("_unsafe_view",
    TORCH_FN(wrapper_XLA___unsafe_view));
    m.impl("_unsafe_view.out",
    TORCH_FN(wrapper_XLA_out__unsafe_view_out));
    m.impl("abs",
    TORCH_FN(wrapper_XLA__abs));
    m.impl("abs.out",
    TORCH_FN(wrapper_XLA_out_abs_out));
    m.impl("abs_",
    TORCH_FN(wrapper_XLA__abs_));
    m.impl("acos",
    TORCH_FN(wrapper_XLA__acos));
    m.impl("acos.out",
    TORCH_FN(wrapper_XLA_out_acos_out));
    m.impl("acos_",
    TORCH_FN(wrapper_XLA__acos_));
    m.impl("acosh",
    TORCH_FN(wrapper_XLA__acosh));
    m.impl("acosh.out",
    TORCH_FN(wrapper_XLA_out_acosh_out));
    m.impl("acosh_",
    TORCH_FN(wrapper_XLA__acosh_));
    m.impl("adaptive_max_pool2d",
    TORCH_FN(wrapper_XLA__adaptive_max_pool2d));
    m.impl("adaptive_max_pool2d.out",
    TORCH_FN(wrapper_XLA_out_adaptive_max_pool2d_out));
    m.impl("adaptive_max_pool2d_backward",
    TORCH_FN(wrapper_XLA__adaptive_max_pool2d_backward));
    m.impl("adaptive_max_pool2d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_adaptive_max_pool2d_backward_out));
    m.impl("add.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_add));
    m.impl("add.out",
    TORCH_FN(wrapper_XLA_out_add_out));
    m.impl("add_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_add_));
    m.impl("add.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_add));
    m.impl("add.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_add_out));
    m.impl("add_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_add_));
    m.impl("addcdiv",
    TORCH_FN(wrapper_XLA__addcdiv));
    m.impl("addcdiv.out",
    TORCH_FN(wrapper_XLA_out_addcdiv_out));
    m.impl("addcdiv_",
    TORCH_FN(wrapper_XLA__addcdiv_));
    m.impl("addcmul",
    TORCH_FN(wrapper_XLA__addcmul));
    m.impl("addcmul.out",
    TORCH_FN(wrapper_XLA_out_addcmul_out));
    m.impl("addcmul_",
    TORCH_FN(wrapper_XLA__addcmul_));
    m.impl("addmm",
    TORCH_FN(wrapper_XLA__addmm));
    m.impl("addmm.out",
    TORCH_FN(wrapper_XLA_out_addmm_out));
    m.impl("addmm_",
    TORCH_FN(wrapper_XLA__addmm_));
    m.impl("affine_grid_generator",
    TORCH_FN(wrapper_XLA__affine_grid_generator));
    m.impl("affine_grid_generator.out",
    TORCH_FN(wrapper_XLA_out_affine_grid_generator_out));
    m.impl("alias",
    TORCH_FN(wrapper_XLA__alias));
    m.impl("alias_copy",
    TORCH_FN(wrapper_XLA__alias_copy));
    m.impl("alias_copy.out",
    TORCH_FN(wrapper_XLA_out_alias_copy_out));
    m.impl("all.dim",
    TORCH_FN(wrapper_XLA_dim_all));
    m.impl("all.out",
    TORCH_FN(wrapper_XLA_out_all_out));
    m.impl("all",
    TORCH_FN(wrapper_XLA__all));
    m.impl("all.all_out",
    TORCH_FN(wrapper_XLA_all_out_all_out));
    m.impl("amax",
    TORCH_FN(wrapper_XLA__amax));
    m.impl("amax.out",
    TORCH_FN(wrapper_XLA_out_amax_out));
    m.impl("amin",
    TORCH_FN(wrapper_XLA__amin));
    m.impl("amin.out",
    TORCH_FN(wrapper_XLA_out_amin_out));
    m.impl("any.dim",
    TORCH_FN(wrapper_XLA_dim_any));
    m.impl("any.out",
    TORCH_FN(wrapper_XLA_out_any_out));
    m.impl("any",
    TORCH_FN(wrapper_XLA__any));
    m.impl("any.all_out",
    TORCH_FN(wrapper_XLA_all_out_any_out));
    m.impl("arange.start_out",
    TORCH_FN(wrapper_XLA_start_out_arange_out));
    m.impl("argmax",
    TORCH_FN(wrapper_XLA__argmax));
    m.impl("argmax.out",
    TORCH_FN(wrapper_XLA_out_argmax_out));
    m.impl("argmin",
    TORCH_FN(wrapper_XLA__argmin));
    m.impl("argmin.out",
    TORCH_FN(wrapper_XLA_out_argmin_out));
    m.impl("as_strided",
    TORCH_FN(wrapper_XLA__as_strided));
    m.impl("as_strided_",
    TORCH_FN(wrapper_XLA__as_strided_));
    m.impl("as_strided_copy",
    TORCH_FN(wrapper_XLA__as_strided_copy));
    m.impl("as_strided_copy.out",
    TORCH_FN(wrapper_XLA_out_as_strided_copy_out));
    m.impl("as_strided_scatter",
    TORCH_FN(wrapper_XLA__as_strided_scatter));
    m.impl("as_strided_scatter.out",
    TORCH_FN(wrapper_XLA_out_as_strided_scatter_out));
    m.impl("asin",
    TORCH_FN(wrapper_XLA__asin));
    m.impl("asin.out",
    TORCH_FN(wrapper_XLA_out_asin_out));
    m.impl("asin_",
    TORCH_FN(wrapper_XLA__asin_));
    m.impl("asinh",
    TORCH_FN(wrapper_XLA__asinh));
    m.impl("asinh.out",
    TORCH_FN(wrapper_XLA_out_asinh_out));
    m.impl("asinh_",
    TORCH_FN(wrapper_XLA__asinh_));
    m.impl("atan",
    TORCH_FN(wrapper_XLA__atan));
    m.impl("atan.out",
    TORCH_FN(wrapper_XLA_out_atan_out));
    m.impl("atan_",
    TORCH_FN(wrapper_XLA__atan_));
    m.impl("atan2",
    TORCH_FN(wrapper_XLA__atan2));
    m.impl("atan2.out",
    TORCH_FN(wrapper_XLA_out_atan2_out));
    m.impl("atan2_",
    TORCH_FN(wrapper_XLA__atan2_));
    m.impl("atanh",
    TORCH_FN(wrapper_XLA__atanh));
    m.impl("atanh.out",
    TORCH_FN(wrapper_XLA_out_atanh_out));
    m.impl("atanh_",
    TORCH_FN(wrapper_XLA__atanh_));
    m.impl("avg_pool2d",
    TORCH_FN(wrapper_XLA__avg_pool2d));
    m.impl("avg_pool2d.out",
    TORCH_FN(wrapper_XLA_out_avg_pool2d_out));
    m.impl("avg_pool2d_backward",
    TORCH_FN(wrapper_XLA__avg_pool2d_backward));
    m.impl("avg_pool2d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_avg_pool2d_backward_out));
    m.impl("avg_pool3d",
    TORCH_FN(wrapper_XLA__avg_pool3d));
    m.impl("avg_pool3d.out",
    TORCH_FN(wrapper_XLA_out_avg_pool3d_out));
    m.impl("avg_pool3d_backward",
    TORCH_FN(wrapper_XLA__avg_pool3d_backward));
    m.impl("avg_pool3d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_avg_pool3d_backward_out));
    m.impl("baddbmm",
    TORCH_FN(wrapper_XLA__baddbmm));
    m.impl("baddbmm.out",
    TORCH_FN(wrapper_XLA_out_baddbmm_out));
    m.impl("baddbmm_",
    TORCH_FN(wrapper_XLA__baddbmm_));
    m.impl("bernoulli",
    TORCH_FN(wrapper_XLA__bernoulli));
    m.impl("bernoulli.out",
    TORCH_FN(wrapper_XLA_out_bernoulli_out));
    m.impl("bernoulli_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bernoulli_));
    m.impl("bernoulli.p",
    TORCH_FN(wrapper_XLA_p_bernoulli));
    m.impl("bernoulli.float_out",
    TORCH_FN(wrapper_XLA_float_out_bernoulli_out));
    m.impl("bernoulli_.float",
    TORCH_FN(wrapper_XLA_float_bernoulli_));
    m.impl("binary_cross_entropy",
    TORCH_FN(wrapper_XLA__binary_cross_entropy));
    m.impl("binary_cross_entropy.out",
    TORCH_FN(wrapper_XLA_out_binary_cross_entropy_out));
    m.impl("binary_cross_entropy_backward",
    TORCH_FN(wrapper_XLA__binary_cross_entropy_backward));
    m.impl("binary_cross_entropy_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_binary_cross_entropy_backward_out));
    m.impl("binary_cross_entropy_with_logits",
    TORCH_FN(wrapper_XLA__binary_cross_entropy_with_logits));
    m.impl("binary_cross_entropy_with_logits.out",
    TORCH_FN(wrapper_XLA_out_binary_cross_entropy_with_logits_out));
    m.impl("bitwise_and.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_and));
    m.impl("bitwise_and.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_bitwise_and_out));
    m.impl("bitwise_and_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_and_));
    m.impl("bitwise_left_shift.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_left_shift));
    m.impl("bitwise_left_shift.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_bitwise_left_shift_out));
    m.impl("bitwise_left_shift_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_left_shift_));
    m.impl("bitwise_not",
    TORCH_FN(wrapper_XLA__bitwise_not));
    m.impl("bitwise_not.out",
    TORCH_FN(wrapper_XLA_out_bitwise_not_out));
    m.impl("bitwise_not_",
    TORCH_FN(wrapper_XLA__bitwise_not_));
    m.impl("bitwise_or.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_or));
    m.impl("bitwise_or.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_bitwise_or_out));
    m.impl("bitwise_or_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_or_));
    m.impl("bitwise_right_shift.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_right_shift));
    m.impl("bitwise_right_shift.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_bitwise_right_shift_out));
    m.impl("bitwise_right_shift_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_right_shift_));
    m.impl("bitwise_xor.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_xor));
    m.impl("bitwise_xor.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_bitwise_xor_out));
    m.impl("bitwise_xor_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_bitwise_xor_));
    m.impl("block_diag",
    TORCH_FN(wrapper_XLA__block_diag));
    m.impl("block_diag.out",
    TORCH_FN(wrapper_XLA_out_block_diag_out));
    m.impl("bmm",
    TORCH_FN(wrapper_XLA__bmm));
    m.impl("bmm.out",
    TORCH_FN(wrapper_XLA_out_bmm_out));
    m.impl("cat",
    TORCH_FN(wrapper_XLA__cat));
    m.impl("cat.out",
    TORCH_FN(wrapper_XLA_out_cat_out));
    m.impl("ceil",
    TORCH_FN(wrapper_XLA__ceil));
    m.impl("ceil.out",
    TORCH_FN(wrapper_XLA_out_ceil_out));
    m.impl("ceil_",
    TORCH_FN(wrapper_XLA__ceil_));
    m.impl("celu",
    TORCH_FN(wrapper_XLA__celu));
    m.impl("celu.out",
    TORCH_FN(wrapper_XLA_out_celu_out));
    m.impl("celu_",
    TORCH_FN(wrapper_XLA__celu_));
    m.impl("cholesky",
    TORCH_FN(wrapper_XLA__cholesky));
    m.impl("cholesky.out",
    TORCH_FN(wrapper_XLA_out_cholesky_out));
    m.impl("clamp",
    TORCH_FN(wrapper_XLA__clamp));
    m.impl("clamp.out",
    TORCH_FN(wrapper_XLA_out_clamp_out));
    m.impl("clamp_",
    TORCH_FN(wrapper_XLA__clamp_));
    m.impl("clamp.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_clamp));
    m.impl("clamp.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_clamp_out));
    m.impl("clamp_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_clamp_));
    m.impl("clamp_max",
    TORCH_FN(wrapper_XLA__clamp_max));
    m.impl("clamp_max.out",
    TORCH_FN(wrapper_XLA_out_clamp_max_out));
    m.impl("clamp_max_",
    TORCH_FN(wrapper_XLA__clamp_max_));
    m.impl("clamp_max.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_clamp_max));
    m.impl("clamp_max.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_clamp_max_out));
    m.impl("clamp_max_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_clamp_max_));
    m.impl("clamp_min",
    TORCH_FN(wrapper_XLA__clamp_min));
    m.impl("clamp_min.out",
    TORCH_FN(wrapper_XLA_out_clamp_min_out));
    m.impl("clamp_min_",
    TORCH_FN(wrapper_XLA__clamp_min_));
    m.impl("clamp_min.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_clamp_min));
    m.impl("clamp_min.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_clamp_min_out));
    m.impl("clamp_min_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_clamp_min_));
    m.impl("clone",
    TORCH_FN(wrapper_XLA__clone));
    m.impl("clone.out",
    TORCH_FN(wrapper_XLA_out_clone_out));
    m.impl("constant_pad_nd",
    TORCH_FN(wrapper_XLA__constant_pad_nd));
    m.impl("constant_pad_nd.out",
    TORCH_FN(wrapper_XLA_out_constant_pad_nd_out));
    m.impl("convolution_backward",
    TORCH_FN(wrapper_XLA__convolution_backward));
    m.impl("convolution_backward.out",
    TORCH_FN(wrapper_XLA_out_convolution_backward_out));
    m.impl("convolution_backward_overrideable",
    TORCH_FN(wrapper_XLA__convolution_backward_overrideable));
    m.impl("convolution_backward_overrideable.out",
    TORCH_FN(wrapper_XLA_out_convolution_backward_overrideable_out));
    m.impl("convolution_overrideable",
    TORCH_FN(wrapper_XLA__convolution_overrideable));
    m.impl("convolution_overrideable.out",
    TORCH_FN(wrapper_XLA_out_convolution_overrideable_out));
    m.impl("copy",
    TORCH_FN(wrapper_XLA__copy));
    m.impl("copy.out",
    TORCH_FN(wrapper_XLA_out_copy_out));
    m.impl("copy_",
    TORCH_FN(wrapper_XLA__copy_));
    m.impl("cos",
    TORCH_FN(wrapper_XLA__cos));
    m.impl("cos.out",
    TORCH_FN(wrapper_XLA_out_cos_out));
    m.impl("cos_",
    TORCH_FN(wrapper_XLA__cos_));
    m.impl("cosh",
    TORCH_FN(wrapper_XLA__cosh));
    m.impl("cosh.out",
    TORCH_FN(wrapper_XLA_out_cosh_out));
    m.impl("cosh_",
    TORCH_FN(wrapper_XLA__cosh_));
    m.impl("count_nonzero.dim_IntList",
    TORCH_FN(wrapper_XLA_dim_IntList_count_nonzero));
    m.impl("count_nonzero.dim_IntList_out",
    TORCH_FN(wrapper_XLA_dim_IntList_out_count_nonzero_out));
    m.impl("count_nonzero",
    TORCH_FN(wrapper_XLA__count_nonzero));
    m.impl("count_nonzero.out",
    TORCH_FN(wrapper_XLA_out_count_nonzero_out));
    m.impl("cross",
    TORCH_FN(wrapper_XLA__cross));
    m.impl("cross.out",
    TORCH_FN(wrapper_XLA_out_cross_out));
    m.impl("cummax",
    TORCH_FN(wrapper_XLA__cummax));
    m.impl("cummax.out",
    TORCH_FN(wrapper_XLA_out_cummax_out));
    m.impl("cumprod",
    TORCH_FN(wrapper_XLA__cumprod));
    m.impl("cumprod.out",
    TORCH_FN(wrapper_XLA_out_cumprod_out));
    m.impl("cumprod_",
    TORCH_FN(wrapper_XLA__cumprod_));
    m.impl("cumsum",
    TORCH_FN(wrapper_XLA__cumsum));
    m.impl("cumsum.out",
    TORCH_FN(wrapper_XLA_out_cumsum_out));
    m.impl("cumsum_",
    TORCH_FN(wrapper_XLA__cumsum_));
    m.impl("detach_copy",
    TORCH_FN(wrapper_XLA__detach_copy));
    m.impl("detach_copy.out",
    TORCH_FN(wrapper_XLA_out_detach_copy_out));
    m.impl("diag",
    TORCH_FN(wrapper_XLA__diag));
    m.impl("diag.out",
    TORCH_FN(wrapper_XLA_out_diag_out));
    m.impl("diag_embed",
    TORCH_FN(wrapper_XLA__diag_embed));
    m.impl("diag_embed.out",
    TORCH_FN(wrapper_XLA_out_diag_embed_out));
    m.impl("diagonal",
    TORCH_FN(wrapper_XLA__diagonal));
    m.impl("diagonal_backward",
    TORCH_FN(wrapper_XLA__diagonal_backward));
    m.impl("diagonal_backward.out",
    TORCH_FN(wrapper_XLA_out_diagonal_backward_out));
    m.impl("diagonal_copy",
    TORCH_FN(wrapper_XLA__diagonal_copy));
    m.impl("diagonal_copy.out",
    TORCH_FN(wrapper_XLA_out_diagonal_copy_out));
    m.impl("diagonal_scatter",
    TORCH_FN(wrapper_XLA__diagonal_scatter));
    m.impl("diagonal_scatter.out",
    TORCH_FN(wrapper_XLA_out_diagonal_scatter_out));
    m.impl("div.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_div));
    m.impl("div.out",
    TORCH_FN(wrapper_XLA_out_div_out));
    m.impl("div_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_div_));
    m.impl("div.Tensor_mode",
    TORCH_FN(wrapper_XLA_Tensor_mode_div));
    m.impl("div.out_mode",
    TORCH_FN(wrapper_XLA_out_mode_div_out));
    m.impl("div_.Tensor_mode",
    TORCH_FN(wrapper_XLA_Tensor_mode_div_));
    m.impl("div.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_div));
    m.impl("div.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_div_out));
    m.impl("div_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_div_));
    m.impl("dot",
    TORCH_FN(wrapper_XLA__dot));
    m.impl("dot.out",
    TORCH_FN(wrapper_XLA_out_dot_out));
    m.impl("elu",
    TORCH_FN(wrapper_XLA__elu));
    m.impl("elu.out",
    TORCH_FN(wrapper_XLA_out_elu_out));
    m.impl("elu_",
    TORCH_FN(wrapper_XLA__elu_));
    m.impl("elu_backward",
    TORCH_FN(wrapper_XLA__elu_backward));
    m.impl("elu_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_elu_backward_out));
    m.impl("embedding",
    TORCH_FN(wrapper_XLA__embedding));
    m.impl("embedding.out",
    TORCH_FN(wrapper_XLA_out_embedding_out));
    m.impl("embedding_dense_backward",
    TORCH_FN(wrapper_XLA__embedding_dense_backward));
    m.impl("embedding_dense_backward.out",
    TORCH_FN(wrapper_XLA_out_embedding_dense_backward_out));
    m.impl("empty.memory_format",
    TORCH_FN(wrapper_XLA_memory_format_empty));
    m.impl("empty.out",
    TORCH_FN(wrapper_XLA_out_empty_out));
    m.impl("empty_strided",
    TORCH_FN(wrapper_XLA__empty_strided));
    m.impl("empty_strided.out",
    TORCH_FN(wrapper_XLA_out_empty_strided_out));
    m.impl("eq.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_eq));
    m.impl("eq.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_eq_out));
    m.impl("eq_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_eq_));
    m.impl("eq.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_eq));
    m.impl("eq.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_eq_out));
    m.impl("eq_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_eq_));
    m.impl("erf",
    TORCH_FN(wrapper_XLA__erf));
    m.impl("erf.out",
    TORCH_FN(wrapper_XLA_out_erf_out));
    m.impl("erf_",
    TORCH_FN(wrapper_XLA__erf_));
    m.impl("erfc",
    TORCH_FN(wrapper_XLA__erfc));
    m.impl("erfc.out",
    TORCH_FN(wrapper_XLA_out_erfc_out));
    m.impl("erfc_",
    TORCH_FN(wrapper_XLA__erfc_));
    m.impl("erfinv",
    TORCH_FN(wrapper_XLA__erfinv));
    m.impl("erfinv.out",
    TORCH_FN(wrapper_XLA_out_erfinv_out));
    m.impl("erfinv_",
    TORCH_FN(wrapper_XLA__erfinv_));
    m.impl("exp",
    TORCH_FN(wrapper_XLA__exp));
    m.impl("exp.out",
    TORCH_FN(wrapper_XLA_out_exp_out));
    m.impl("exp_",
    TORCH_FN(wrapper_XLA__exp_));
    m.impl("expand",
    TORCH_FN(wrapper_XLA__expand));
    m.impl("expand_copy",
    TORCH_FN(wrapper_XLA__expand_copy));
    m.impl("expand_copy.out",
    TORCH_FN(wrapper_XLA_out_expand_copy_out));
    m.impl("expm1",
    TORCH_FN(wrapper_XLA__expm1));
    m.impl("expm1.out",
    TORCH_FN(wrapper_XLA_out_expm1_out));
    m.impl("expm1_",
    TORCH_FN(wrapper_XLA__expm1_));
    m.impl("exponential_",
    TORCH_FN(wrapper_XLA__exponential_));
    m.impl("eye.out",
    TORCH_FN(wrapper_XLA_out_eye_out));
    m.impl("eye.m_out",
    TORCH_FN(wrapper_XLA_m_out_eye_out));
    m.impl("fill_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_fill_));
    m.impl("fill_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_fill_));
    m.impl("flip",
    TORCH_FN(wrapper_XLA__flip));
    m.impl("flip.out",
    TORCH_FN(wrapper_XLA_out_flip_out));
    m.impl("floor",
    TORCH_FN(wrapper_XLA__floor));
    m.impl("floor.out",
    TORCH_FN(wrapper_XLA_out_floor_out));
    m.impl("floor_",
    TORCH_FN(wrapper_XLA__floor_));
    m.impl("floor_divide",
    TORCH_FN(wrapper_XLA__floor_divide));
    m.impl("floor_divide.out",
    TORCH_FN(wrapper_XLA_out_floor_divide_out));
    m.impl("floor_divide_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_floor_divide_));
    m.impl("fmod.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_fmod));
    m.impl("fmod.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_fmod_out));
    m.impl("fmod_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_fmod_));
    m.impl("fmod.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_fmod));
    m.impl("fmod.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_fmod_out));
    m.impl("fmod_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_fmod_));
    m.impl("frac",
    TORCH_FN(wrapper_XLA__frac));
    m.impl("frac.out",
    TORCH_FN(wrapper_XLA_out_frac_out));
    m.impl("frac_",
    TORCH_FN(wrapper_XLA__frac_));
    m.impl("full",
    TORCH_FN(wrapper_XLA__full));
    m.impl("full.out",
    TORCH_FN(wrapper_XLA_out_full_out));
    m.impl("gather",
    TORCH_FN(wrapper_XLA__gather));
    m.impl("gather.out",
    TORCH_FN(wrapper_XLA_out_gather_out));
    m.impl("ge.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_ge));
    m.impl("ge.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_ge_out));
    m.impl("ge_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_ge_));
    m.impl("ge.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_ge));
    m.impl("ge.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_ge_out));
    m.impl("ge_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_ge_));
    m.impl("gelu",
    TORCH_FN(wrapper_XLA__gelu));
    m.impl("gelu.out",
    TORCH_FN(wrapper_XLA_out_gelu_out));
    m.impl("gelu_",
    TORCH_FN(wrapper_XLA__gelu_));
    m.impl("gelu_backward",
    TORCH_FN(wrapper_XLA__gelu_backward));
    m.impl("gelu_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_gelu_backward_out));
    m.impl("glu",
    TORCH_FN(wrapper_XLA__glu));
    m.impl("glu.out",
    TORCH_FN(wrapper_XLA_out_glu_out));
    m.impl("gt.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_gt));
    m.impl("gt.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_gt_out));
    m.impl("gt_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_gt_));
    m.impl("gt.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_gt));
    m.impl("gt.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_gt_out));
    m.impl("gt_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_gt_));
    m.impl("hardshrink",
    TORCH_FN(wrapper_XLA__hardshrink));
    m.impl("hardshrink.out",
    TORCH_FN(wrapper_XLA_out_hardshrink_out));
    m.impl("hardshrink_backward",
    TORCH_FN(wrapper_XLA__hardshrink_backward));
    m.impl("hardshrink_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_hardshrink_backward_out));
    m.impl("hardsigmoid",
    TORCH_FN(wrapper_XLA__hardsigmoid));
    m.impl("hardsigmoid.out",
    TORCH_FN(wrapper_XLA_out_hardsigmoid_out));
    m.impl("hardsigmoid_",
    TORCH_FN(wrapper_XLA__hardsigmoid_));
    m.impl("hardsigmoid_backward",
    TORCH_FN(wrapper_XLA__hardsigmoid_backward));
    m.impl("hardsigmoid_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_hardsigmoid_backward_out));
    m.impl("hardswish",
    TORCH_FN(wrapper_XLA__hardswish));
    m.impl("hardswish.out",
    TORCH_FN(wrapper_XLA_out_hardswish_out));
    m.impl("hardswish_",
    TORCH_FN(wrapper_XLA__hardswish_));
    m.impl("hardswish_backward",
    TORCH_FN(wrapper_XLA__hardswish_backward));
    m.impl("hardswish_backward.out",
    TORCH_FN(wrapper_XLA_out_hardswish_backward_out));
    m.impl("hardtanh",
    TORCH_FN(wrapper_XLA__hardtanh));
    m.impl("hardtanh.out",
    TORCH_FN(wrapper_XLA_out_hardtanh_out));
    m.impl("hardtanh_",
    TORCH_FN(wrapper_XLA__hardtanh_));
    m.impl("hardtanh_backward",
    TORCH_FN(wrapper_XLA__hardtanh_backward));
    m.impl("hardtanh_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_hardtanh_backward_out));
    m.impl("index.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_index));
    m.impl("index.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_index_out));
    m.impl("index_add",
    TORCH_FN(wrapper_XLA__index_add));
    m.impl("index_add.out",
    TORCH_FN(wrapper_XLA_out_index_add_out));
    m.impl("index_add_",
    TORCH_FN(wrapper_XLA__index_add_));
    m.impl("index_copy",
    TORCH_FN(wrapper_XLA__index_copy));
    m.impl("index_copy.out",
    TORCH_FN(wrapper_XLA_out_index_copy_out));
    m.impl("index_copy_",
    TORCH_FN(wrapper_XLA__index_copy_));
    m.impl("index_fill_.int_Scalar",
    TORCH_FN(wrapper_XLA_int_Scalar_index_fill_));
    m.impl("index_fill_.int_Tensor",
    TORCH_FN(wrapper_XLA_int_Tensor_index_fill_));
    m.impl("index_put_",
    TORCH_FN(wrapper_XLA__index_put_));
    m.impl("index_select",
    TORCH_FN(wrapper_XLA__index_select));
    m.impl("index_select.out",
    TORCH_FN(wrapper_XLA_out_index_select_out));
    m.impl("inverse",
    TORCH_FN(wrapper_XLA__inverse));
    m.impl("inverse.out",
    TORCH_FN(wrapper_XLA_out_inverse_out));
    m.impl("isnan",
    TORCH_FN(wrapper_XLA__isnan));
    m.impl("isnan.out",
    TORCH_FN(wrapper_XLA_out_isnan_out));
    m.impl("isneginf",
    TORCH_FN(wrapper_XLA__isneginf));
    m.impl("isneginf.out",
    TORCH_FN(wrapper_XLA_out_isneginf_out));
    m.impl("kl_div",
    TORCH_FN(wrapper_XLA__kl_div));
    m.impl("kthvalue",
    TORCH_FN(wrapper_XLA__kthvalue));
    m.impl("kthvalue.values",
    TORCH_FN(wrapper_XLA_values_kthvalue_out));
    m.impl("le.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_le));
    m.impl("le.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_le_out));
    m.impl("le_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_le_));
    m.impl("le.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_le));
    m.impl("le.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_le_out));
    m.impl("le_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_le_));
    m.impl("leaky_relu",
    TORCH_FN(wrapper_XLA__leaky_relu));
    m.impl("leaky_relu.out",
    TORCH_FN(wrapper_XLA_out_leaky_relu_out));
    m.impl("leaky_relu_",
    TORCH_FN(wrapper_XLA__leaky_relu_));
    m.impl("leaky_relu_backward",
    TORCH_FN(wrapper_XLA__leaky_relu_backward));
    m.impl("leaky_relu_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_leaky_relu_backward_out));
    m.impl("lerp.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_lerp));
    m.impl("lerp.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_lerp_out));
    m.impl("lerp_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_lerp_));
    m.impl("lerp.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_lerp));
    m.impl("lerp.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_lerp_out));
    m.impl("lerp_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_lerp_));
    m.impl("lift",
    TORCH_FN(wrapper_XLA__lift));
    m.impl("lift.out",
    TORCH_FN(wrapper_XLA_out_lift_out));
    m.impl("lift_fresh",
    TORCH_FN(wrapper_XLA__lift_fresh));
    m.impl("linalg_inv_ex",
    TORCH_FN(wrapper_XLA__linalg_inv_ex));
    m.impl("linalg_inv_ex.inverse",
    TORCH_FN(wrapper_XLA_inverse_linalg_inv_ex_out));
    m.impl("linalg_pinv.atol_rtol_tensor",
    TORCH_FN(wrapper_XLA_atol_rtol_tensor_linalg_pinv));
    m.impl("linalg_pinv.atol_rtol_tensor_out",
    TORCH_FN(wrapper_XLA_atol_rtol_tensor_out_linalg_pinv_out));
    m.impl("linalg_vector_norm",
    TORCH_FN(wrapper_XLA__linalg_vector_norm));
    m.impl("linalg_vector_norm.out",
    TORCH_FN(wrapper_XLA_out_linalg_vector_norm_out));
    m.impl("linspace",
    TORCH_FN(wrapper_XLA__linspace));
    m.impl("linspace.out",
    TORCH_FN(wrapper_XLA_out_linspace_out));
    m.impl("log",
    TORCH_FN(wrapper_XLA__log));
    m.impl("log.out",
    TORCH_FN(wrapper_XLA_out_log_out));
    m.impl("log_",
    TORCH_FN(wrapper_XLA__log_));
    m.impl("log10",
    TORCH_FN(wrapper_XLA__log10));
    m.impl("log10.out",
    TORCH_FN(wrapper_XLA_out_log10_out));
    m.impl("log10_",
    TORCH_FN(wrapper_XLA__log10_));
    m.impl("log1p",
    TORCH_FN(wrapper_XLA__log1p));
    m.impl("log1p.out",
    TORCH_FN(wrapper_XLA_out_log1p_out));
    m.impl("log1p_",
    TORCH_FN(wrapper_XLA__log1p_));
    m.impl("log2",
    TORCH_FN(wrapper_XLA__log2));
    m.impl("log2.out",
    TORCH_FN(wrapper_XLA_out_log2_out));
    m.impl("log2_",
    TORCH_FN(wrapper_XLA__log2_));
    m.impl("log_sigmoid_backward",
    TORCH_FN(wrapper_XLA__log_sigmoid_backward));
    m.impl("log_sigmoid_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_log_sigmoid_backward_out));
    m.impl("log_sigmoid_forward",
    TORCH_FN(wrapper_XLA__log_sigmoid_forward));
    m.impl("log_sigmoid_forward.output",
    TORCH_FN(wrapper_XLA_output_log_sigmoid_forward_out));
    m.impl("logdet",
    TORCH_FN(wrapper_XLA__logdet));
    m.impl("logical_and",
    TORCH_FN(wrapper_XLA__logical_and));
    m.impl("logical_and.out",
    TORCH_FN(wrapper_XLA_out_logical_and_out));
    m.impl("logical_and_",
    TORCH_FN(wrapper_XLA__logical_and_));
    m.impl("logical_not",
    TORCH_FN(wrapper_XLA__logical_not));
    m.impl("logical_not.out",
    TORCH_FN(wrapper_XLA_out_logical_not_out));
    m.impl("logical_not_",
    TORCH_FN(wrapper_XLA__logical_not_));
    m.impl("logical_or",
    TORCH_FN(wrapper_XLA__logical_or));
    m.impl("logical_or.out",
    TORCH_FN(wrapper_XLA_out_logical_or_out));
    m.impl("logical_or_",
    TORCH_FN(wrapper_XLA__logical_or_));
    m.impl("logical_xor",
    TORCH_FN(wrapper_XLA__logical_xor));
    m.impl("logical_xor.out",
    TORCH_FN(wrapper_XLA_out_logical_xor_out));
    m.impl("logical_xor_",
    TORCH_FN(wrapper_XLA__logical_xor_));
    m.impl("logit",
    TORCH_FN(wrapper_XLA__logit));
    m.impl("logit.out",
    TORCH_FN(wrapper_XLA_out_logit_out));
    m.impl("logit_",
    TORCH_FN(wrapper_XLA__logit_));
    m.impl("logsumexp",
    TORCH_FN(wrapper_XLA__logsumexp));
    m.impl("logsumexp.out",
    TORCH_FN(wrapper_XLA_out_logsumexp_out));
    m.impl("lt.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_lt));
    m.impl("lt.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_lt_out));
    m.impl("lt_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_lt_));
    m.impl("lt.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_lt));
    m.impl("lt.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_lt_out));
    m.impl("lt_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_lt_));
    m.impl("masked_fill.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_masked_fill));
    m.impl("masked_fill.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_masked_fill_out));
    m.impl("masked_fill_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_masked_fill_));
    m.impl("masked_fill.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_masked_fill));
    m.impl("masked_fill.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_masked_fill_out));
    m.impl("masked_fill_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_masked_fill_));
    m.impl("masked_scatter",
    TORCH_FN(wrapper_XLA__masked_scatter));
    m.impl("masked_scatter.out",
    TORCH_FN(wrapper_XLA_out_masked_scatter_out));
    m.impl("masked_scatter_",
    TORCH_FN(wrapper_XLA__masked_scatter_));
    m.impl("masked_select",
    TORCH_FN(wrapper_XLA__masked_select));
    m.impl("masked_select.out",
    TORCH_FN(wrapper_XLA_out_masked_select_out));
    m.impl("max.dim",
    TORCH_FN(wrapper_XLA_dim_max));
    m.impl("max.dim_max",
    TORCH_FN(wrapper_XLA_dim_max_max_out));
    m.impl("max",
    TORCH_FN(wrapper_XLA__max));
    m.impl("max.unary_out",
    TORCH_FN(wrapper_XLA_unary_out_max_out));
    m.impl("max_pool2d_with_indices",
    TORCH_FN(wrapper_XLA__max_pool2d_with_indices));
    m.impl("max_pool2d_with_indices.out",
    TORCH_FN(wrapper_XLA_out_max_pool2d_with_indices_out));
    m.impl("max_pool2d_with_indices_backward",
    TORCH_FN(wrapper_XLA__max_pool2d_with_indices_backward));
    m.impl("max_pool2d_with_indices_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_max_pool2d_with_indices_backward_out));
    m.impl("max_pool3d_with_indices",
    TORCH_FN(wrapper_XLA__max_pool3d_with_indices));
    m.impl("max_pool3d_with_indices.out",
    TORCH_FN(wrapper_XLA_out_max_pool3d_with_indices_out));
    m.impl("max_pool3d_with_indices_backward",
    TORCH_FN(wrapper_XLA__max_pool3d_with_indices_backward));
    m.impl("max_pool3d_with_indices_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_max_pool3d_with_indices_backward_out));
    m.impl("max_unpool2d",
    TORCH_FN(wrapper_XLA__max_unpool2d));
    m.impl("max_unpool2d.out",
    TORCH_FN(wrapper_XLA_out_max_unpool2d_out));
    m.impl("max_unpool3d",
    TORCH_FN(wrapper_XLA__max_unpool3d));
    m.impl("max_unpool3d.out",
    TORCH_FN(wrapper_XLA_out_max_unpool3d_out));
    m.impl("maximum",
    TORCH_FN(wrapper_XLA__maximum));
    m.impl("maximum.out",
    TORCH_FN(wrapper_XLA_out_maximum_out));
    m.impl("mean",
    TORCH_FN(wrapper_XLA__mean));
    m.impl("mean.dtype_out",
    TORCH_FN(wrapper_XLA_dtype_out_mean_out));
    m.impl("mean.dim",
    TORCH_FN(wrapper_XLA_dim_mean));
    m.impl("mean.out",
    TORCH_FN(wrapper_XLA_out_mean_out));
    m.impl("min.dim",
    TORCH_FN(wrapper_XLA_dim_min));
    m.impl("min.dim_min",
    TORCH_FN(wrapper_XLA_dim_min_min_out));
    m.impl("min",
    TORCH_FN(wrapper_XLA__min));
    m.impl("min.unary_out",
    TORCH_FN(wrapper_XLA_unary_out_min_out));
    m.impl("minimum",
    TORCH_FN(wrapper_XLA__minimum));
    m.impl("minimum.out",
    TORCH_FN(wrapper_XLA_out_minimum_out));
    m.impl("mish",
    TORCH_FN(wrapper_XLA__mish));
    m.impl("mish.out",
    TORCH_FN(wrapper_XLA_out_mish_out));
    m.impl("mish_",
    TORCH_FN(wrapper_XLA__mish_));
    m.impl("mm",
    TORCH_FN(wrapper_XLA__mm));
    m.impl("mm.out",
    TORCH_FN(wrapper_XLA_out_mm_out));
    m.impl("mse_loss",
    TORCH_FN(wrapper_XLA__mse_loss));
    m.impl("mse_loss.out",
    TORCH_FN(wrapper_XLA_out_mse_loss_out));
    m.impl("mse_loss_backward",
    TORCH_FN(wrapper_XLA__mse_loss_backward));
    m.impl("mse_loss_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_mse_loss_backward_out));
    m.impl("mul.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_mul));
    m.impl("mul.out",
    TORCH_FN(wrapper_XLA_out_mul_out));
    m.impl("mul_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_mul_));
    m.impl("mul.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_mul));
    m.impl("mul.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_mul_out));
    m.impl("mul_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_mul_));
    m.impl("multinomial",
    TORCH_FN(wrapper_XLA__multinomial));
    m.impl("multinomial.out",
    TORCH_FN(wrapper_XLA_out_multinomial_out));
    m.impl("mv",
    TORCH_FN(wrapper_XLA__mv));
    m.impl("mv.out",
    TORCH_FN(wrapper_XLA_out_mv_out));
    m.impl("mvlgamma",
    TORCH_FN(wrapper_XLA__mvlgamma));
    m.impl("mvlgamma.out",
    TORCH_FN(wrapper_XLA_out_mvlgamma_out));
    m.impl("mvlgamma_",
    TORCH_FN(wrapper_XLA__mvlgamma_));
    m.impl("nan_to_num",
    TORCH_FN(wrapper_XLA__nan_to_num));
    m.impl("nan_to_num.out",
    TORCH_FN(wrapper_XLA_out_nan_to_num_out));
    m.impl("nan_to_num_",
    TORCH_FN(wrapper_XLA__nan_to_num_));
    m.impl("narrow_copy",
    TORCH_FN(wrapper_XLA__narrow_copy));
    m.impl("narrow_copy.out",
    TORCH_FN(wrapper_XLA_out_narrow_copy_out));
    m.impl("native_batch_norm",
    TORCH_FN(wrapper_XLA__native_batch_norm));
    m.impl("native_batch_norm.out",
    TORCH_FN(wrapper_XLA_out_native_batch_norm_out));
    m.impl("native_batch_norm_backward",
    TORCH_FN(wrapper_XLA__native_batch_norm_backward));
    m.impl("native_batch_norm_backward.out",
    TORCH_FN(wrapper_XLA_out_native_batch_norm_backward_out));
    m.impl("native_dropout",
    TORCH_FN(wrapper_XLA__native_dropout));
    m.impl("native_dropout.out",
    TORCH_FN(wrapper_XLA_out_native_dropout_out));
    m.impl("native_dropout_backward",
    TORCH_FN(wrapper_XLA__native_dropout_backward));
    m.impl("native_dropout_backward.out",
    TORCH_FN(wrapper_XLA_out_native_dropout_backward_out));
    m.impl("ne.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_ne));
    m.impl("ne.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_ne_out));
    m.impl("ne_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_ne_));
    m.impl("ne.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_ne));
    m.impl("ne.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_ne_out));
    m.impl("ne_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_ne_));
    m.impl("neg",
    TORCH_FN(wrapper_XLA__neg));
    m.impl("neg.out",
    TORCH_FN(wrapper_XLA_out_neg_out));
    m.impl("neg_",
    TORCH_FN(wrapper_XLA__neg_));
    m.impl("new_empty_strided",
    TORCH_FN(wrapper_XLA__new_empty_strided));
    m.impl("new_empty_strided.out",
    TORCH_FN(wrapper_XLA_out_new_empty_strided_out));
    m.impl("nll_loss2d_backward",
    TORCH_FN(wrapper_XLA__nll_loss2d_backward));
    m.impl("nll_loss2d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_nll_loss2d_backward_out));
    m.impl("nll_loss2d_forward",
    TORCH_FN(wrapper_XLA__nll_loss2d_forward));
    m.impl("nll_loss2d_forward.output",
    TORCH_FN(wrapper_XLA_output_nll_loss2d_forward_out));
    m.impl("nll_loss_backward",
    TORCH_FN(wrapper_XLA__nll_loss_backward));
    m.impl("nll_loss_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_nll_loss_backward_out));
    m.impl("nll_loss_forward",
    TORCH_FN(wrapper_XLA__nll_loss_forward));
    m.impl("nll_loss_forward.output",
    TORCH_FN(wrapper_XLA_output_nll_loss_forward_out));
    m.impl("nonzero",
    TORCH_FN(wrapper_XLA__nonzero));
    m.impl("nonzero.out",
    TORCH_FN(wrapper_XLA_out_nonzero_out));
    m.impl("norm.ScalarOpt_dtype",
    TORCH_FN(wrapper_XLA_ScalarOpt_dtype_norm));
    m.impl("norm.ScalarOpt_dtype_out",
    TORCH_FN(wrapper_XLA_ScalarOpt_dtype_out_norm_out));
    m.impl("norm.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_norm));
    m.impl("norm.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_norm_out));
    m.impl("norm.ScalarOpt_dim_dtype",
    TORCH_FN(wrapper_XLA_ScalarOpt_dim_dtype_norm));
    m.impl("norm.dtype_out",
    TORCH_FN(wrapper_XLA_dtype_out_norm_out));
    m.impl("norm.ScalarOpt_dim",
    TORCH_FN(wrapper_XLA_ScalarOpt_dim_norm));
    m.impl("norm.out",
    TORCH_FN(wrapper_XLA_out_norm_out));
    m.impl("normal.Tensor_float",
    TORCH_FN(wrapper_XLA_Tensor_float_normal));
    m.impl("normal.Tensor_float_out",
    TORCH_FN(wrapper_XLA_Tensor_float_out_normal_out));
    m.impl("normal.float_Tensor",
    TORCH_FN(wrapper_XLA_float_Tensor_normal));
    m.impl("normal.float_Tensor_out",
    TORCH_FN(wrapper_XLA_float_Tensor_out_normal_out));
    m.impl("normal.Tensor_Tensor",
    TORCH_FN(wrapper_XLA_Tensor_Tensor_normal));
    m.impl("normal.Tensor_Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_Tensor_out_normal_out));
    m.impl("normal_",
    TORCH_FN(wrapper_XLA__normal_));
    m.impl("permute",
    TORCH_FN(wrapper_XLA__permute));
    m.impl("permute_copy",
    TORCH_FN(wrapper_XLA__permute_copy));
    m.impl("permute_copy.out",
    TORCH_FN(wrapper_XLA_out_permute_copy_out));
    m.impl("pixel_shuffle",
    TORCH_FN(wrapper_XLA__pixel_shuffle));
    m.impl("pixel_shuffle.out",
    TORCH_FN(wrapper_XLA_out_pixel_shuffle_out));
    m.impl("pixel_unshuffle",
    TORCH_FN(wrapper_XLA__pixel_unshuffle));
    m.impl("pixel_unshuffle.out",
    TORCH_FN(wrapper_XLA_out_pixel_unshuffle_out));
    m.impl("pow.Tensor_Tensor",
    TORCH_FN(wrapper_XLA_Tensor_Tensor_pow));
    m.impl("pow.Tensor_Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_Tensor_out_pow_out));
    m.impl("pow_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_pow_));
    m.impl("pow.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_pow));
    m.impl("pow.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_pow_out));
    m.impl("pow.Tensor_Scalar",
    TORCH_FN(wrapper_XLA_Tensor_Scalar_pow));
    m.impl("pow.Tensor_Scalar_out",
    TORCH_FN(wrapper_XLA_Tensor_Scalar_out_pow_out));
    m.impl("pow_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_pow_));
    m.impl("prod",
    TORCH_FN(wrapper_XLA__prod));
    m.impl("prod.out",
    TORCH_FN(wrapper_XLA_out_prod_out));
    m.impl("prod.dim_int",
    TORCH_FN(wrapper_XLA_dim_int_prod));
    m.impl("prod.int_out",
    TORCH_FN(wrapper_XLA_int_out_prod_out));
    m.impl("put_",
    TORCH_FN(wrapper_XLA__put_));
    m.impl("qr",
    TORCH_FN(wrapper_XLA__qr));
    m.impl("qr.Q",
    TORCH_FN(wrapper_XLA_Q_qr_out));
    m.impl("random_.from",
    TORCH_FN(wrapper_XLA_from_random_));
    m.impl("random_.to",
    TORCH_FN(wrapper_XLA_to_random_));
    m.impl("random_",
    TORCH_FN(wrapper_XLA__random_));
    m.impl("randperm",
    TORCH_FN(wrapper_XLA__randperm));
    m.impl("randperm.out",
    TORCH_FN(wrapper_XLA_out_randperm_out));
    m.impl("reciprocal",
    TORCH_FN(wrapper_XLA__reciprocal));
    m.impl("reciprocal.out",
    TORCH_FN(wrapper_XLA_out_reciprocal_out));
    m.impl("reciprocal_",
    TORCH_FN(wrapper_XLA__reciprocal_));
    m.impl("reflection_pad1d",
    TORCH_FN(wrapper_XLA__reflection_pad1d));
    m.impl("reflection_pad1d.out",
    TORCH_FN(wrapper_XLA_out_reflection_pad1d_out));
    m.impl("reflection_pad1d_backward",
    TORCH_FN(wrapper_XLA__reflection_pad1d_backward));
    m.impl("reflection_pad1d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_reflection_pad1d_backward_out));
    m.impl("reflection_pad2d",
    TORCH_FN(wrapper_XLA__reflection_pad2d));
    m.impl("reflection_pad2d.out",
    TORCH_FN(wrapper_XLA_out_reflection_pad2d_out));
    m.impl("reflection_pad2d_backward",
    TORCH_FN(wrapper_XLA__reflection_pad2d_backward));
    m.impl("reflection_pad2d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_reflection_pad2d_backward_out));
    m.impl("reflection_pad3d",
    TORCH_FN(wrapper_XLA__reflection_pad3d));
    m.impl("reflection_pad3d.out",
    TORCH_FN(wrapper_XLA_out_reflection_pad3d_out));
    m.impl("reflection_pad3d_backward",
    TORCH_FN(wrapper_XLA__reflection_pad3d_backward));
    m.impl("reflection_pad3d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_reflection_pad3d_backward_out));
    m.impl("relu",
    TORCH_FN(wrapper_XLA__relu));
    m.impl("relu.out",
    TORCH_FN(wrapper_XLA_out_relu_out));
    m.impl("relu_",
    TORCH_FN(wrapper_XLA__relu_));
    m.impl("remainder.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_remainder));
    m.impl("remainder.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_remainder_out));
    m.impl("remainder_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_remainder_));
    m.impl("remainder.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_remainder));
    m.impl("remainder.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_remainder_out));
    m.impl("remainder_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_remainder_));
    m.impl("repeat",
    TORCH_FN(wrapper_XLA__repeat));
    m.impl("repeat.out",
    TORCH_FN(wrapper_XLA_out_repeat_out));
    m.impl("replication_pad1d",
    TORCH_FN(wrapper_XLA__replication_pad1d));
    m.impl("replication_pad1d.out",
    TORCH_FN(wrapper_XLA_out_replication_pad1d_out));
    m.impl("replication_pad1d_backward",
    TORCH_FN(wrapper_XLA__replication_pad1d_backward));
    m.impl("replication_pad1d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_replication_pad1d_backward_out));
    m.impl("replication_pad2d",
    TORCH_FN(wrapper_XLA__replication_pad2d));
    m.impl("replication_pad2d.out",
    TORCH_FN(wrapper_XLA_out_replication_pad2d_out));
    m.impl("replication_pad2d_backward",
    TORCH_FN(wrapper_XLA__replication_pad2d_backward));
    m.impl("replication_pad2d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_replication_pad2d_backward_out));
    m.impl("replication_pad3d",
    TORCH_FN(wrapper_XLA__replication_pad3d));
    m.impl("replication_pad3d.out",
    TORCH_FN(wrapper_XLA_out_replication_pad3d_out));
    m.impl("replication_pad3d_backward",
    TORCH_FN(wrapper_XLA__replication_pad3d_backward));
    m.impl("replication_pad3d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_replication_pad3d_backward_out));
    m.impl("resize_",
    TORCH_FN(wrapper_XLA__resize_));
    m.impl("roll",
    TORCH_FN(wrapper_XLA__roll));
    m.impl("roll.out",
    TORCH_FN(wrapper_XLA_out_roll_out));
    m.impl("round",
    TORCH_FN(wrapper_XLA__round));
    m.impl("round.out",
    TORCH_FN(wrapper_XLA_out_round_out));
    m.impl("round_",
    TORCH_FN(wrapper_XLA__round_));
    m.impl("rrelu_with_noise_backward",
    TORCH_FN(wrapper_XLA__rrelu_with_noise_backward));
    m.impl("rrelu_with_noise_backward.out",
    TORCH_FN(wrapper_XLA_out_rrelu_with_noise_backward_out));
    m.impl("rrelu_with_noise",
    TORCH_FN(wrapper_XLA__rrelu_with_noise));
    m.impl("rsqrt",
    TORCH_FN(wrapper_XLA__rsqrt));
    m.impl("rsqrt.out",
    TORCH_FN(wrapper_XLA_out_rsqrt_out));
    m.impl("rsqrt_",
    TORCH_FN(wrapper_XLA__rsqrt_));
    m.impl("rsub.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_rsub));
    m.impl("rsub.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_rsub_out));
    m.impl("rsub.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_rsub));
    m.impl("rsub.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_rsub_out));
    m.impl("scatter.src",
    TORCH_FN(wrapper_XLA_src_scatter));
    m.impl("scatter.src_out",
    TORCH_FN(wrapper_XLA_src_out_scatter_out));
    m.impl("scatter_.src",
    TORCH_FN(wrapper_XLA_src_scatter_));
    m.impl("scatter.value",
    TORCH_FN(wrapper_XLA_value_scatter));
    m.impl("scatter.value_out",
    TORCH_FN(wrapper_XLA_value_out_scatter_out));
    m.impl("scatter_.value",
    TORCH_FN(wrapper_XLA_value_scatter_));
    m.impl("scatter.reduce",
    TORCH_FN(wrapper_XLA_reduce_scatter));
    m.impl("scatter.reduce_out",
    TORCH_FN(wrapper_XLA_reduce_out_scatter_out));
    m.impl("scatter_.reduce",
    TORCH_FN(wrapper_XLA_reduce_scatter_));
    m.impl("scatter.value_reduce",
    TORCH_FN(wrapper_XLA_value_reduce_scatter));
    m.impl("scatter.value_reduce_out",
    TORCH_FN(wrapper_XLA_value_reduce_out_scatter_out));
    m.impl("scatter_.value_reduce",
    TORCH_FN(wrapper_XLA_value_reduce_scatter_));
    m.impl("scatter_add",
    TORCH_FN(wrapper_XLA__scatter_add));
    m.impl("scatter_add.out",
    TORCH_FN(wrapper_XLA_out_scatter_add_out));
    m.impl("scatter_add_",
    TORCH_FN(wrapper_XLA__scatter_add_));
    m.impl("scatter_reduce.two",
    TORCH_FN(wrapper_XLA_two_scatter_reduce));
    m.impl("scatter_reduce.two_out",
    TORCH_FN(wrapper_XLA_two_out_scatter_reduce_out));
    m.impl("scatter_reduce_.two",
    TORCH_FN(wrapper_XLA_two_scatter_reduce_));
    m.impl("select.int",
    TORCH_FN(wrapper_XLA_int_select));
    m.impl("select_backward",
    TORCH_FN(wrapper_XLA__select_backward));
    m.impl("select_backward.out",
    TORCH_FN(wrapper_XLA_out_select_backward_out));
    m.impl("select_copy.int",
    TORCH_FN(wrapper_XLA_int_select_copy));
    m.impl("select_copy.int_out",
    TORCH_FN(wrapper_XLA_int_out_select_copy_out));
    m.impl("select_scatter",
    TORCH_FN(wrapper_XLA__select_scatter));
    m.impl("select_scatter.out",
    TORCH_FN(wrapper_XLA_out_select_scatter_out));
    m.impl("selu",
    TORCH_FN(wrapper_XLA__selu));
    m.impl("selu_",
    TORCH_FN(wrapper_XLA__selu_));
    m.impl("set_.source_Tensor",
    TORCH_FN(wrapper_XLA_source_Tensor_set_));
    m.impl("sgn",
    TORCH_FN(wrapper_XLA__sgn));
    m.impl("sgn.out",
    TORCH_FN(wrapper_XLA_out_sgn_out));
    m.impl("sgn_",
    TORCH_FN(wrapper_XLA__sgn_));
    m.impl("sigmoid",
    TORCH_FN(wrapper_XLA__sigmoid));
    m.impl("sigmoid.out",
    TORCH_FN(wrapper_XLA_out_sigmoid_out));
    m.impl("sigmoid_",
    TORCH_FN(wrapper_XLA__sigmoid_));
    m.impl("sigmoid_backward",
    TORCH_FN(wrapper_XLA__sigmoid_backward));
    m.impl("sigmoid_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_sigmoid_backward_out));
    m.impl("sign",
    TORCH_FN(wrapper_XLA__sign));
    m.impl("sign.out",
    TORCH_FN(wrapper_XLA_out_sign_out));
    m.impl("sign_",
    TORCH_FN(wrapper_XLA__sign_));
    m.impl("silu",
    TORCH_FN(wrapper_XLA__silu));
    m.impl("silu.out",
    TORCH_FN(wrapper_XLA_out_silu_out));
    m.impl("silu_",
    TORCH_FN(wrapper_XLA__silu_));
    m.impl("silu_backward",
    TORCH_FN(wrapper_XLA__silu_backward));
    m.impl("silu_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_silu_backward_out));
    m.impl("sin",
    TORCH_FN(wrapper_XLA__sin));
    m.impl("sin.out",
    TORCH_FN(wrapper_XLA_out_sin_out));
    m.impl("sin_",
    TORCH_FN(wrapper_XLA__sin_));
    m.impl("sinh",
    TORCH_FN(wrapper_XLA__sinh));
    m.impl("sinh.out",
    TORCH_FN(wrapper_XLA_out_sinh_out));
    m.impl("sinh_",
    TORCH_FN(wrapper_XLA__sinh_));
    m.impl("slice.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_slice));
    m.impl("slice_backward",
    TORCH_FN(wrapper_XLA__slice_backward));
    m.impl("slice_backward.out",
    TORCH_FN(wrapper_XLA_out_slice_backward_out));
    m.impl("slice_copy.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_slice_copy));
    m.impl("slice_copy.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_slice_copy_out));
    m.impl("slice_scatter",
    TORCH_FN(wrapper_XLA__slice_scatter));
    m.impl("slice_scatter.out",
    TORCH_FN(wrapper_XLA_out_slice_scatter_out));
    m.impl("smooth_l1_loss",
    TORCH_FN(wrapper_XLA__smooth_l1_loss));
    m.impl("smooth_l1_loss.out",
    TORCH_FN(wrapper_XLA_out_smooth_l1_loss_out));
    m.impl("smooth_l1_loss_backward",
    TORCH_FN(wrapper_XLA__smooth_l1_loss_backward));
    m.impl("smooth_l1_loss_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_smooth_l1_loss_backward_out));
    m.impl("softplus",
    TORCH_FN(wrapper_XLA__softplus));
    m.impl("softplus.out",
    TORCH_FN(wrapper_XLA_out_softplus_out));
    m.impl("softplus_backward",
    TORCH_FN(wrapper_XLA__softplus_backward));
    m.impl("softplus_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_softplus_backward_out));
    m.impl("softshrink",
    TORCH_FN(wrapper_XLA__softshrink));
    m.impl("softshrink.out",
    TORCH_FN(wrapper_XLA_out_softshrink_out));
    m.impl("softshrink_backward",
    TORCH_FN(wrapper_XLA__softshrink_backward));
    m.impl("softshrink_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_softshrink_backward_out));
    m.impl("sort",
    TORCH_FN(wrapper_XLA__sort));
    m.impl("sort.values",
    TORCH_FN(wrapper_XLA_values_sort_out));
    m.impl("sort.stable",
    TORCH_FN(wrapper_XLA_stable_sort));
    m.impl("sort.values_stable",
    TORCH_FN(wrapper_XLA_values_stable_sort_out));
    m.impl("split_copy.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_split_copy));
    m.impl("split_copy.Tensor_out",
    TORCH_FN(wrapper_XLA_Tensor_out_split_copy_out));
    m.impl("split_with_sizes_copy",
    TORCH_FN(wrapper_XLA__split_with_sizes_copy));
    m.impl("split_with_sizes_copy.out",
    TORCH_FN(wrapper_XLA_out_split_with_sizes_copy_out));
    m.impl("sqrt",
    TORCH_FN(wrapper_XLA__sqrt));
    m.impl("sqrt.out",
    TORCH_FN(wrapper_XLA_out_sqrt_out));
    m.impl("sqrt_",
    TORCH_FN(wrapper_XLA__sqrt_));
    m.impl("squeeze_copy",
    TORCH_FN(wrapper_XLA__squeeze_copy));
    m.impl("squeeze_copy.out",
    TORCH_FN(wrapper_XLA_out_squeeze_copy_out));
    m.impl("squeeze_copy.dim",
    TORCH_FN(wrapper_XLA_dim_squeeze_copy));
    m.impl("squeeze_copy.dim_out",
    TORCH_FN(wrapper_XLA_dim_out_squeeze_copy_out));
    m.impl("squeeze_copy.dims",
    TORCH_FN(wrapper_XLA_dims_squeeze_copy));
    m.impl("squeeze_copy.dims_out",
    TORCH_FN(wrapper_XLA_dims_out_squeeze_copy_out));
    m.impl("stack",
    TORCH_FN(wrapper_XLA__stack));
    m.impl("stack.out",
    TORCH_FN(wrapper_XLA_out_stack_out));
    m.impl("std",
    TORCH_FN(wrapper_XLA__std));
    m.impl("std.dim",
    TORCH_FN(wrapper_XLA_dim_std));
    m.impl("std.out",
    TORCH_FN(wrapper_XLA_out_std_out));
    m.impl("std.correction",
    TORCH_FN(wrapper_XLA_correction_std));
    m.impl("std.correction_out",
    TORCH_FN(wrapper_XLA_correction_out_std_out));
    m.impl("std_mean.correction",
    TORCH_FN(wrapper_XLA_correction_std_mean));
    m.impl("std_mean.correction_out",
    TORCH_FN(wrapper_XLA_correction_out_std_mean_out));
    m.impl("sub.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_sub));
    m.impl("sub.out",
    TORCH_FN(wrapper_XLA_out_sub_out));
    m.impl("sub_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_sub_));
    m.impl("sub.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_sub));
    m.impl("sub.Scalar_out",
    TORCH_FN(wrapper_XLA_Scalar_out_sub_out));
    m.impl("sub_.Scalar",
    TORCH_FN(wrapper_XLA_Scalar_sub_));
    m.impl("sum",
    TORCH_FN(wrapper_XLA__sum));
    m.impl("sum.out",
    TORCH_FN(wrapper_XLA_out_sum_out));
    m.impl("sum.dim_IntList",
    TORCH_FN(wrapper_XLA_dim_IntList_sum));
    m.impl("sum.IntList_out",
    TORCH_FN(wrapper_XLA_IntList_out_sum_out));
    m.impl("svd",
    TORCH_FN(wrapper_XLA__svd));
    m.impl("svd.U",
    TORCH_FN(wrapper_XLA_U_svd_out));
    m.impl("t",
    TORCH_FN(wrapper_XLA__t));
    m.impl("t_copy",
    TORCH_FN(wrapper_XLA__t_copy));
    m.impl("t_copy.out",
    TORCH_FN(wrapper_XLA_out_t_copy_out));
    m.impl("take",
    TORCH_FN(wrapper_XLA__take));
    m.impl("take.out",
    TORCH_FN(wrapper_XLA_out_take_out));
    m.impl("tan",
    TORCH_FN(wrapper_XLA__tan));
    m.impl("tan.out",
    TORCH_FN(wrapper_XLA_out_tan_out));
    m.impl("tan_",
    TORCH_FN(wrapper_XLA__tan_));
    m.impl("tanh",
    TORCH_FN(wrapper_XLA__tanh));
    m.impl("tanh.out",
    TORCH_FN(wrapper_XLA_out_tanh_out));
    m.impl("tanh_",
    TORCH_FN(wrapper_XLA__tanh_));
    m.impl("tanh_backward",
    TORCH_FN(wrapper_XLA__tanh_backward));
    m.impl("tanh_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_tanh_backward_out));
    m.impl("threshold",
    TORCH_FN(wrapper_XLA__threshold));
    m.impl("threshold.out",
    TORCH_FN(wrapper_XLA_out_threshold_out));
    m.impl("threshold_",
    TORCH_FN(wrapper_XLA__threshold_));
    m.impl("threshold_backward",
    TORCH_FN(wrapper_XLA__threshold_backward));
    m.impl("threshold_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_threshold_backward_out));
    m.impl("topk",
    TORCH_FN(wrapper_XLA__topk));
    m.impl("topk.values",
    TORCH_FN(wrapper_XLA_values_topk_out));
    m.impl("trace",
    TORCH_FN(wrapper_XLA__trace));
    m.impl("trace.out",
    TORCH_FN(wrapper_XLA_out_trace_out));
    m.impl("transpose_copy.int",
    TORCH_FN(wrapper_XLA_int_transpose_copy));
    m.impl("transpose_copy.int_out",
    TORCH_FN(wrapper_XLA_int_out_transpose_copy_out));
    m.impl("triangular_solve",
    TORCH_FN(wrapper_XLA__triangular_solve));
    m.impl("triangular_solve.X",
    TORCH_FN(wrapper_XLA_X_triangular_solve_out));
    m.impl("tril",
    TORCH_FN(wrapper_XLA__tril));
    m.impl("tril.out",
    TORCH_FN(wrapper_XLA_out_tril_out));
    m.impl("tril_",
    TORCH_FN(wrapper_XLA__tril_));
    m.impl("triu",
    TORCH_FN(wrapper_XLA__triu));
    m.impl("triu.out",
    TORCH_FN(wrapper_XLA_out_triu_out));
    m.impl("triu_",
    TORCH_FN(wrapper_XLA__triu_));
    m.impl("trunc",
    TORCH_FN(wrapper_XLA__trunc));
    m.impl("trunc.out",
    TORCH_FN(wrapper_XLA_out_trunc_out));
    m.impl("trunc_",
    TORCH_FN(wrapper_XLA__trunc_));
    m.impl("unbind_copy.int",
    TORCH_FN(wrapper_XLA_int_unbind_copy));
    m.impl("unbind_copy.int_out",
    TORCH_FN(wrapper_XLA_int_out_unbind_copy_out));
    m.impl("uniform_",
    TORCH_FN(wrapper_XLA__uniform_));
    m.impl("unsqueeze_copy",
    TORCH_FN(wrapper_XLA__unsqueeze_copy));
    m.impl("unsqueeze_copy.out",
    TORCH_FN(wrapper_XLA_out_unsqueeze_copy_out));
    m.impl("upsample_bilinear2d",
    TORCH_FN(wrapper_XLA__upsample_bilinear2d));
    m.impl("upsample_bilinear2d.out",
    TORCH_FN(wrapper_XLA_out_upsample_bilinear2d_out));
    m.impl("upsample_bilinear2d_backward",
    TORCH_FN(wrapper_XLA__upsample_bilinear2d_backward));
    m.impl("upsample_bilinear2d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_upsample_bilinear2d_backward_out));
    m.impl("upsample_nearest2d",
    TORCH_FN(wrapper_XLA__upsample_nearest2d));
    m.impl("upsample_nearest2d.out",
    TORCH_FN(wrapper_XLA_out_upsample_nearest2d_out));
    m.impl("upsample_nearest2d_backward",
    TORCH_FN(wrapper_XLA__upsample_nearest2d_backward));
    m.impl("upsample_nearest2d_backward.grad_input",
    TORCH_FN(wrapper_XLA_grad_input_upsample_nearest2d_backward_out));
    m.impl("var.correction",
    TORCH_FN(wrapper_XLA_correction_var));
    m.impl("var.correction_out",
    TORCH_FN(wrapper_XLA_correction_out_var_out));
    m.impl("var_mean.correction",
    TORCH_FN(wrapper_XLA_correction_var_mean));
    m.impl("var_mean.correction_out",
    TORCH_FN(wrapper_XLA_correction_out_var_mean_out));
    m.impl("view",
    TORCH_FN(wrapper_XLA__view));
    m.impl("view_as_complex_copy",
    TORCH_FN(wrapper_XLA__view_as_complex_copy));
    m.impl("view_as_complex_copy.out",
    TORCH_FN(wrapper_XLA_out_view_as_complex_copy_out));
    m.impl("view_as_real_copy",
    TORCH_FN(wrapper_XLA__view_as_real_copy));
    m.impl("view_as_real_copy.out",
    TORCH_FN(wrapper_XLA_out_view_as_real_copy_out));
    m.impl("view_copy",
    TORCH_FN(wrapper_XLA__view_copy));
    m.impl("view_copy.out",
    TORCH_FN(wrapper_XLA_out_view_copy_out));
    m.impl("where.self",
    TORCH_FN(wrapper_XLA_self_where));
    m.impl("where.self_out",
    TORCH_FN(wrapper_XLA_self_out_where_out));
    m.impl("xlogy.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_xlogy));
    m.impl("xlogy.OutTensor",
    TORCH_FN(wrapper_XLA_OutTensor_xlogy_out));
    m.impl("xlogy_.Tensor",
    TORCH_FN(wrapper_XLA_Tensor_xlogy_));
    m.impl("zero_",
    TORCH_FN(wrapper_XLA__zero_));
}
namespace xla {
} // namespace xla
} // namespace at
