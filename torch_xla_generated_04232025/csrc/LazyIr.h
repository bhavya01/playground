#pragma once

// This file contains autogenerated LazyTensor IR nodes
#include <ATen/core/Formatting.h>
#include <c10/core/ScalarType.h>
#include <torch/csrc/lazy/core/hash.h>
#include <torch/csrc/lazy/core/ir.h>
#include <torch/csrc/lazy/core/shape.h>
#include <optional>
#include <vector>
#include "torch_xla/csrc/generated_file_include.h"

namespace torch_xla {
using at::operator<<;

// kNullValue is used to contribute a static hash value any time
// a node has an Optional<Value> input that is nullopt.  It is important
// to differentiate between HASH(std::nullopt, something) and HASH(something, std::nullopt),
// and using kNullValue in the hash function in the order of arguments
// serves this purpose.
static const torch::lazy::Value kNullValue = torch::lazy::Value();

class AdaptiveAvgPool2d : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::_adaptive_avg_pool2d);
  }

  AdaptiveAvgPool2d(const torch::lazy::Value& self, const ::std::vector<int64_t>& output_size)
      : XlaNode(torch::lazy::OpKind(at::aten::_adaptive_avg_pool2d),
              {self},
              [&]() { return AdaptiveAvgPool2dOutputShape(self, output_size); },
              /* num_outputs */ 1,
              torch::lazy::MHash(output_size)),
        output_size(output_size)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", output_size=" << output_size;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const ::std::vector<int64_t>& output_size) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  ::std::vector<int64_t> output_size;
  

};

class AdaptiveAvgPool2dBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::_adaptive_avg_pool2d_backward);
  }

  AdaptiveAvgPool2dBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::_adaptive_avg_pool2d_backward),
              {grad_output, self},
              [&]() { return AdaptiveAvgPool2dBackwardOutputShape(grad_output, self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class AdaptiveAvgPool3d : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::_adaptive_avg_pool3d);
  }

  AdaptiveAvgPool3d(const torch::lazy::Value& self, const ::std::vector<int64_t>& output_size)
      : XlaNode(torch::lazy::OpKind(at::aten::_adaptive_avg_pool3d),
              {self},
              [&]() { return AdaptiveAvgPool3dOutputShape(self, output_size); },
              /* num_outputs */ 1,
              torch::lazy::MHash(output_size)),
        output_size(output_size)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", output_size=" << output_size;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const ::std::vector<int64_t>& output_size) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  ::std::vector<int64_t> output_size;
  

};

class AdaptiveAvgPool3dBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::_adaptive_avg_pool3d_backward);
  }

  AdaptiveAvgPool3dBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::_adaptive_avg_pool3d_backward),
              {grad_output, self},
              [&]() { return AdaptiveAvgPool3dBackwardOutputShape(grad_output, self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class ConjCopy : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::_conj_copy);
  }

  ConjCopy(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::_conj_copy),
              {self},
              [&]() { return ConjCopyOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Abs : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::abs);
  }

  Abs(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::abs),
              {self},
              [&]() { return AbsOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Acos : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::acos);
  }

  Acos(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::acos),
              {self},
              [&]() { return AcosOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Acosh : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::acosh);
  }

  Acosh(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::acosh),
              {self},
              [&]() { return AcoshOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Addcdiv : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::addcdiv);
  }

  Addcdiv(const torch::lazy::Value& self, const torch::lazy::Value& tensor1, const torch::lazy::Value& tensor2, const torch::lazy::Value& value)
      : XlaNode(torch::lazy::OpKind(at::aten::addcdiv),
              {self, tensor1, tensor2, value},
              [&]() { return AddcdivOutputShape(self, tensor1, tensor2, value); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& tensor1, const torch::lazy::Value& tensor2, const torch::lazy::Value& value) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Addcmul : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::addcmul);
  }

  Addcmul(const torch::lazy::Value& self, const torch::lazy::Value& tensor1, const torch::lazy::Value& tensor2, const torch::lazy::Value& value)
      : XlaNode(torch::lazy::OpKind(at::aten::addcmul),
              {self, tensor1, tensor2, value},
              [&]() { return AddcmulOutputShape(self, tensor1, tensor2, value); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& tensor1, const torch::lazy::Value& tensor2, const torch::lazy::Value& value) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class AllDim : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::all);
  }

  AllDim(const torch::lazy::Value& self, const int64_t& dim, const bool& keepdim)
      : XlaNode(torch::lazy::OpKind(at::aten::all),
              {self},
              [&]() { return AllDimOutputShape(self, dim, keepdim); },
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim)),
        dim(dim),
        keepdim(keepdim)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", dim=" << dim;
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const int64_t& dim, const bool& keepdim) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  int64_t dim;
  bool keepdim;
  

};

class All : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::all);
  }

  All(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::all),
              {self},
              [&]() { return AllOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Amax : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::amax);
  }

  Amax(const torch::lazy::Value& self, const ::std::vector<int64_t>& dim, const bool& keepdim)
      : XlaNode(torch::lazy::OpKind(at::aten::amax),
              {self},
              [&]() { return AmaxOutputShape(self, dim, keepdim); },
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim)),
        dim(dim),
        keepdim(keepdim)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", dim=" << dim;
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const ::std::vector<int64_t>& dim, const bool& keepdim) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  ::std::vector<int64_t> dim;
  bool keepdim;
  

};

class Amin : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::amin);
  }

  Amin(const torch::lazy::Value& self, const ::std::vector<int64_t>& dim, const bool& keepdim)
      : XlaNode(torch::lazy::OpKind(at::aten::amin),
              {self},
              [&]() { return AminOutputShape(self, dim, keepdim); },
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim)),
        dim(dim),
        keepdim(keepdim)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", dim=" << dim;
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const ::std::vector<int64_t>& dim, const bool& keepdim) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  ::std::vector<int64_t> dim;
  bool keepdim;
  

};

class AnyDim : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::any);
  }

  AnyDim(const torch::lazy::Value& self, const int64_t& dim, const bool& keepdim)
      : XlaNode(torch::lazy::OpKind(at::aten::any),
              {self},
              [&]() { return AnyDimOutputShape(self, dim, keepdim); },
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim)),
        dim(dim),
        keepdim(keepdim)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", dim=" << dim;
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const int64_t& dim, const bool& keepdim) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  int64_t dim;
  bool keepdim;
  

};

class Any : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::any);
  }

  Any(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::any),
              {self},
              [&]() { return AnyOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Argmax : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::argmax);
  }

  Argmax(const torch::lazy::Value& self, const ::std::optional<int64_t>& dim, const bool& keepdim)
      : XlaNode(torch::lazy::OpKind(at::aten::argmax),
              {self},
              [&]() { return ArgmaxOutputShape(self, dim, keepdim); },
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim)),
        dim(dim),
        keepdim(keepdim)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    if (dim.has_value()) {
      ss << ", dim=" << dim.value();
    } else {
      ss << ", dim=null";
    }
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const ::std::optional<int64_t>& dim, const bool& keepdim) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  ::std::optional<int64_t> dim;
  bool keepdim;
  

};

class Argmin : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::argmin);
  }

  Argmin(const torch::lazy::Value& self, const ::std::optional<int64_t>& dim, const bool& keepdim)
      : XlaNode(torch::lazy::OpKind(at::aten::argmin),
              {self},
              [&]() { return ArgminOutputShape(self, dim, keepdim); },
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim)),
        dim(dim),
        keepdim(keepdim)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    if (dim.has_value()) {
      ss << ", dim=" << dim.value();
    } else {
      ss << ", dim=null";
    }
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const ::std::optional<int64_t>& dim, const bool& keepdim) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  ::std::optional<int64_t> dim;
  bool keepdim;
  

};

class Asin : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::asin);
  }

  Asin(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::asin),
              {self},
              [&]() { return AsinOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Asinh : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::asinh);
  }

  Asinh(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::asinh),
              {self},
              [&]() { return AsinhOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Atan : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::atan);
  }

  Atan(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::atan),
              {self},
              [&]() { return AtanOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Atan2 : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::atan2);
  }

  Atan2(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::atan2),
              {self, other},
              [&]() { return Atan2OutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Atanh : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::atanh);
  }

  Atanh(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::atanh),
              {self},
              [&]() { return AtanhOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Baddbmm : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::baddbmm);
  }

  Baddbmm(const torch::lazy::Value& self, const torch::lazy::Value& batch1, const torch::lazy::Value& batch2, const torch::lazy::Value& beta, const torch::lazy::Value& alpha)
      : XlaNode(torch::lazy::OpKind(at::aten::baddbmm),
              {self, batch1, batch2, beta, alpha},
              [&]() { return BaddbmmOutputShape(self, batch1, batch2, beta, alpha); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& batch1, const torch::lazy::Value& batch2, const torch::lazy::Value& beta, const torch::lazy::Value& alpha) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class BinaryCrossEntropy : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::binary_cross_entropy);
  }

  BinaryCrossEntropy(const torch::lazy::Value& self, const torch::lazy::Value& target, const ::std::optional<torch::lazy::Value>& weight, const int64_t& reduction)
      : XlaNode(torch::lazy::OpKind(at::aten::binary_cross_entropy),
              {self, target, weight.value_or(kNullValue)},
              [&]() { return BinaryCrossEntropyOutputShape(self, target, weight, reduction); },
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction)),
        reduction(reduction)
  {
    has_weight = !!weight;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", reduction=" << reduction;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& target, const ::std::optional<torch::lazy::Value>& weight, const int64_t& reduction) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  int64_t reduction;
  bool has_weight: 1;

};

class BinaryCrossEntropyBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::binary_cross_entropy_backward);
  }

  BinaryCrossEntropyBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& target, const ::std::optional<torch::lazy::Value>& weight, const int64_t& reduction)
      : XlaNode(torch::lazy::OpKind(at::aten::binary_cross_entropy_backward),
              {grad_output, self, target, weight.value_or(kNullValue)},
              [&]() { return BinaryCrossEntropyBackwardOutputShape(grad_output, self, target, weight, reduction); },
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction)),
        reduction(reduction)
  {
    has_weight = !!weight;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", reduction=" << reduction;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& target, const ::std::optional<torch::lazy::Value>& weight, const int64_t& reduction) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  int64_t reduction;
  bool has_weight: 1;

};

class BitwiseAndTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::bitwise_and);
  }

  BitwiseAndTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::bitwise_and),
              {self, other},
              [&]() { return BitwiseAndTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class BitwiseLeftShiftTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::bitwise_left_shift);
  }

  BitwiseLeftShiftTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::bitwise_left_shift),
              {self, other},
              [&]() { return BitwiseLeftShiftTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class BitwiseNot : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::bitwise_not);
  }

  BitwiseNot(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::bitwise_not),
              {self},
              [&]() { return BitwiseNotOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class BitwiseOrTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::bitwise_or);
  }

  BitwiseOrTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::bitwise_or),
              {self, other},
              [&]() { return BitwiseOrTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class BitwiseRightShiftTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::bitwise_right_shift);
  }

  BitwiseRightShiftTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::bitwise_right_shift),
              {self, other},
              [&]() { return BitwiseRightShiftTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class BitwiseXorTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::bitwise_xor);
  }

  BitwiseXorTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::bitwise_xor),
              {self, other},
              [&]() { return BitwiseXorTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Ceil : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::ceil);
  }

  Ceil(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::ceil),
              {self},
              [&]() { return CeilOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Cholesky : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::cholesky);
  }

  Cholesky(const torch::lazy::Value& self, const bool& upper)
      : XlaNode(torch::lazy::OpKind(at::aten::cholesky),
              {self},
              [&]() { return CholeskyOutputShape(self, upper); },
              /* num_outputs */ 1,
              torch::lazy::MHash(upper)),
        upper(upper)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", upper=" << upper;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const bool& upper) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  bool upper;
  

};

class ClampTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::clamp);
  }

  ClampTensor(const torch::lazy::Value& self, const ::std::optional<torch::lazy::Value>& min, const ::std::optional<torch::lazy::Value>& max)
      : XlaNode(torch::lazy::OpKind(at::aten::clamp),
              {self, min.value_or(kNullValue), max.value_or(kNullValue)},
              [&]() { return ClampTensorOutputShape(self, min, max); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    has_min = !!min;
    has_max = !!max;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const ::std::optional<torch::lazy::Value>& min, const ::std::optional<torch::lazy::Value>& max) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  bool has_min: 1;
  bool has_max: 1;

};

class ClampMaxTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::clamp_max);
  }

  ClampMaxTensor(const torch::lazy::Value& self, const torch::lazy::Value& max)
      : XlaNode(torch::lazy::OpKind(at::aten::clamp_max),
              {self, max},
              [&]() { return ClampMaxTensorOutputShape(self, max); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& max) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class ClampMinTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::clamp_min);
  }

  ClampMinTensor(const torch::lazy::Value& self, const torch::lazy::Value& min)
      : XlaNode(torch::lazy::OpKind(at::aten::clamp_min),
              {self, min},
              [&]() { return ClampMinTensorOutputShape(self, min); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& min) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Cos : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::cos);
  }

  Cos(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::cos),
              {self},
              [&]() { return CosOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Cosh : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::cosh);
  }

  Cosh(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::cosh),
              {self},
              [&]() { return CoshOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Elu : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::elu);
  }

  Elu(const torch::lazy::Value& self, const torch::lazy::Value& alpha, const torch::lazy::Value& scale, const torch::lazy::Value& input_scale)
      : XlaNode(torch::lazy::OpKind(at::aten::elu),
              {self, alpha, scale, input_scale},
              [&]() { return EluOutputShape(self, alpha, scale, input_scale); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& alpha, const torch::lazy::Value& scale, const torch::lazy::Value& input_scale) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class EqScalar : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::eq);
  }

  EqScalar(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::eq),
              {self, other},
              [&]() { return EqScalarOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class EqTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::eq);
  }

  EqTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::eq),
              {self, other},
              [&]() { return EqTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Erf : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::erf);
  }

  Erf(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::erf),
              {self},
              [&]() { return ErfOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Erfc : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::erfc);
  }

  Erfc(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::erfc),
              {self},
              [&]() { return ErfcOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Erfinv : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::erfinv);
  }

  Erfinv(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::erfinv),
              {self},
              [&]() { return ErfinvOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Exp : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::exp);
  }

  Exp(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::exp),
              {self},
              [&]() { return ExpOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Expm1 : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::expm1);
  }

  Expm1(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::expm1),
              {self},
              [&]() { return Expm1OutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Floor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::floor);
  }

  Floor(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::floor),
              {self},
              [&]() { return FloorOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Frac : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::frac);
  }

  Frac(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::frac),
              {self},
              [&]() { return FracOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class GeScalar : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::ge);
  }

  GeScalar(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::ge),
              {self, other},
              [&]() { return GeScalarOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class GeTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::ge);
  }

  GeTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::ge),
              {self, other},
              [&]() { return GeTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Glu : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::glu);
  }

  Glu(const torch::lazy::Value& self, const int64_t& dim)
      : XlaNode(torch::lazy::OpKind(at::aten::glu),
              {self},
              [&]() { return GluOutputShape(self, dim); },
              /* num_outputs */ 1,
              torch::lazy::MHash(dim)),
        dim(dim)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", dim=" << dim;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const int64_t& dim) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  int64_t dim;
  

};

class GtScalar : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::gt);
  }

  GtScalar(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::gt),
              {self, other},
              [&]() { return GtScalarOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class GtTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::gt);
  }

  GtTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::gt),
              {self, other},
              [&]() { return GtTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Hardshrink : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::hardshrink);
  }

  Hardshrink(const torch::lazy::Value& self, const torch::lazy::Value& lambd)
      : XlaNode(torch::lazy::OpKind(at::aten::hardshrink),
              {self, lambd},
              [&]() { return HardshrinkOutputShape(self, lambd); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& lambd) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class HardshrinkBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::hardshrink_backward);
  }

  HardshrinkBackward(const torch::lazy::Value& grad_out, const torch::lazy::Value& self, const torch::lazy::Value& lambd)
      : XlaNode(torch::lazy::OpKind(at::aten::hardshrink_backward),
              {grad_out, self, lambd},
              [&]() { return HardshrinkBackwardOutputShape(grad_out, self, lambd); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_out, const torch::lazy::Value& self, const torch::lazy::Value& lambd) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Hardsigmoid : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::hardsigmoid);
  }

  Hardsigmoid(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::hardsigmoid),
              {self},
              [&]() { return HardsigmoidOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class HardsigmoidBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::hardsigmoid_backward);
  }

  HardsigmoidBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::hardsigmoid_backward),
              {grad_output, self},
              [&]() { return HardsigmoidBackwardOutputShape(grad_output, self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Hardswish : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::hardswish);
  }

  Hardswish(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::hardswish),
              {self},
              [&]() { return HardswishOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class HardswishBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::hardswish_backward);
  }

  HardswishBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::hardswish_backward),
              {grad_output, self},
              [&]() { return HardswishBackwardOutputShape(grad_output, self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Inverse : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::inverse);
  }

  Inverse(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::inverse),
              {self},
              [&]() { return InverseOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Isnan : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::isnan);
  }

  Isnan(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::isnan),
              {self},
              [&]() { return IsnanOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Isneginf : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::isneginf);
  }

  Isneginf(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::isneginf),
              {self},
              [&]() { return IsneginfOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LeScalar : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::le);
  }

  LeScalar(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::le),
              {self, other},
              [&]() { return LeScalarOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LeTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::le);
  }

  LeTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::le),
              {self, other},
              [&]() { return LeTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LeakyRelu : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::leaky_relu);
  }

  LeakyRelu(const torch::lazy::Value& self, const torch::lazy::Value& negative_slope)
      : XlaNode(torch::lazy::OpKind(at::aten::leaky_relu),
              {self, negative_slope},
              [&]() { return LeakyReluOutputShape(self, negative_slope); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& negative_slope) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LeakyReluBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::leaky_relu_backward);
  }

  LeakyReluBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& negative_slope, const bool& self_is_result)
      : XlaNode(torch::lazy::OpKind(at::aten::leaky_relu_backward),
              {grad_output, self, negative_slope},
              [&]() { return LeakyReluBackwardOutputShape(grad_output, self, negative_slope, self_is_result); },
              /* num_outputs */ 1,
              torch::lazy::MHash(self_is_result)),
        self_is_result(self_is_result)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", self_is_result=" << self_is_result;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& negative_slope, const bool& self_is_result) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  bool self_is_result;
  

};

class LogSigmoidBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::log_sigmoid_backward);
  }

  LogSigmoidBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& buffer)
      : XlaNode(torch::lazy::OpKind(at::aten::log_sigmoid_backward),
              {grad_output, self, buffer},
              [&]() { return LogSigmoidBackwardOutputShape(grad_output, self, buffer); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& buffer) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LogSigmoidForward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::log_sigmoid_forward);
  }

  LogSigmoidForward(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::log_sigmoid_forward),
              {self},
              [&]() { return LogSigmoidForwardOutputShape(self); },
              /* num_outputs */ 2,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Logdet : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::logdet);
  }

  Logdet(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::logdet),
              {self},
              [&]() { return LogdetOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LogicalAnd : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::logical_and);
  }

  LogicalAnd(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::logical_and),
              {self, other},
              [&]() { return LogicalAndOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LogicalNot : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::logical_not);
  }

  LogicalNot(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::logical_not),
              {self},
              [&]() { return LogicalNotOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LogicalOr : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::logical_or);
  }

  LogicalOr(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::logical_or),
              {self, other},
              [&]() { return LogicalOrOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LogicalXor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::logical_xor);
  }

  LogicalXor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::logical_xor),
              {self, other},
              [&]() { return LogicalXorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LtScalar : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::lt);
  }

  LtScalar(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::lt),
              {self, other},
              [&]() { return LtScalarOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class LtTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::lt);
  }

  LtTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::lt),
              {self, other},
              [&]() { return LtTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class MaskedFillScalar : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::masked_fill);
  }

  MaskedFillScalar(const torch::lazy::Value& self, const torch::lazy::Value& mask, const torch::lazy::Value& value)
      : XlaNode(torch::lazy::OpKind(at::aten::masked_fill),
              {self, mask, value},
              [&]() { return MaskedFillScalarOutputShape(self, mask, value); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& mask, const torch::lazy::Value& value) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class MaskedFillTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::masked_fill);
  }

  MaskedFillTensor(const torch::lazy::Value& self, const torch::lazy::Value& mask, const torch::lazy::Value& value)
      : XlaNode(torch::lazy::OpKind(at::aten::masked_fill),
              {self, mask, value},
              [&]() { return MaskedFillTensorOutputShape(self, mask, value); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& mask, const torch::lazy::Value& value) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Maximum : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::maximum);
  }

  Maximum(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::maximum),
              {self, other},
              [&]() { return MaximumOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Minimum : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::minimum);
  }

  Minimum(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::minimum),
              {self, other},
              [&]() { return MinimumOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class NativeDropoutBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::native_dropout_backward);
  }

  NativeDropoutBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& mask, const double& scale)
      : XlaNode(torch::lazy::OpKind(at::aten::native_dropout_backward),
              {grad_output, mask},
              [&]() { return NativeDropoutBackwardOutputShape(grad_output, mask); },
              /* num_outputs */ 1,
              torch::lazy::MHash(scale)),
        scale(scale)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", scale=" << scale;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& mask, const double& scale) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  double scale;
  

};

class NeScalar : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::ne);
  }

  NeScalar(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::ne),
              {self, other},
              [&]() { return NeScalarOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class NeTensor : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::ne);
  }

  NeTensor(const torch::lazy::Value& self, const torch::lazy::Value& other)
      : XlaNode(torch::lazy::OpKind(at::aten::ne),
              {self, other},
              [&]() { return NeTensorOutputShape(self, other); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& other) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Reciprocal : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::reciprocal);
  }

  Reciprocal(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::reciprocal),
              {self},
              [&]() { return ReciprocalOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Relu : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::relu);
  }

  Relu(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::relu),
              {self},
              [&]() { return ReluOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Repeat : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::repeat);
  }

  Repeat(const torch::lazy::Value& self, const ::std::vector<int64_t>& repeats)
      : XlaNode(torch::lazy::OpKind(at::aten::repeat),
              {self},
              [&]() { return RepeatOutputShape(self, repeats); },
              /* num_outputs */ 1,
              torch::lazy::MHash(repeats)),
        repeats(repeats)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", repeats=" << repeats;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const ::std::vector<int64_t>& repeats) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  ::std::vector<int64_t> repeats;
  

};

class Round : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::round);
  }

  Round(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::round),
              {self},
              [&]() { return RoundOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Rsqrt : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::rsqrt);
  }

  Rsqrt(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::rsqrt),
              {self},
              [&]() { return RsqrtOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Selu : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::selu);
  }

  Selu(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::selu),
              {self},
              [&]() { return SeluOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Sgn : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::sgn);
  }

  Sgn(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::sgn),
              {self},
              [&]() { return SgnOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Sigmoid : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::sigmoid);
  }

  Sigmoid(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::sigmoid),
              {self},
              [&]() { return SigmoidOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Sign : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::sign);
  }

  Sign(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::sign),
              {self},
              [&]() { return SignOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Silu : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::silu);
  }

  Silu(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::silu),
              {self},
              [&]() { return SiluOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class SiluBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::silu_backward);
  }

  SiluBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::silu_backward),
              {grad_output, self},
              [&]() { return SiluBackwardOutputShape(grad_output, self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Sin : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::sin);
  }

  Sin(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::sin),
              {self},
              [&]() { return SinOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Sinh : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::sinh);
  }

  Sinh(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::sinh),
              {self},
              [&]() { return SinhOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Softshrink : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::softshrink);
  }

  Softshrink(const torch::lazy::Value& self, const torch::lazy::Value& lambd)
      : XlaNode(torch::lazy::OpKind(at::aten::softshrink),
              {self, lambd},
              [&]() { return SoftshrinkOutputShape(self, lambd); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& lambd) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class SoftshrinkBackward : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::softshrink_backward);
  }

  SoftshrinkBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& lambd)
      : XlaNode(torch::lazy::OpKind(at::aten::softshrink_backward),
              {grad_output, self, lambd},
              [&]() { return SoftshrinkBackwardOutputShape(grad_output, self, lambd); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& lambd) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Sqrt : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::sqrt);
  }

  Sqrt(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::sqrt),
              {self},
              [&]() { return SqrtOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Take : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::take);
  }

  Take(const torch::lazy::Value& self, const torch::lazy::Value& index)
      : XlaNode(torch::lazy::OpKind(at::aten::take),
              {self, index},
              [&]() { return TakeOutputShape(self, index); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const torch::lazy::Value& index) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Tan : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::tan);
  }

  Tan(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::tan),
              {self},
              [&]() { return TanOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Tanh : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::tanh);
  }

  Tanh(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::tanh),
              {self},
              [&]() { return TanhOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

class Tril : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::tril);
  }

  Tril(const torch::lazy::Value& self, const int64_t& diagonal)
      : XlaNode(torch::lazy::OpKind(at::aten::tril),
              {self},
              [&]() { return TrilOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash(diagonal)),
        diagonal(diagonal)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", diagonal=" << diagonal;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const int64_t& diagonal) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  int64_t diagonal;
  

};

class Triu : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::triu);
  }

  Triu(const torch::lazy::Value& self, const int64_t& diagonal)
      : XlaNode(torch::lazy::OpKind(at::aten::triu),
              {self},
              [&]() { return TriuOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash(diagonal)),
        diagonal(diagonal)
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    ss << ", diagonal=" << diagonal;
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self, const int64_t& diagonal) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  int64_t diagonal;
  

};

class Trunc : public XlaNode {
 public:
  static torch::lazy::OpKind ClassOpKind() {
    return torch::lazy::OpKind(at::aten::trunc);
  }

  Trunc(const torch::lazy::Value& self)
      : XlaNode(torch::lazy::OpKind(at::aten::trunc),
              {self},
              [&]() { return TruncOutputShape(self); },
              /* num_outputs */ 1,
              torch::lazy::MHash())
  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << XlaNode::ToString();
    
    return ss.str();
  }

  

  bool CanBeReused(const torch::lazy::Value& self) const {
    return false;
    }

  torch_xla::XlaOpVector Lower(LoweringContext* loctx) const override;

  
  

};

} // namespace torch_xla
