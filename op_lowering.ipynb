{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5670fd",
   "metadata": {},
   "source": [
    "## PyTorch dispatcher tutorial\n",
    "\n",
    "**What is the PyTorch Dispatcher?**\n",
    "\n",
    "At its core, the PyTorch dispatcher is a central routing mechanism within the PyTorch framework. Think of it like a sophisticated traffic controller for operator calls. When you execute a PyTorch operation like torch.add(a, b), the dispatcher's job is to figure out exactly which piece of code (called a kernel) should handle this operation based on the properties of the input tensors (a and b) and the current context.\n",
    "\n",
    "Its primary role is to decouple the definition of an operation (what torch.add means conceptually) from its various implementations (how to perform addition on CPU tensors, CUDA tensors, how to handle automatic differentiation, etc.). It acts as the central hub that manages and invokes the correct kernel for a given set of inputs and system state.\n",
    "\n",
    "The dispatcher is crucial for handling cross-cutting concerns â€“ features that apply across many different operators. These include:\n",
    "\n",
    "- **Device Type**: Running the operation on CPU, CUDA, MPS, XLA, etc.\n",
    "- **Data Type**: Handling different dtypes like float32, float16, int64.\n",
    "- **Autograd**: Enabling automatic differentiation by tracking computation graphs.\n",
    "- **Other Features**: Supporting things like TorchScript tracing, quantization, functionalization, and more.\n",
    "\n",
    "**Why is it Needed?**\n",
    "\n",
    "Without the dispatcher, we'd need a massive conditional block:\n",
    "```c++\n",
    "Tensor add(const Tensor& a, const Tensor& b) {\n",
    "    if (a.device().type() == kCPU && b.device().type() == kCPU) {\n",
    "        if (requires_grad(a) || requires_grad(b)) {\n",
    "            // Call CPU autograd addition kernel\n",
    "        } else {\n",
    "            // Call plain CPU addition kernel\n",
    "        }\n",
    "    }\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "- **Operator**: A fundamental operation in PyTorch, usually exposed as a function in the torch namespace (e.g., torch.add, torch.matmul, torch.relu). Operators have a defined schema that specifies their name, inputs, and outputs.\n",
    "- **Kernel**: A specific C++ function that implements an operator for a particular dispatch key. For example, there's a CPU kernel for torch.add, a CUDA kernel for torch.add, an Autograd kernel, etc.\n",
    "- **Dispatch Key**: An enum value (c10::DispatchKey) representing a specific context, feature, or backend. Keys are used to tag tensors and direct the dispatcher. Examples include CPU, CUDA, Autograd, QuantizedCPU, CompositeImplicitAutograd. Dispatch keys form a hierarchy or set, allowing the dispatcher to select the most appropriate kernel.\n",
    "- **Operator Schema**: A formal definition of an operator's signature, including its name (with overload name if applicable), arguments (name and type), and return values (name and type). Example: aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor. Schemas ensure type safety and consistency across different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e476805",
   "metadata": {},
   "source": [
    "## Dispatch keys\n",
    "\n",
    "Dispatch Keys are the fundamental identifiers used by the dispatcher to select the appropriate kernel. These keys are defined in `c10::DispatchKey` enum located in [c10/core/DispatchKey.h](https://github.com/pytorch/pytorch/blob/f252f9df5e0fa5d942218108cc5983bb72182086/c10/core/DispatchKey.h#L136) This enum establishes a prioritized ordering, where keys corresponding to more specialized or wrapping functionalities generally have higher priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd161ce",
   "metadata": {},
   "source": [
    "## Registering Kernels with the Dispatcher\n",
    "\n",
    "Kernels are the specific implementations of operators. To make the dispatcher aware of these implementations, you need to register them. This is primarily done using the `TORCH_LIBRARY` and `TORCH_LIBRARY_IMPL` macros in C++. These macros are the standard way to register operators and kernels from C++. They are typically used within C++ files that are compiled as part of PyTorch itself or as a C++ extension.\n",
    "\n",
    "`TORCH_LIBRARY(ns, m)`: Defines a library of operators under a specific namespace (ns). Common namespaces include `aten` (for standard PyTorch operators) or custom namespaces for extensions. Inside the TORCH_LIBRARY block, you use methods like m.def() to define the operator schema.\n",
    "\n",
    "```c++\n",
    "#include <torch/library.h>\n",
    "#include <ATen/core/dispatch/DispatchKey.h>\n",
    "#include <ATen/core/dispatch/Dispatcher.h>\n",
    "#include <ATen/Tensor.h>\n",
    "\n",
    "// TODO: Fix Scalar a and Scalar b.\n",
    "// Define the schema for 'xla_ops::ax_by'\n",
    "TORCH_LIBRARY(xla_ops, m) {\n",
    "    // Schema: xla_ops::ax_by(Tensor self, Tensor other, Scalar a, Scalar b) -> Tensor\n",
    "    m.def(\"ax_by(Tensor self, Tensor other, Scalar a, Scalar b) -> Tensor\");\n",
    "}\n",
    "```\n",
    "\n",
    "`TORCH_LIBRARY_IMPL(ns, key, m)`: Registers kernel implementations for operators within the namespace `ns` for a specific dispatch key. Inside the `TORCH_LIBRARY_IMPL` block, you use `m.impl()` to link an operator name (matching a schema defined in TORCH_LIBRARY) to a specific C++ kernel function. `key` is a `c10::DispatchKey` enum value (e.g., CPU, CUDA, Autograd)\n",
    "\n",
    "\n",
    "```c++\n",
    "#include <ATen/ops/add.h>\n",
    "\n",
    "// TODO: Fix Scalar a and Scalar b and implementation.\n",
    "at::Tensor xla_ax_by(const at::Tensor& self, const at::Tensor& other, Scalar a, Scalar b) {\n",
    "    std::cout << \"Executing xla_ops::ax_by Kernel!\" << std::endl;\n",
    "    return at::add(a*self, b*other);\n",
    "}\n",
    "\n",
    "// Register the kernel function for the XLA dispatch key\n",
    "TORCH_LIBRARY_IMPL(xla_ops, XLA, m) {\n",
    "    // Link \"ax_by\" (matching the schema) to our C++ function xla_ax_by\n",
    "    // The dispatcher ensures the function signature matches the schema.\n",
    "    m.impl(\"ax_by\", &xla_ax_by);\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7f6ee",
   "metadata": {},
   "source": [
    "\n",
    "To enable logs for PyTorch dispatcher traces showing which kernels are called, build pytorch with:\n",
    "\n",
    "```bash\n",
    "export CFLAGS=\"-DHAS_TORCH_SHOW_DISPATCH_TRACE\"\n",
    "python setup.py bdist_wheel\n",
    "python setup.py develop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49a6ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_SHOW_DISPATCH_TRACE=1\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_SHOW_DISPATCH_TRACE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d96e202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::ones], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::ones], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::ones], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::ones], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::ones], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::ones], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::ones], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::ones], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::abs], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::abs], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::abs.out], key=[CPU]\n",
      " [call] op=[aten::positive], key=[AutogradCPU]\n",
      " [call] op=[aten::neg], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::neg], key=[CPU]\n",
      " [call] op=[aten::is_nonzero], key=[AutogradCPU]\n",
      "  [call] op=[aten::item], key=[AutogradCPU]\n",
      "   [call] op=[aten::_local_scalar_dense], key=[AutogradCPU]\n",
      "    [redispatch] op=[aten::_local_scalar_dense], key=[CPU]\n",
      " [call] op=[aten::bitwise_not], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_not], key=[CPU]\n",
      " [call] op=[aten::pow.Tensor_Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::pow.Tensor_Tensor], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      " [call] op=[aten::matmul], key=[AutogradCPU]\n",
      "  [call] op=[aten::dot], key=[AutogradCPU]\n",
      "   [redispatch] op=[aten::dot], key=[CPU]\n",
      "    [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "    [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::floor_divide], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::floor_divide], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::div.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::div.Tensor], key=[CPU]\n",
      " [call] op=[aten::remainder.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::remainder.Tensor], key=[CPU]\n",
      " [call] op=[aten::add.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::add.Tensor], key=[CPU]\n",
      " [call] op=[aten::lt.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::lt.Tensor], key=[CPU]\n",
      " [call] op=[aten::gt.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::gt.Tensor], key=[CPU]\n",
      " [call] op=[aten::ge.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::ge.Tensor], key=[CPU]\n",
      " [call] op=[aten::le.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::le.Tensor], key=[CPU]\n",
      " [call] op=[aten::ne.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::ne.Tensor], key=[CPU]\n",
      " [call] op=[aten::eq.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::eq.Tensor], key=[CPU]\n",
      " [call] op=[aten::sub.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::sub.Tensor], key=[CPU]\n",
      " [call] op=[aten::pow_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::pow_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::pow_.Tensor], key=[CPU]\n",
      " [call] op=[aten::mul_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::mul_.Tensor], key=[CPU]\n",
      " [call] op=[aten::matmul], key=[AutogradCPU]\n",
      "  [call] op=[aten::dot], key=[AutogradCPU]\n",
      "   [redispatch] op=[aten::dot], key=[CPU]\n",
      "    [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "    [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::floor_divide_.Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::floor_divide_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::floor_divide_.Tensor], key=[CPU]\n",
      " [call] op=[aten::div_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::div_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::div_.Tensor], key=[CPU]\n",
      " [call] op=[aten::remainder_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::remainder_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::remainder_.Tensor], key=[CPU]\n",
      " [call] op=[aten::add_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::add_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::add_.Tensor], key=[CPU]\n",
      " [call] op=[aten::sub_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::sub_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::sub_.Tensor], key=[CPU]\n",
      " [call] op=[aten::__and__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_and.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_and.Tensor], key=[CPU]\n",
      " [call] op=[aten::__or__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_or.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_or.Tensor], key=[CPU]\n",
      " [call] op=[aten::__xor__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_xor.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_xor.Tensor], key=[CPU]\n",
      " [call] op=[aten::__iand__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_and_.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_and_.Tensor], key=[ADInplaceOrView]\n",
      "    [redispatch] op=[aten::bitwise_and_.Tensor], key=[CPU]\n",
      " [call] op=[aten::__ixor__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_xor_.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_xor_.Tensor], key=[ADInplaceOrView]\n",
      "    [redispatch] op=[aten::bitwise_xor_.Tensor], key=[CPU]\n",
      " [call] op=[aten::__ior__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_or_.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_or_.Tensor], key=[ADInplaceOrView]\n",
      "    [redispatch] op=[aten::bitwise_or_.Tensor], key=[CPU]\n",
      " [call] op=[aten::__lshift__.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::__lshift__.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::__rshift__.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::__rshift__.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::__ilshift__.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::__ilshift__.Scalar], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::__ilshift__.Scalar], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::__irshift__.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::__irshift__.Scalar], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::__irshift__.Scalar], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::select.int], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::select.int], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::select.int], key=[CPU]\n",
      "    [call] op=[aten::as_strided], key=[CPU]\n",
      " [call] op=[aten::pow.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::pow.Scalar], key=[CPU]\n",
      "   [call] op=[aten::result_type.Scalar_Tensor], key=[CPU]\n",
      "    [call] op=[aten::result_type.Scalar], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::floor_divide], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::floor_divide], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::reciprocal], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::reciprocal], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::remainder.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::remainder.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::remainder.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::add.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::add.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::gt.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::gt.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::lt.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::lt.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::le.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::le.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::ge.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::ge.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::ne.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::ne.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::eq.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::eq.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::rsub.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::rsub.Scalar], key=[CPU]\n",
      "   [call] op=[aten::sub.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::pow.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::pow.Scalar], key=[CPU]\n",
      "   [call] op=[aten::result_type.Scalar_Tensor], key=[CPU]\n",
      "    [call] op=[aten::result_type.Scalar], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::floor_divide], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::floor_divide], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::reciprocal], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::reciprocal], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::remainder.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::remainder.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::remainder.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::add.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::add.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::rsub.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::rsub.Scalar], key=[CPU]\n",
      "   [call] op=[aten::sub.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_and.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_and.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_and.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_or.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_or.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_or.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_xor.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_xor.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_xor.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_and.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_and.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_and.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_xor.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_xor.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_xor.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_or.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_or.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_or.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_left_shift.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_left_shift.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::bitwise_left_shift.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_right_shift.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_right_shift.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::bitwise_right_shift.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_left_shift.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_left_shift.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::bitwise_left_shift.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_right_shift.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_right_shift.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::bitwise_right_shift.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      "E0416 19:04:31.218000 3529524 torch/_inductor/codegen/cuda/cuda_env.py:22] Error getting cuda arch: Torch not compiled with CUDA enabled\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a002e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::randn], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::randn], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::normal_], key=[CPU]\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafb8fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::to.dtype_layout], key=[AutogradCPU]\n",
      "  [call] op=[aten::_to_copy], key=[AutogradCPU]\n",
      "   [redispatch] op=[aten::_to_copy], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::_to_copy], key=[XLA]\n",
      "     [call] op=[aten::to.dtype_layout], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::to.dtype_layout], key=[CPU]\n",
      "     [call] op=[aten::to.dtype_layout], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::to.dtype_layout], key=[CPU]\n"
     ]
    }
   ],
   "source": [
    "x = t.to('xla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46d2265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::add.Tensor], key=[AutogradXLA]\n",
      "  [redispatch] op=[aten::add.Tensor], key=[Functionalize]\n",
      "   [callBoxed] op=[aten::add.Tensor], key=[XLA]\n",
      "    [call] op=[aten::result_type.Tensor], key=[XLA]\n",
      "    [call] op=[aten::result_type.Tensor], key=[XLA]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      "    [call] op=[aten::item], key=[CPU]\n",
      "     [call] op=[aten::_local_scalar_dense], key=[CPU]\n"
     ]
    }
   ],
   "source": [
    "y = x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75140b8",
   "metadata": {},
   "source": [
    "## Registering kernels with dispatcher keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7b38c5",
   "metadata": {},
   "source": [
    "## Boxed vs Unboxed kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7533508",
   "metadata": {},
   "source": [
    "## Debugging dispatcher\n",
    "\n",
    "| Tool/Technique  | Description | Use Case |\n",
    "|---|---|---|\n",
    "| `torch._C._dispatch_dump(\"namespace::op_name\")` | Prints a list of all kernels registered for a specific operator overload, including the dispatch key they are registered for and their source code location. | Verify that a specific kernel (e.g., custom CUDA kernel) is actually registered for the correct operator and dispatch key. See competing kernels. |\n",
    "| `torch._C._dispatch_dump_table(\"namespace::op_name\")` | Shows the computed dispatch table for an operator. For each dispatch key, it indicates which registered kernel would be selected based on priority rules. | Understand kernel priorities, identify which kernel should run for a given key, and see if fallbacks are being used. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "245f8fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: aten::add.Tensor\n",
      "schema: aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n",
      "debug: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n",
      "alias analysis kind: FROM_SCHEMA\n",
      "MkldnnCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterMkldnnCPU_0.cpp:171 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "Named: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 :: (none) [ fallthrough boxed ]\n",
      "ZeroTensor: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterZeroTensor_0.cpp:123 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "Tracer: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17827 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "FuncTorchBatched: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:352 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "Batched: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "CPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCPU_0.cpp:1316 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "XLA: registered at bazel-out/k8-opt/bin/torch_xla/csrc/RegisterXLA.cpp:5050 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "Meta: registered at /dev/null:241 :: (none) [ boxed ]\n",
      "Meta (inactive): registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterMeta_0.cpp:1165 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "SparseCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSparseCPU_0.cpp:350 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "SparseMeta: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSparseMeta_0.cpp:151 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "SparseCsrCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSparseCsrCPU_0.cpp:402 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "SparseCsrMeta: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSparseCsrMeta_0.cpp:393 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "NestedTensorCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterNestedTensorCPU_0.cpp:318 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "NestedTensorHPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterNestedTensorHPU_0.cpp:306 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "Autograd[alias]: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "CompositeExplicitAutogradNonFunctional[alias]: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 :: (Tensor _0, Tensor _1, Scalar _2) -> Tensor _0 [ boxed unboxed ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch._C._dispatch_dump(\"aten::add.Tensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a535923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undefined: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "CPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCPU_0.cpp:1316 [kernel]\n",
      "CUDA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "HIP: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "XLA: registered at bazel-out/k8-opt/bin/torch_xla/csrc/RegisterXLA.cpp:5050 [kernel]\n",
      "MPS: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "IPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "XPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "HPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "VE: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "MTIA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "MAIA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "PrivateUse1: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "PrivateUse2: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "PrivateUse3: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "Meta: registered at /dev/null:241 [kernel]\n",
      "FPGA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "Vulkan: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "Metal: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedCUDA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedHIP: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedMPS: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedIPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedXPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedHPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedVE: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedMTIA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedMAIA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedPrivateUse1: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedPrivateUse2: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedPrivateUse3: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "QuantizedMeta: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "CustomRNGKeyId: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "MkldnnCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterMkldnnCPU_0.cpp:171 [kernel]\n",
      "SparseCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSparseCPU_0.cpp:350 [kernel]\n",
      "SparseMeta: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSparseMeta_0.cpp:151 [kernel]\n",
      "SparseCsrCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSparseCsrCPU_0.cpp:402 [kernel]\n",
      "SparseCsrCUDA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrHIP: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrMPS: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrIPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrXPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrHPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrVE: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrMTIA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrMAIA: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrPrivateUse1: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrPrivateUse2: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrPrivateUse3: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1380 [default backend kernel]\n",
      "SparseCsrMeta: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterSparseCsrMeta_0.cpp:393 [kernel]\n",
      "NestedTensorCPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterNestedTensorCPU_0.cpp:318 [kernel]\n",
      "NestedTensorHPU: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterNestedTensorHPU_0.cpp:306 [kernel]\n",
      "BackendSelect: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\n",
      "Functionalize: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\n",
      "Named: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\n",
      "Conjugate: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\n",
      "ZeroTensor: registered at /home/bbahl_google_com/pytorch/build/aten/src/ATen/RegisterZeroTensor_0.cpp:123 [kernel]\n",
      "ADInplaceOrView: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\n",
      "AutogradOther: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradCPU: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradCUDA: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradHIP: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradXLA: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradMPS: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradIPU: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradXPU: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradHPU: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradVE: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradLazy: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradMTIA: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradMAIA: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradPrivateUse1: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradPrivateUse2: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradPrivateUse3: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradMeta: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "AutogradNestedTensor: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
      "Tracer: registered at /home/bbahl_google_com/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17827 [kernel]\n",
      "AutocastCPU: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\n",
      "AutocastMTIA: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\n",
      "AutocastMAIA: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/autocast_mode.cpp:504 [backend fallback]\n",
      "AutocastXPU: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/autocast_mode.cpp:542 [backend fallback]\n",
      "AutocastXLA: fallthrough registered at torch_xla/csrc/autocast_mode.cpp:25 [backend fallback]\n",
      "AutocastMPS: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\n",
      "AutocastCUDA: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\n",
      "FuncTorchBatched: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/functorch/BatchRulesBinaryOps.cpp:352 [kernel]\n",
      "BatchedNestedTensor: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\n",
      "Batched: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\n",
      "VmapMode: fallthrough registered at /home/bbahl_google_com/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:208 [backend fallback]\n",
      "PythonTLSSnapshot: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\n",
      "PreDispatch: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\n",
      "PythonDispatcher: registered at /home/bbahl_google_com/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch._C._dispatch_dump_table(\"aten::add.Tensor\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
