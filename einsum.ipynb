{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bbahl/miniconda3/envs/torchprime/lib/python3.10/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[2,5,6]{2,1,0} xla::device_data(), xla_shape=f32[2,5,6]{2,1,0}\n",
      "  %1 = f32[2,3,4,5]{3,2,1,0} xla::device_data(), xla_shape=f32[2,3,4,5]{3,2,1,0}\n",
      "  %2 = f32[2,3,4,6]{3,2,1,0} aten::einsum(%1, %0), xla_shape=f32[2,3,4,6]{3,2,1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "HloModule IrToHlo.6, entry_computation_layout={(f32[2,5,6]{2,1,0}, f32[2,3,4,5]{3,2,1,0})->(f32[2,3,4,6]{3,2,1,0})}\n",
      "\n",
      "ENTRY %IrToHlo.6 (p0.1: f32[2,5,6], p1.2: f32[2,3,4,5]) -> (f32[2,3,4,6]) {\n",
      "  %p1.2 = f32[2,3,4,5]{3,2,1,0} parameter(1)\n",
      "  %p0.1 = f32[2,5,6]{2,1,0} parameter(0)\n",
      "  %dot.3 = f32[2,3,4,6]{3,2,1,0} dot(f32[2,3,4,5]{3,2,1,0} %p1.2, f32[2,5,6]{2,1,0} %p0.1), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n",
      "  %transpose.4 = f32[2,3,4,6]{3,2,1,0} transpose(f32[2,3,4,6]{3,2,1,0} %dot.3), dimensions={0,1,2,3}\n",
      "  ROOT %tuple.5 = (f32[2,3,4,6]{3,2,1,0}) tuple(f32[2,3,4,6]{3,2,1,0} %transpose.4)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch_xla.runtime\n",
    "import torch\n",
    "import torch_xla\n",
    "\n",
    "x = torch.randn(2, 3, 4, 5, requires_grad=True)\n",
    "y = torch.randn(2, 5, 6, requires_grad=True)\n",
    "\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    x = x.to('xla').requires_grad_()\n",
    "    y = y.to('xla').requires_grad_()\n",
    "    out = torch.einsum(\"ebcm,emh->ebch\", x, y)\n",
    "\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out]))\n",
    "print(torch_xla._XLAC._get_xla_tensors_hlo([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[2,5,4,8]{3,1,2,0} xla::device_data(), xla_shape=f32[2,5,4,8]{3,1,2,0}\n",
      "  %1 = f32[2,5,6]{2,1,0} xla::device_data(), xla_shape=f32[2,5,6]{2,1,0}\n",
      "  %2 = f32[2,6,4,8]{3,2,1,0} aten::einsum(%1, %0), xla_shape=f32[2,6,4,8]{3,2,1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "HloModule IrToHlo.6, entry_computation_layout={(f32[2,5,4,8]{3,1,2,0}, f32[2,5,6]{2,1,0})->(f32[2,6,4,8]{3,2,1,0})}\n",
      "\n",
      "ENTRY %IrToHlo.6 (p0.1: f32[2,5,4,8], p1.2: f32[2,5,6]) -> (f32[2,6,4,8]) {\n",
      "  %p1.2 = f32[2,5,6]{2,1,0} parameter(1)\n",
      "  %p0.1 = f32[2,5,4,8]{3,1,2,0} parameter(0)\n",
      "  %dot.3 = f32[2,6,4,8]{3,2,1,0} dot(f32[2,5,6]{2,1,0} %p1.2, f32[2,5,4,8]{3,1,2,0} %p0.1), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n",
      "  %transpose.4 = f32[2,6,4,8]{3,2,1,0} transpose(f32[2,6,4,8]{3,2,1,0} %dot.3), dimensions={0,1,2,3}\n",
      "  ROOT %tuple.5 = (f32[2,6,4,8]{3,2,1,0}) tuple(f32[2,6,4,8]{3,2,1,0} %transpose.4)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch_xla.device()\n",
    "x1 = torch.randn(2, 5, 6, requires_grad=True)\n",
    "y1 = torch.randn(2, 5, 4, 8, requires_grad=True)\n",
    "\n",
    "with torch.enable_grad():\n",
    "  x1 = x1.to('xla')\n",
    "  y1 = y1.to('xla')\n",
    "  out1 = torch.einsum(\"bsm,bsec->bmec\", x1, y1)\n",
    "\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out1]))\n",
    "print(torch_xla._XLAC._get_xla_tensors_hlo([out1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[4,5,6]{2,1,0} xla::device_data(), xla_shape=f32[4,5,6]{2,1,0}\n",
      "  %1 = f32[2,5,4,8]{3,1,2,0} xla::device_data(), xla_shape=f32[2,5,4,8]{3,1,2,0}\n",
      "  %2 = f32[4,2,8,6]{3,2,1,0} aten::einsum(%1, %0), xla_shape=f32[4,2,8,6]{3,2,1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "HloModule IrToHlo.6, entry_computation_layout={(f32[4,5,6]{2,1,0}, f32[2,5,4,8]{3,1,2,0})->(f32[4,2,8,6]{3,2,1,0})}\n",
      "\n",
      "ENTRY %IrToHlo.6 (p0.1: f32[4,5,6], p1.2: f32[2,5,4,8]) -> (f32[4,2,8,6]) {\n",
      "  %p1.2 = f32[2,5,4,8]{3,1,2,0} parameter(1)\n",
      "  %p0.1 = f32[4,5,6]{2,1,0} parameter(0)\n",
      "  %dot.3 = f32[4,2,8,6]{3,2,1,0} dot(f32[2,5,4,8]{3,1,2,0} %p1.2, f32[4,5,6]{2,1,0} %p0.1), lhs_batch_dims={2}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n",
      "  %transpose.4 = f32[4,2,8,6]{3,2,1,0} transpose(f32[4,2,8,6]{3,2,1,0} %dot.3), dimensions={0,1,2,3}\n",
      "  ROOT %tuple.5 = (f32[4,2,8,6]{3,2,1,0}) tuple(f32[4,2,8,6]{3,2,1,0} %transpose.4)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.randn(2, 5, 4, 8, requires_grad=True)\n",
    "y2 = torch.randn(4, 5, 6, requires_grad=True)\n",
    "\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    x2 = x2.to('xla').requires_grad_()\n",
    "    y2 = y2.to('xla').requires_grad_()\n",
    "    out2= torch.einsum(\"bmec,emh->ebch\", x2, y2)\n",
    "\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out2]))\n",
    "print(torch_xla._XLAC._get_xla_tensors_hlo([out2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[2,8,6]{1,2,0} xla::device_data(), xla_shape=f32[2,8,6]{1,2,0}\n",
      "  %1 = f32[2,5,4,8]{3,1,2,0} xla::device_data(), xla_shape=f32[2,5,4,8]{3,1,2,0}\n",
      "  %2 = f32[2,5,4,6]{3,2,1,0} aten::einsum(%1, %0), xla_shape=f32[2,5,4,6]{3,2,1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "HloModule IrToHlo.6, entry_computation_layout={(f32[2,8,6]{1,2,0}, f32[2,5,4,8]{3,1,2,0})->(f32[2,5,4,6]{3,2,1,0})}\n",
      "\n",
      "ENTRY %IrToHlo.6 (p0.1: f32[2,8,6], p1.2: f32[2,5,4,8]) -> (f32[2,5,4,6]) {\n",
      "  %p1.2 = f32[2,5,4,8]{3,1,2,0} parameter(1)\n",
      "  %p0.1 = f32[2,8,6]{1,2,0} parameter(0)\n",
      "  %dot.3 = f32[2,5,4,6]{3,2,1,0} dot(f32[2,5,4,8]{3,1,2,0} %p1.2, f32[2,8,6]{1,2,0} %p0.1), lhs_batch_dims={0}, lhs_contracting_dims={3}, rhs_batch_dims={0}, rhs_contracting_dims={1}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n",
      "  %transpose.4 = f32[2,5,4,6]{3,2,1,0} transpose(f32[2,5,4,6]{3,2,1,0} %dot.3), dimensions={0,1,2,3}\n",
      "  ROOT %tuple.5 = (f32[2,5,4,6]{3,2,1,0}) tuple(f32[2,5,4,6]{3,2,1,0} %transpose.4)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x3 = torch.randn(2, 5, 4, 8, requires_grad=True)\n",
    "y3 = torch.randn(2, 8, 6, requires_grad=True)\n",
    "\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    x3 = x3.to('xla').requires_grad_()\n",
    "    y3 = y3.to('xla').requires_grad_()\n",
    "    out3= torch.einsum(\"ebch,ehm->ebcm\", x3, y3)\n",
    "\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out3]))\n",
    "print(torch_xla._XLAC._get_xla_tensors_hlo([out3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[5,6,2,4]{1,0,3,2} xla::device_data(), xla_shape=f32[5,6,2,4]{1,0,3,2}\n",
      "  %1 = f32[2,5,4,8]{3,1,2,0} xla::device_data(), xla_shape=f32[2,5,4,8]{3,1,2,0}\n",
      "  %2 = f32[5,6,8]{1,2,0} aten::einsum(%1, %0), xla_shape=f32[5,6,8]{1,2,0}, ROOT=0\n",
      "}\n",
      "\n",
      "HloModule IrToHlo.6, entry_computation_layout={(f32[5,6,2,4]{1,0,3,2}, f32[2,5,4,8]{3,1,2,0})->(f32[5,6,8]{1,2,0})}\n",
      "\n",
      "ENTRY %IrToHlo.6 (p0.1: f32[5,6,2,4], p1.2: f32[2,5,4,8]) -> (f32[5,6,8]) {\n",
      "  %p1.2 = f32[2,5,4,8]{3,1,2,0} parameter(1)\n",
      "  %p0.1 = f32[5,6,2,4]{1,0,3,2} parameter(0)\n",
      "  %dot.3 = f32[5,8,6]{2,1,0} dot(f32[2,5,4,8]{3,1,2,0} %p1.2, f32[5,6,2,4]{1,0,3,2} %p0.1), lhs_batch_dims={1}, lhs_contracting_dims={0,2}, rhs_batch_dims={0}, rhs_contracting_dims={2,3}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n",
      "  %transpose.4 = f32[5,6,8]{1,2,0} transpose(f32[5,8,6]{2,1,0} %dot.3), dimensions={0,2,1}\n",
      "  ROOT %tuple.5 = (f32[5,6,8]{1,2,0}) tuple(f32[5,6,8]{1,2,0} %transpose.4)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x4 = torch.randn(2, 5, 4, 8, requires_grad=True)\n",
    "y4 = torch.randn(5, 6 , 2, 4, requires_grad=True)\n",
    "\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    x4 = x4.to('xla').requires_grad_()\n",
    "    y4 = y4.to('xla').requires_grad_()\n",
    "    out4= torch.einsum(\"ebcm,bsec->bsm\", x4, y4)\n",
    "\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out4]))\n",
    "print(torch_xla._XLAC._get_xla_tensors_hlo([out4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchprime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
